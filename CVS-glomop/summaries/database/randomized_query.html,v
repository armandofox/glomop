head	1.2;
access;
symbols;
locks; strict;
comment	@# @;


1.2
date	97.02.20.18.46.03;	author fox;	state Exp;
branches;
next	1.1;

1.1
date	97.02.20.09.18.30;	author gribble;	state Exp;
branches;
next	;


desc
@@


1.2
log
@*** empty log message ***
@
text
@<html>
<title>
Randomized Algorithms for Optimizing Large Join Queries
</title>
<body>
<a href=index.html><i>Back to index</i></a>
<h1>
Randomized Algorithms for Optimizing Large Join Queries
</h1>
<strong>
Yannis E. Ioannidis and Younkyung Cha Kang
</strong>

<p><b>One-line summary:</b>
Two randomized algorithms (iterative improvement and simulated annealing),
as well as a hybrid, are presented, analyzed, and measured;  the hybrid
scheme seems to win.
</p>

<h2>Overview/Main Points</h2>

<ul>
     <li> <B>State space</B>: each state in query optimization corresponds
	  to an access plan (strategy) of the query to be optimized.  Here,
	  plans are join processing trees, where leaves are base relations,
	  internal nodes are join operators, and edges indicate the flow of
	  data.
     <li> <B>Neighbours</B>: neighbours of a state are determined by a set
	  of transformation rules.  Applying a rule usually changes the
	  cost of the tree only in a local spot of the tree, so recomputing
	  costs is cheap.
	  <ol>
	       <li> the join method choice (hash, nested loops, sort-merge)
		    can be changed
	       <li> the join can be commuted: a join b -> b join a
	       <li> associativity can be applied: (a join b) join c -> a
		    join (b join c)
	       <li> a left join exchange: (a join b) join c -> (a join c)
		    -> join b
	       <li> a right join exchange: a join (b join c) -> b join (a
		    join c)
	  </ol>
     <li> <B>Iterative Improvement (II)</B>:
	  <ul>
	       <li> pick a random state
	       <li> while not at a local minima,
		    <ul>
			 <li> pick a random neighbour to the current state
			 <li> if the neighbour has a lower cost, move there
		    </ul>
	       <li> repeat until a stopping condition is achieved (a timeout
		    for II)
	  </ul>
     <li> <B>Simulated annealing (SA)</B>: (frozen is defined as temp 0)
	  <ul>
	       <li> while not frozen,
		    <ul>
			 <li> while not equilibrium,
			      <ul>
				   <li> pick random neighbour
				   <li> if neighbour is lower cost, go
					there
				   <li> if not, go there anyway with a
					probability directly related to
					temperature and inversely related
					to cost
			      </ul>
			 <li> reduce temperature
		    </ul>
	       <li> stop when frozen
	  </ul>
     <li> <B>Hybrid - Two phase optimization (2PO)</B>:
	  <ul>
	       <li> Run II for some period of time to find a number of
		    local optima
	       <li> take the best local optima, and use it as the starting
		    state of SA, but run SA with a low starting temperature
		    so it doesn't jump around too much
	  </ul>

     <li> <B>Performance</B>:
	  <ul>
	       <li> experiments run with random queries (start and tree
		    queries, ranging between 5 and 100 joins).  Three
		    different relation catalogs of varying relation
		    cardinalities, unique values, and selectivities were
		    used.
	       <li> in nearly all cases, 2PO beats SA, and SA beats II.
		    Although II initially decreases in cost quite quickly,
		    it does not converge to the lowest global cost very
		    quickly at all.  SA eventually does converge to lowest
		    global cost, but takes a while to get there.  2PO is
		    best of both worlds.
	  </ul>
     <li> <B>State space analysis</B>:
	  <ul>
	       <li> Some (rather cheesy) experiments were done to try and
		    quantify the shape of the very large dimensional state
		    space.
	       <li> Their conclusion was that state space should be
		    visualized as a cup, in which the floor of the cup has
		    many local minima close to each other.
	  </ul>
</ul>

<h2>Relevance</h2>
Randomized algorithms work well for query optimization;  they produce query
plans that are competitive in quality with exhaustive search, but at faster
rates.

<h2>Flaws</h2>

<ul>
     <li> The assumptions made for the cost functions seem highly
          unlifelike: no pipelining of joins, minimal buffering, no
          duplicate elimination.
     <li> Scant support for the claim that the execution time savings of
          using <i>r-local</i> minima outweigh the potential misses of
          true local minima.
     <li> The state space analysis was just totally unbelievable.  They
	  leap to conclusions about a potentially very hairy topology with
	  nearly no data at all.
     <li> Are these the only randomized algorithms that are applicable?
	  What others should be explored?
     <li> The workloads they ran the optimizations against were generated
	  with healthy doses of randomness, leading me to suspect that they
	  had little to do with the real world.  How do these algorithms
	  perform with real-world workloads?
</ul>
<hr>
<a href=index.html><i>Back to index</i></a>

</body></html>
@


1.1
log
@Added randomized query paper summary.
@
text
@d114 6
@
