head	1.3;
access;
symbols;
locks; strict;
comment	@# @;


1.3
date	97.02.19.18.43.26;	author fox;	state Exp;
branches;
next	1.2;

1.2
date	97.02.04.02.36.52;	author fox;	state Exp;
branches;
next	1.1;

1.1
date	97.02.04.02.32.42;	author fox;	state Exp;
branches;
next	;


desc
@@


1.3
log
@added bayou
@
text
@<html>
<title>
Linear Clustering
</title>
<body>
<a href=index.html><i>Back to index</i></a>
<h1>
Linear Clustering of Objects with Multiple Attributes
</h1>
<strong>
H.V. Jagadish, Bell Labs
</strong>
<br><br>
<i>Summary by Armando Fox and Steve Gribble</i>


<p><b>One-line summary:</b>
Survey of  various schemes for
mapping multidimensional data into a linear space, with investigation of
locality-preserving properties for column-based and range queries.
</p>

<h2>Overview/Main Points</h2>

<ul>
  <li> Methods for mapping 2D to 1D:
       <ul>
         <li> Column scan
         <li> Snake scan: reverse direction of scan on alternate
              columns, so last element of column 2n and last element of
              column 2n+1 stay adjacent
         <li> Z-curve scan: obtained by interleaving bits of binary
              representation X,Y coords; scanning order is LL, UL, LR,
              UR, recursively.
         <li> Gray coding: like z-curve, but Gray-code to obtain 1D
              coord.
         <li> Hilbert curve: see fig. 4 in paper; hard to describe but
              easy to see.
       </ul>
  <li> "Goodness" metrics: number of disk blocks fetched, and number of
       non-consecutive block pairs in fetch.
  <li> Query types: &lt;*,value&gt; or &lt;value,*&gt;, i.e. horiz. or
       vertical line in 2-space; or &lt;Rx,Ry&gt; where Ri is a range on
       dimension i, i.e. a rectangle in 2-space.
  <li> expected number of continuous runs to
       complete row or column query:
       <ul>
         <li>        column scan sucks except when a column is
       picked
         <li> snake slightly better for horiz. lines near "top" or
       "bottom" rows;
         <li> z-curve and Hilbert curve least affected by
              horiz. vs. vertical
         <li> No "clear winner" among z, Hilbert, snake
       </ul>
  <li> expected number of continuous runs to complete rectangle query:
       all do almost equally well, with snake doing a bit better! (see
       tables below)
  <li> expected linear span of mapping a rectangle: O(perimeter) for all
       mappings
  <li> So why use complicated z/Gray/Hilbert instead of simple snake
       mapping? Answer: if multiple 2D grid points can be mapped to
       single disk block, analysis changes.
  <li> Experimental results ("simulations"? hard to say) show that
       "locality at every level" (my phrase) in z, Hilbert, Gray makes
       them do fewer I/O's to complete query than snake, with Hilbert
       and Gray usually doing best.
  <li> Insight: for all except snake, number of hits per block to fetch
       a selection is proportional to size of selected size, but
       <i>independent</i> of size of database.  Snake is always
       proportional to size of database since columns are scanned
       contiguously.
  <li> Same insight explains superior performance over snake w/r/t fraction of
       blocks read sequentially and hits per block.
  <li> Extensions to multiple dimensions: 3D "primitives" for Hilbert
       and z/Gray are shown.
  <li> Extensions to non-square regions: replicate ("tile") pattern to
       map the area, or have a primitive whose interpoint distances are
       non-invariant w/r/t rotation.  E.g. fig 18, last page of paper.
  <li> Previous work result: <i>k</i>-attribute selection on database
       with <i>N</i> records has disk access cost
       O(N<sup>(k-1)/k</sup>); but
       this paper shows that for specific classes of queries we can do
       much better, in fact O(k) independent of N for some cases.
</ul>

<h2>Relevance</h2>

Concrete and practical "goodness metrics" for locality ("clustering")  in
database sense (i.e. "few disk I/O's"), and of how to preserve locality
inherent in a multiple-dimension representation while
mapping down to a smaller number of dimensions.

<h2>Flaws</h2>

<ul>
  <li> "Experimental results" - seems like they could have just been
       computed from first principles, if the distribution of request
       sizes and parameters was known or could be assumed.
</ul>
<hr>
<a href=index.html><i>Back to index</i></a>

</body></html>
@


1.2
log
@*** empty log message ***
@
text
@d13 3
@


1.1
log
@*** empty log message ***
@
text
@d79 1
a79 1
       O(N<super>(k-1)/k</super>); but
@
