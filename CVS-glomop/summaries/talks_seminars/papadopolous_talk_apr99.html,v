head	1.2;
access;
symbols;
locks; strict;
comment	@# @;


1.2
date	99.04.07.22.09.41;	author gribble;	state Exp;
branches;
next	1.1;

1.1
date	99.04.07.18.58.04;	author gribble;	state Exp;
branches;
next	;


desc
@@


1.2
log
@*** empty log message ***
@
text
@<HTML>
<HEAD>
   <META HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=iso-8859-1">
   <META NAME="GENERATOR" CONTENT="Mozilla/4.05 [en] (X11; U; Linux 2.0.33 i586) [Netscape]">
</HEAD>
<BODY bgcolor="#ffffff">
Papadopolous talk

<P>Gap:&nbsp; networks 8x in a year, processors 2x in a year.&nbsp; services
placing increasing demands on what we need from processors, and processors
can't keep up:

<P>40000 processors, 4 TB RAM, million simultaneous users, ...:&nbsp; AOL
- acres of machines just to support very limited narrowband experience.

<P>harsh realities:&nbsp; systems are incredibly complex to design and
manage and expensive to run, but mission critical.&nbsp; biz scaling pressures
outstrip technology pressures by factor of two.&nbsp; complexity:&nbsp;
marginally stable systems.&nbsp; unmanageable testing combinatorics (platforms
and software components).&nbsp; costs are in the leaves: desktops and workgroup
servers.

<P>mitigating complexity: recentralize state, pull from leaves and up through
network.&nbsp; consolidate services onto fewer, bigger systems to recentralize
control.&nbsp; unitary services on (scalable) partitioned servers.

<P>I/O centric view:&nbsp; flows passing through services and transformed
on the way.

<P>ref arch:&nbsp; wan/lan &lt;-- gbit enet/atm &lt;--> swtiched san fabraic
&lt;--> gbyte links &lt;-->&nbsp;smp array&nbsp;&lt;-->&nbsp;gbyte links
&lt;--> switched storage fabric &lt;--> storage arrays/containers

<P>what makes parallelism hard:&nbsp; hard to break computing into parallel
chunks (processes, threads).&nbsp; hard to layout data across nodes.&nbsp;
really hard to do both at once.

<P>"unless problem decomposes nicely, so know how to stripe or layout data
in memory/disk, end up destroying locality, have trouble."&nbsp;-->&nbsp;perfect
motivation for SDDS.&nbsp; pick problem domain carefully.

<P>control decomposition:&nbsp; workload distrib &amp; sync.&nbsp; data
decomposition: data dstirbut and comm.&nbsp; these decompositions get coupled.&nbsp;
non-uniformity makes this coupling worse.&nbsp;&nbsp; solutions:&nbsp;&nbsp;
randomize data placement to eliminate hotspots, make communication free,
and decouple problems with symmetry.

<P><B>symmetry permits data locality to be destroyed.</B>

<P>availability: design demands at least two independent domains per application.&nbsp;
more than two domains decreases performance downside, but increases complexity.
<BR>golden rule:&nbsp; keep domain scaling and replication orthogonal&nbsp;
&nbsp;(availability).

<P>What limits SMPs?&nbsp;&nbsp; address coherence bandwidth?&nbsp; data
bandwidth?&nbsp; competitive latencies?
<BR>&nbsp;&nbsp; --> all three in a balanced system, but fundamentally,
historic problem is address coherence bandwidth

<P>coherence argument:&nbsp;address (snoop) bandwidth can be increased
only with parallel transactions.

<P>cardinal design flaw:&nbsp; designing a machine that scales up, but
now down.&nbsp; performance _and_ scalability.

<P>systems that minimize I/O and memory non-uniformity will win.

<P>systems calculus:
<UL>
<LI>
decompose</LI>

<LI>
reintegrate</LI>

<LI>
partition</LI>
</UL>
NOW&nbsp;mythology:
<UL>
<LI>
hardware arguments are specious:&nbsp; things are really homogenious, except
for interconnects</LI>

<LI>
software arguments hold a little more weight</LI>

<LI>
works well for decouples problems:&nbsp;especially with independent I/O
streams.&nbsp; useful for consolidation.</LI>
</UL>
scalable only if:&nbsp; add to benefit performance without adding complexity
to detriment of management of apps.

<p>
<b>Summary meme</b>:  in a distributed system, close the feedback loop to allow
responsive adaptation, load balancing, fault detection, etc.
</BODY>
</HTML>
@


1.1
log
@done
@
text
@d94 4
@
