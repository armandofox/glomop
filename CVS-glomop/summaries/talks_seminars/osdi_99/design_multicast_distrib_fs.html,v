head	1.2;
access;
symbols;
locks; strict;
comment	@# @;


1.2
date	99.02.26.17.35.12;	author gribble;	state Exp;
branches;
next	1.1;

1.1
date	99.02.26.17.30.04;	author gribble;	state Exp;
branches;
next	;


desc
@@


1.2
log
@whoops
@
text
@<html> <head>
<title>Design of a Multicast-based Distributed FS</title
</head>

<body>
<h2>Design of a Multicast-based Distributed FS <i>(JetFile)</i><p>
Swedish Institute of Computer Science</h2>

<h3>Paper summary</h3>
<ul>
     <li> Target:  personal computing.  Designed for ubiquitous FS access
	  (local and WAN), wireless, satellite.  Assume peer-to-peer
	  multicast communications.  Assume no RPC.  Clients are servers.
     <li> Take protocol design approach:  design to hide propagation
	  delays, retransmission induced delays, bandwidth induced delays.
	  Minimize traffic, or at worst localize traffic.
     <li> file managers - clients that act as servers as well.  Versioning
	  server:  does serialization of updates.  Storage server:
	  long-term storage of files and backup.  Key server: crypto for
	  file signing and encryption, if need this.
     <li> JetFile SRM:
	  <ul>
	       <li> Files are versioned.  Names are tuples (organization,
		    volume, file number, verrsion).  hash this tuple
		    (except version) to obtain multicast channel.
	       <li> deals with data units - status object (request, repair)
		    and data object (request, repair).  Also have
		    version-request, version-repair.
	       <li> Initial file segment request over multicast, and
		    response acts to identify host serving file.  Use
		    unicast to get rest of file.
	  </ul>
     <li> Write-on-close semantics.  <i>(Large conflict
	  window??)</i>
     <li> Update implies new version.  (optimize: reuse same
	  version number for multiple updates if no sharing)
     <li> Client becomes server for new file version.
     <li> Updates:  new version number issued to order file updates.
	  Concurrently updated files get different version numbers.
	  Unexpected version number implies update conflict.  (data never
	  lost)
     <li> best-effort multicast callbacks
     <li> Consistency:  because using multicast, consistency depends on
	  volume of traffic.  In case of little traffic, high consistency
	  (as good as AFS).  In case of high contention, lower consistency
	  (no worse than NFS).
     <li> more info:  <a
	  href="http://www.sics.se/cna/dist_app.html">
	  http://www.sics.se/cna/dist_app.html</a>  
</ul>

<h3>Questions from audience</h3>
<ul>
     <li> Security:  what sort of trust relationship between FS peers?
	  Will you really have to do a signature check on a file before you
	  use it?  (Of course.)
     <li> Scalability:  each file is own multicast channel.  Doesn't that
	  nail you in terms of router state?  Answers:  hash many files to
	  same multicast channel, and filter.
</ul>
<h3>Steve's thoughts</h3>
<ul>
     <li> Seems like a poor match:  multicast (designed to get same info to
	  many people) and FS access (individual clients accessing independent
	  pieces of FS).  Incongruous modus operandi?
     <li> Ahh...not for everything.  Things like call-back cache validation
	  work much better no (1 multicast instead of N unicasts that AFS
	  would need).  Basically, use multicast for often-shared metadata,
	  and unicast for rarely-shared data.
     <li> took xFS ideas, multicast ideas, coda ideas, and conflagrated
	  them. Difficult to see if they get best of all, or worst of all,
	  or both.
</ul>

<hr>
<address><a href="http://www.cs.berkeley.edu/~gribble/">Steve
Gribble</a> / <i><a   
HREF="mailto:gribble@@cs.berkeley.edu">gribble@@cs.berkeley.edu</a></i></address>
<!-- hhmts start -->
Last modified: Thu Feb 25 10:02:22 1999
<!-- hhmts end -->
</body> </html>
@


1.1
log
@more good stuff - osdi 1999
@
text
@d65 1
a65 1
	  pieces of FS).  Incongruous modus operati?
@
