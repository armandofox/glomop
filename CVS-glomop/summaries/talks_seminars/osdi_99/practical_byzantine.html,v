head	1.1;
access;
symbols;
locks; strict;
comment	@# @;


1.1
date	99.02.26.17.30.06;	author gribble;	state Exp;
branches;
next	;


desc
@@


1.1
log
@more good stuff - osdi 1999
@
text
@<html> <head>
<title>Practical Byzantine Fualt Tolerance</title>
</head>

<body>
<h2>Practical Byzantine Fualt Tolerance</p>
Castro and Liskov</h2>

<h3>Paper summary</h3>
<ul>
     <li> Byzantine fault tolerance:  handles situation if hacker controls
	  nodes in a distributed system.  compare with fail-stop or fail-safe
     <li> problems with previous work: theoretical, few implementations,
	  little analysis, rely on synchrony for correctness, too slow.
     <li> algorithm properties:  safety (linearizability as tho0ugh had
	  correct centralized service) and liveness
	  (eventually get replies).  Assumptions:  need 3f+1 replicas to
	  tolerate f faults.  for liveness, assume eventual message
	  delivery (weaker assumption than synchrony).
     <li> algorithm:  
	  <ul>
	       <li> deterministic replicas of state machines start in the
		    same state, and execute same requests in same order.
	       <li> view:  primary plus set of backups.
	       <li> ordering:   primary picks ordering, backups ensure
		    primary behaves.  (certify correct statemetns, trigger
		    view changes to change primary)
	       <li> 3 phase: pre-prepare picks order, prepare ensures order
		    within views, commit ensures order across views
	       <li> replicas remember messages in log.  Messages are
		    authenticatd (to ensure identity)
	       <li> pre-prepare:  assign sequence number n to message m in view
		    v.  sign request, send to al replicas.
	       <li> prepare: if accept prepare, all replicas mulitcast a
		    prepare message out.  If receive 2f prepares, move to
		    commit.
	       <li> view changes: liveness when primary fails - 
	  </ul>
	  
</ul>

<h3>Questions from audience</h3>
<ul>
     <li> performance overhead (for their FS implementation, 26% overhead,
	  roughly.  but the disk is involved...)
</ul>
<h3>Steve's thoughts</h3>
<ul>
     <li> primary:  performance bottleneck if not careful (but can
	  partition primaries across cluster based on message request type
	  or service)
     <li> very expensive in terms of network messages (at least 2
	  multicasts per node in cluster).
</ul>

<hr>
<address><a href="http://www.cs.berkeley.edu/~gribble/">Steve
Gribble</a> / <i><a   
HREF="mailto:gribble@@cs.berkeley.edu">gribble@@cs.berkeley.edu</a></i></address>
<!-- hhmts start -->
Last modified: Wed Feb 24 12:32:32 1999
<!-- hhmts end -->
</body> </html>
@
