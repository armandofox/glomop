head     1.1;
branch   1.1.1;
access   ;
symbols  initial:1.1.1.1 initial:1.1.1;
locks    ; strict;
comment  @# @;


1.1
date     96.11.14.07.51.58;  author fox;  state Exp;
branches 1.1.1.1;
next     ;

1.1.1.1
date     96.11.14.07.51.58;  author fox;  state Exp;
branches ;
next     ;


desc
@@



1.1
log
@Initial revision
@
text
@<HTML>
<HEAD>
  <TITLE>WWW Speculative Prefetching</TITLE>
  <META NAME="GENERATOR" CONTENT="Adobe PageMill 2.0 Mac">
</HEAD>
<BODY>

<H1>Using Speculation to Reduce Server Load and Service Time on the WWW</H1>

<P>Azer Bestavros, B.U. </P>

<H2>One-line summary</H2>

<P>Speculating small documents costs little bandwidth but wins big in latency
for &quot;institutional&quot; or multiuser caches. </P>

<H2>Main ideas</H2>

<UL>
  <LI>Let Tw be a fixed time interval, let P[i,j] be a matrix giving the
  probability that an access to document <I>i</I> will be followed by an
  access to document <I>j</I> within time Tw, and let P*=P^N  be the closure
  of P.  Authors computed P* from BU CS dept. logs.</LI>
  <LI>Speculatively serve Dj if Di is requested, given a server configured
  for some threshold probability <I>p</I>.  Distinguish <I>embedded</I> (&quot;always&quot;)
  dependencies from <I>&quot;traveersal&quot;</I> (&quot;sometimes&quot;)
  dependencies.</LI>
  <LI><B>Metrics for performance </B>when speculation is employed or not:
  bandwidth ratio (total # bytes served), server load ratio (# requests to
  server), service time ratio (latency perceived at client), miss rate ratio
  (in bytes, not documents).<B></B></LI>
  <LI><B>5% extra bandwidth =&gt; 30% reduction </B>in server load, 23% reduction
  in service time, 18% reduction in client miss rate.</LI>
  <LI>Speculating small documents costs little and buys a lot, mostly due
  to protocol overhead for small requests.  Graph shows that approx. 15KB
  is the cutoff point.</LI>
  <LI>Client caching modeled using &quot;session timeout&quot; after which
  document is evicted (as opposed to LRU-type policies).</LI>
  <LI>&quot;Cooperative client&quot; can include with each request the list
  of documents it has cached, so server doesn't speculate documents client
  already has cached.</LI>
  <LI>Implementation: proxy can get documents &quot;pushed&quot; from server.</LI>
  <LI>Separate work in progress goal is to combine user-directed speculation
  (based on per-user access patterns) with server speculation (based on server
  logs)</LI>
</UL>

<H2>Flaws/comments</H2>

<UL>
  <LI>&quot;WWW offers opportunity to study unprecedented distribution of
  doc sizes and types&quot;: this isn't really true in practice.</LI>
  <LI>The extra bandwidth and computation for the client to include its cache
  directory with every request do not appear to be modeled.</LI>
  <LI>Not clear that client cache is best modeled by session timeout as opposed
  to capacity limited.</LI>
  <LI></LI>
</UL>

<H2><HR></H2>

<P><A HREF="index.html">Back to index</A> 
</BODY>
</HTML>
@


1.1.1.1
log
@
@
text
@@
