head	1.2;
access;
symbols;
locks; strict;
comment	@# @;


1.2
date	97.03.20.09.55.26;	author fox;	state Exp;
branches;
next	1.1;

1.1
date	97.02.20.18.46.05;	author fox;	state Exp;
branches;
next	;


desc
@@


1.2
log
@added adaptive credit based flow control
@
text
@<html>
<title>
End-to-End Packet Delay In the Internet
</title>
<body>
<a href="index.html"><i>Back to index</i></a>
<h1>
Characterizing End-to-End Packet Delay In the Internet
</h1>
<strong>
Jean-Chrysostome Bolot, INRIA
</strong>

<p><b>One-line summary:</b>
UDP roundtrip measurements and some clever analysis based on queueing
theory show  that end-to-end delays and packet losses are correlated at
short time scales, random at long time scales.
</p>

<h2>Overview/Main Points</h2>

<ul>
  <li> UDP ping-ponged between INRIA and UMD.  Each has unique timestamp
       and serial number.  Only roundtrip times were measured.
  <li> Analysis strategy: Much is known about system under study, so
       don't use plain autoregressive or moving-average models to study
       RTT's in isolation; instead, use a model of two streams entering
       a FIFO server queue, where one stream with a fixed delay models
       the fixed part of the probe RTT, the other stream is
       randomly-distributed Internet
       traffic, and the server models the variation in probe RTT.
  <li> For low traffic, W<sub>n+1</sub>=W<sub>n</sub>+e<sub>n</sub>, where
       W<sub>n</sub> is waiting time of packet n, e<sub>n</sub> is
       exponential random variable with small variance and mean=0.
       <ul>
         <li>Points will lie along the diagonal
              rtt<sub>n+1</sub>=rtt<sub>n</sub> in phase plot.
       </ul>
  <li> When d (interarrival time of probes) is small and other traffic
       is B (which is very heavy), then
       W<sub>n+1</sub>=W<sub>n</sub>+(B/m) where m is queue service
       rate, or rtt<sub>n+1</sub>-rtt<sub>n</sub>=B/m.  If no Internet
       traffic is received betwen probe packets n+1 and n+k, then probe
       packets n+1 thru n+k will leave the queue P/m seconds apart, so
       that rtt<sub>n+2</sub>-rtt<sub>n+1</sub>=P/m&nbsp;-&nbsp;d.
       <ul>
         <li> So points will lie along that "lower" diagonal on phase
              plot.
       </ul>
  <li> When d is very large, the maximum queueing delay is barely larger
       than the interarrival times, and the points are scattered all
       over the phase plane.
  <li> Using the Lindley recurrence
       W<sub>n+1</sub>=W<sub>n</sub>+Y<sub>n</sub>-X<sub>n</sub> (where
       Y is service time and X is interarrival time; see "graphical
       proof" in paper):
       <ul>
         <li> Assume that Internet traffic contributes b bits during
              slot n (i.e. time interval [nd,(n+1)d]).
         <li> Apply Lindley's recurrence twice and substitute, to get
              W<sub>n+1</sub>=W<sub>n</sub>&nbsp;+&nbsp;(P+b)/m&nbsp;-&nbsp;d,
              or rewriting,
              b<sub>n</sub>=m(W<sub>n+1</sub>-W<sub>n</sub>+d).  That
              is, we can estimate Internet traffic from the distribution
              of packet interarrival times!
         <li> Note that above property doesn't hold if the queue does
              not get a chance to empty during the time slot
              [nd,(n+1)d].  So we must make d small enough, i.e. make
              m*d smaller than some average value of b.
       </ul>
  <li> Applying this analysis to the observed distribution of the
       W<sub>n</sub> for d=20ms, we can see probes that got stuck
       behind a single FTP packet, probes that got stuck behind two FTP
       packets, etc.
  <li> As d gets bigger, correlations between successive packets get
       worse and graph loses some structure.  (For d=20ms, it's clearly
       multimodal; for d=100ms, it's noisier.)
  <li> <b>Packet loss analysis:</b> For large values of d, the
       unconditional  and conditional packet loss probs approach each
       other.  I.e. losses are random at large time scales, but
       reasonably well correlated (ulp and clp are quite distinct) at
       shorter scales.
  <li> Unexplained result: even for enormous d, ulp is observed to be as
       high as 10%.
  <li> Loss gap stays close to 1 even for large values of d!  So, e.g.,
       audio apps should rely on FEC to fix most lost-packet errors.
</ul>

<h2>Relevance</h2>

Another neat trick of indirect observation plus a clever model to deduce
end-to-end behavior properties for a large system.

<h2>Flaws</h2>

<ul>
  <li> Internet traffic offered during each queue slot is modeled as
       exponential random variable.  Is this reasonable?
</ul>
<hr>
<a href="index.html"><i>Back to index</i></a>

</body></html>
@


1.1
log
@*** empty log message ***
@
text
@d32 1
a32 1
  <li> For low traffic, W<sub>n+1</sub>=W<sub>n</sub>+e<sub>n</n>, where
@
