head     1.1;
branch   1.1.1;
access   ;
symbols  initial:1.1.1.1 initial:1.1.1;
locks    ; strict;
comment  @# @;


1.1
date     96.12.11.03.09.02;  author fox;  state Exp;
branches 1.1.1.1;
next     ;

1.1.1.1
date     96.12.11.03.09.02;  author fox;  state Exp;
branches ;
next     ;


desc
@@



1.1
log
@Initial revision
@
text
@<html>
<title>
Clock Synchronization
</title>
<body>
<a href=index.html><i>Back to index</i></a>
<h1>
A Probabilistic Approach to Distributed Clock Synchronization
</h1>
<strong>
Flavia Cristian
</strong>

<p><b>One-line summary:</b>
Provable, low bounds on clock synchronization precisions are easily
obtained with a relatively simply protocol and trivial computation.
</p>

<h2>Overview/Main Points</h2>

<UL>

<LI><B>Fundamentals</B>: The fundamental protocol is for a process P
to send a query message to process Q; process Q replies to P with a
message containing Q's clock time at the instant Q received P's
message.  We assume hardware clocks have drift rate <IMG
src="distrib_eqns/rho.gif">, implying the measurement of a time
interval t'-t has an error of at most
<img src="distrib_eqns/rho.gif">(t-t').  Assume further that the
minimum message transmission delay between P and Q is <i>min</i>.

<LI><B>Clock synch and max error:</B>  It is proven in the paper
that if P measures the round-trip time of the request/reply pair
as 2D, the value displayed by Q's clock when it sends
its reply is T, and the value displayed by Q's clock when P receives
this reply is <img src="distrib_eqns/cq.gif">, then the best guess
by P for <img src="distrib_eqns/cq.gif"> is: <BR>
<CENTER> <img src="distrib_eqns/p_guess.gif"> </CENTER>
with maximum error <BR>
<CENTER> <img src="distrib_eqns/p_error.gif">. </CENTER>

<LI><B>Obtaining a desired precision:</B> The precision of the clock
estimate is limited by the measured round-trip time of the reply/request
pair.  By timing out any requests that don't receive replies within
2U seconds, one can bound the imprecision of a given clock (at the
risk of requiring one to resend a clock resynchronization request) to
be e by by choosing a U of:
<CENTER> <img src="distrib_eqns/bound_u.gif">. </CENTER>
Because the minimum U one can pick is <img src="distrib_eqns/min_u.gif">
(because of the minimum delivery latency plus clock drift),
the best precision achievable by this method is
<img src="distrib_eqns/best_e.gif">.

<LI><B>Maintaining a desired precision:</B> Because the local clock
drifts, it must be resynchonized periodically to maintain a desired
precision.  Because delivery latency is potentially unbounded,
one must set a reply/request timeout, which implies a resynchronization
request could fail.  One can calculate the probability of N successive
request failures;  given a current precision, and a specifiable probability
of success and a specifiable desired precision, one can calculate the
delay between the periodic resynchronization attempts.

<LI><B>Detecting failures:</B> Hard bounds on possible imprecisions
are provable with this scheme.  If a calculated clock time falls outside
of those bounds, a clock failure (at the master or at the slave) has
occured.  Because a master has a reliable clock source, the master can
detect if its own clock as failed, and since masters update themselves more
frequently than slaves, a slave can deduce its own clock has failed if
such an erroneous condition occurs two times in a row without the master
declaring itself invalid.

</UL>

<h2>Relevance</h2>

This paper rigorously shows the design and implementation of a
probabilistic clock synchronization protocol.  Such a protocol is
extremely useful when building distributed systems, although the
list of assumptions given in this paper should be read very carefully.
Most of them are reasonable in a friendly environment...

<h2>Flaws</h2>

<UL>

<LI>The only flaw of this paper is that the extremely rigorous presentation
belies the fact that if any of the assumptions are violated, then all
certainty flies out the window.

</UL>

<hr>
<a href=index.html><i>Back to index</i></a>

</body></html>
@


1.1.1.1
log
@cs269 distrib systems summaries
@
text
@@
