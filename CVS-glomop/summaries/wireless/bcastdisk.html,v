head	1.2;
access;
symbols;
locks; strict;
comment	@# @;


1.2
date	97.09.15.18.09.49;	author fox;	state Exp;
branches;
next	1.1;

1.1
date	97.09.03.17.17.04;	author fox;	state Exp;
branches;
next	;


desc
@@


1.2
log
@added mitzenmachers paper and scalable timers
@
text
@<html>
<title>
Broadcast Disks
</title>
<body>
<a href=index.html><i>Back to index</i></a>
<h1>
Broadcast Disks: Data management for asymmetric communication
environments.
</h1>
<strong>
 S. Acharya, R. Alonso, M. Franklin, S. Zdonik.  This summary also
includes material from <i>Dissemination based data delivery using
broadcast disks</i>, same authors minus Franklin.
</strong>

<p><b>One-line summary:</b>
Multilevel (but not truly "hierarchical") carouseling of multiple
broadcast "disks" onto a single channel; constructing the program and
managing the client caches must
be considered in tandem, since not all uncached pages are equidistant
from client.  A caching strategy of LRU-per-disk works well in practice.
</p>

<h2>Overview/Main Points</h2>

<ul>
  <li> Conceptual notion: partition broadcast channel into "slots" and
       treat channel as multiple spinning "disks" of different sizes.
       Listeners can read data when it passes under the head.  Differing
       disk sizes allow hot/cold pages to be scheduled differently.
  <li> Research goals:
       <ul>
         <li> How to organize data on the broadcast disks? ("construct
              the broadcast program")
         <li> How should clients manage their local caches?
         <li> Simplifying Assumptions: static client population;
              read-only data; no  upstream communication.
       </ul>
  <li> Benefit of "Non-flat" (multiple-disk) broadcast programs over  a
       "randomly skewed" single-disk program:
       <ul>
         <li> increasingly optimal for clients as the variance of access
              probabilities for different pages grows.
         <li> Prob. of request arriving during a large gap is greater
              than during a short gap <i>(Ed.: only for Gaussian-mean
              arrival model, right?)</i>
         <li> Unpredictability of arrival time for a particular page
              disallows "sleeping" to conserve client power.
       </ul>
  <li> Creating a multidisk broadcast program: bin pages by expected
       relative frequency of client access, and split each bin (disk)
       with a granularity that is the least common denominator of
       relative frequencies; then interleave chunks.
       <ul>
         <li> Choose relative frequencies with care to avoid extremely
              long "major cycle period" times.
       </ul>
  <li> Cache management on client: Not all missed pages are
       equidistant.  
       <ul>
         <li> Goal: store pages for which local access prob. P is greater
              than frequency of broadcast X (cost-based replacement).
         <li> Implementing "PIX policy" (P inverse X) requires perfect
              knowledge, so instead use LIX: LRU with one chain per
              bcast disk, keeping an EWMA of probability that page will
              be accessed in the next timeslot =
              1/(currentTime-lastAccessTime).  Usually performs within a
              constant of PIX.
       </ul>
  <li> Prefetching: (from "Data dissemination" paper)
       <ul>
         <li> Tag team caching: X and Y are pages on the same bcast
              disk.  Cache X until Y goes by; then replace X with Y.
              Expected cost is 0.125 (half the expected cost of the
              simple "demand fetching" strategy).  Problem of assigning
              pages to "tag teams" is a discrete bin packing problem.
         <li> PT value: product of probability P of page access and
              amount of time T that will elapse until that page appears
              on the broadcast.  PT values change with each "tick" of
              broadcast; it looks like a sawtooth, since when the page
              arrives, T changes discontinuously from its minimum to its
              maximum.
       </ul>       
</ul>

<h2>Relevance</h2>


<h2>Flaws</h2>

I think more attention should be focused on application level
       considerations: what is a "page" from the point of view of
       typical apps?  How do large "logical pages" get subdivided into
       physical pages (application level framing) and can you exploit
       application semantics to determine how to schedule the fragments?
       Can you analyze application usage/traffic to help construct the
       bcast schedule?  I recognize that this paper represents early
       work so maybe the authors will do this as they relax their
       simplifing assumptions.<p>

Also the relationship to NUMA work was mentioned in passing but I'm
surprised they weren't able to draw more from it, or at least state the
differences (e.g. duality between compiler-directed static data
placement for NUMA programs and constructing a static broadcast schedule).

<hr>
<a href=index.html><i>Back to index</i></a>

</body></html>
@


1.1
log
@*** empty log message ***
@
text
@d72 13
a84 1
       
@
