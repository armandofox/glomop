head	1.25;
access;
symbols
	initial:1.1.1.1
	initial:1.1.1;
locks; strict;
comment	@% @;


1.25
date	98.06.03.01.15.26;	author yatin;	state Exp;
branches;
next	1.24;

1.24
date	98.05.19.18.19.46;	author yatin;	state Exp;
branches;
next	1.23;

1.23
date	98.05.15.00.48.54;	author gribble;	state Exp;
branches;
next	1.22;

1.22
date	98.05.15.00.41.21;	author fox;	state Exp;
branches;
next	1.21;

1.21
date	98.05.08.22.22.35;	author gribble;	state Exp;
branches;
next	1.20;

1.20
date	98.05.08.17.47.52;	author fox;	state Exp;
branches;
next	1.19;

1.19
date	98.03.02.01.49.15;	author fox;	state Exp;
branches;
next	1.18;

1.18
date	98.03.01.02.27.45;	author gribble;	state Exp;
branches;
next	1.17;

1.17
date	98.03.01.00.52.08;	author fox;	state Exp;
branches;
next	1.16;

1.16
date	98.02.28.22.29.56;	author fox;	state Exp;
branches;
next	1.15;

1.15
date	98.02.28.04.46.47;	author fox;	state Exp;
branches;
next	1.14;

1.14
date	98.02.28.03.29.56;	author gribble;	state Exp;
branches;
next	1.13;

1.13
date	98.02.28.03.24.30;	author fox;	state Exp;
branches;
next	1.12;

1.12
date	98.02.28.02.22.20;	author fox;	state Exp;
branches;
next	1.11;

1.11
date	98.02.27.21.02.46;	author gribble;	state Exp;
branches;
next	1.10;

1.10
date	98.02.27.19.37.04;	author gribble;	state Exp;
branches;
next	1.9;

1.9
date	98.02.27.09.52.00;	author gribble;	state Exp;
branches;
next	1.8;

1.8
date	98.02.26.21.57.27;	author gribble;	state Exp;
branches;
next	1.7;

1.7
date	98.02.26.21.49.47;	author gribble;	state Exp;
branches;
next	1.6;

1.6
date	98.02.26.19.05.10;	author gribble;	state Exp;
branches;
next	1.5;

1.5
date	98.02.19.23.33.47;	author fox;	state Exp;
branches;
next	1.4;

1.4
date	98.02.18.22.29.50;	author gribble;	state Exp;
branches;
next	1.3;

1.3
date	98.02.18.20.10.58;	author gribble;	state Exp;
branches;
next	1.2;

1.2
date	98.02.18.09.15.42;	author fox;	state Exp;
branches;
next	1.1;

1.1
date	98.02.12.02.15.24;	author fox;	state Exp;
branches
	1.1.1.1;
next	;

1.1.1.1
date	98.02.12.02.15.24;	author fox;	state Exp;
branches;
next	;


desc
@@


1.25
log
@1) In the first sentence of section 3, "In order to to ..." should be "In order
to ...".
@
text
@\section{Scalable Internet Application Servers}
\label{scaling}

In order to accommodate compute-intensive adaptation techniques
by putting resources in the network infrastructure, we must address 
two important challenges:

\begin{enumerate}
\item Infrastructural resources are typically shared, and the sizes of user communities sharing resources such as
Internet Points of Presence (POP's) is increasing exponentially.  A shared
infrastructural service must therefore {\em scale} gracefully to serve
large numbers of users.

\item Infrastructural resources such as the IP routing infrastructure
are expected to be reliable, with availability approaching $24\times 7$
operation.  If we place application-level computing resources such as
distillation engines into this infrastructure, we should be prepared to meet
comparable expectations.

\end{enumerate}

In this section, we focus on the problem of deploying adaptation-based
proxy services to large communities (tens of thousands of users,
representative of the subscription size of a medium-sized Internet
Service Provider).  In
particular, we discuss a cluster-friendly programming model for building
interactive and adaptive Internet services, and measurements of our
implemented prototype of a scalable, cluster-based server that
instantiates the model.  Our framework reflects the implementation of
three real services in use today: TranSend, a scalable transformation
proxy for the 25,000 UC Berkeley dialup users (connecting through a bank
of 600 modems); Top Gun Wingman, the only graphical Web browser for the
3Com PalmPilot handheld device (commercialized by ProxiNet);
and the Inktomi search
engine (commercialized as HotBot), which performs over 10 million queries
per day against a database of over 100 million web pages.  Although
HotBot does not demonstrate client adaptation, we use it to validate
particular design decisions in the implementation of our server
platform, since it pioneered many of the cluster-based scalability
techniques generalized in our scalable server prototype.

We focus our detailed discussion and measurements on TranSend, a
transformational proxy service that performs on-the-fly lossy image
compression.  TranSend applies the ideas explored in the preceding
section to the World Wide Web.

\subsection{TACC: A Programming Model for Internet Services}

We focus on a particular subset of Internet services, based on
{\em transformation} (distillation, filtering, format conversion, etc.),
{\em aggregation} (collecting and collating data from various sources,
as search engines do), {\em caching} (both original and transformed
content), and {\em customization}  (maintenance of a per-user
preferences database that allows workers to tailor their output to the
user's needs or device characteristics).

We refer to this model as TACC, from the initials of the four elements
above.
In the TACC model, applications are built from building
blocks interconnected with simple API's.  Each building block, or {\em
worker}, specializes in a particular task, for example,
scaling/dithering of images in a particular format, conversion between
specific data formats, extracting ``landmark'' information from specific
Web pages, 
etc.  Complete applications are built by {\em composing} workers;
roughly speaking, one worker can {\em chain} to another (similar to
processes in a Unix pipeline), or a worker can {\em call} another as a
subroutine or coroutine.  This model of composition results in a very
general programming model that subsumes transformation proxies
\cite{transend}, proxy filters \cite{proxyfilter}, customized
information aggregators, and search engines.

A {\em TACC server} is a platform that instantiates TACC workers,
provides dispatch rules for routing
network data traffic to and from them, and provides support for the
inter-worker calling and chaining API's.  Similar to a Unix shell, 
a TACC server  provides the
mechanisms that insulate workers from having to deal directly with
low-level concerns such as data routing and exception handling, and
gives workers a clean set of API's for communicating with each other,
the caches, and the customization database (described below).
We describe our prototype implementation of a scalable,
commodity-PC cluster-based TACC server later in this section.

% The tasks done by TACC workers can be grouped into four largely
% orthogonal categories:

% \begin{itemize}

% \item \textbf{Transformation} is an
% operation on a single data object that changes its content; examples
% include filtering, transcoding, re-rendering, encryption, and
% distillation.

% \item \textbf{Aggregation} involves collecting data from several objects
% and collating it in a prespecified way; for example, collecting all
% listings of cultural events from a prespecified set of Web pages,
% extracting the date and event information from each, and composing the
% result into a dynamically-generated ``culture this week'' page.  As a
% simple example, search engines deliver the result of aggregating over an
% offline-generated crawler database.

% \item \textbf{Customization}, or the ability to {\em personalize} the
% delivery of content and services, represents a fundamental advantage of
% the Internet over 
% traditional wide-area media such as television.   The TACC API's call for
% user-profile information from a customization  database
% to be automatically
% delivered to workers along with the input data for a particular 
% request, allowing the worker's operation to be tailored for each user.
% For example, the same image-compression worker can be run with one
% set of parameters to reduce image resolution for faster Web browsing,
% and a different set of parameters to reduce image size and bit depth for
% handheld devices.  Similarly, an HTML markup worker can consider user
% profile information in generating a ``novice'' vs. 
% ``expert'' user interface to a service.

% \item \textbf{Caching} is important because even with a cache as small
% as $0.5$ GB, storing data (or recomputing it) is cheaper than moving it
% across the Internet \cite{uk_cache,singnet}.  In the TACC model, caches
% can store post-transformation (or post-aggregation) content 
% in addition to original Internet content.

% \end{itemize}

\subsection{Cluster-Based TACC Server Architecture}

We observe
that clusters of workstations have some fundamental properties that can
be exploited to meet the requirements of a large-scale network services
(scalability, high availability, and cost effectiveness).  Using
commodity PCs as the unit
of scaling allows the service to ride the leading edge of the
cost/performance curve; the inherent redundancy of clusters can be used
to mask transient failures; and ``embarrassingly parallel'' network
service workloads map well onto networks of commodity workstations.

However, developing cluster software and administering a running cluster
remain complex.  A primary contribution of our work is the design,
analysis, and implementation of a layered framework for building
adaptive network services that addresses this complexity while realizing
the sought-after economies of scale.  New services can use this
framework as an off-the-shelf solution to scalability, availability, and
several other problems, and focus instead on the content of the service
being developed. 

We now describe our proposed system architecture and service-programming
model for building scalable TACC servers using clusters of
PC's.  The architecture attempts to address the challenges of
cluster computing (unwieldy administration, managing partial failures,
and the lack of shared state across components) while exploiting the
strengths of cluster computing  (support for incremental scalability, high
availability through redundancy, and the ability to use commodity
building blocks).  A more detailed discussion of the architecture can be
found in \cite{sosp16}.

The goal of our architecture is to separate the {\em content} of network
services (i.e., what the 
services do)  from their implementation, by encapsulating the
``scalable network service'' (SNS) requirements of high availability,
scalability, and fault tolerance in a reusable layer with narrow
interfaces.  Application writers program to the TACC APIs alluded to in
the previous section, without regard to the underlying TACC server
implementation; the resulting TACC applications automatically receive
the benefits of linear scaling, high availability, and failure
management when run on our cluster-based TACC server.

\begin{figure}[tbh]
\epsfxsize 0.95\hsize
\begin{center}
\makebox{
\epsfbox[0 0 340 303]{./figures/arch.eps}
}
\end{center}
\caption{{\bf Architecture of a cluster-based TACC server}:  Components
include front 
ends (FE), a pool of TACC workers (W) some of which may be caches (\$),
% $ to make emacs happy!!
a user profile database, a graphical monitor, and a fault-tolerant load
manager, whose functionality logically extends into the manager stubs
(MS) and worker stubs (WS).}\label{arch_sns}
\end{figure}

The software-component block diagram of a scalable TACC server is shown
in figure 
\ref{arch_sns}.  Each physical workstation in a network of workstations
(NOW \cite{Ande95b}) supports one or more software components in the
figure, but each component in the diagram is confined to one node.  In
general, the components whose tasks are naturally parallelizable are
replicated for scalability, fault tolerance, or both.

{\bf Front Ends} provide the interface to the TACC server as seen by the
outside world (e.g., HTTP server).  They ``shepherd'' incoming requests
by matching them up with the appropriate user profile from the
customization database, and queueing them for service by one or more
workers.  Front ends maximize system throughput by maintaining state for
many simultaneous outstanding requests, and can be replicated for both
scalability and availability.

The {\bf Worker Pool} consists of caches (currently Harvest \cite{harvest})
and service-specific modules
that implement the actual service (data transformation/filtering,
content aggregation, etc.) Each type of module may be instantiated zero
or more times, depending on offered load.  The TACC API allows all cache
workers to be managed as a single virtual cache by providing URL hashing,
automatic failover, and dynamic growth of the cache pool.

The {\bf Customization Database} stores user profiles that allow mass
customization of request processing.  The Manager balances load across
workers and spawns additional workers as offered load fluctuates or faults
occur.  When necessary, it may assign work to machines in the overflow pool,
a set of backup machines (perhaps on desktops) that can be harnessed to
handle load bursts and provide a smooth transition during incremental
growth.

The {\bf Load Balancing/Fault Tolerance manager} keeps track of what
workers are running where, autostarts new workers as needed, and
balances load across workers.  Its detailed operation is described in
section \ref{meas}, in the context of the TranSend implementation.
Although it is a centralized agent, \cite{sosp16} describes the various
mechanisms, including multicast heartbeat and process-peer fault
tolerance, that keep this and other system components running and allow
the system to survive transient component failures.

The {\bf Graphical Monitor} for system management supports asynchronous
error notification via email or pager, temporary disabling of system
components for hot upgrades, and visualization of the system's behavior
using Tcl/Tk \cite{tcltk}.  The benefits of visualization are not just
cosmetic: We can immediately detect by looking at the visualization
panel what state the system as a whole is in, whether any component is
currently causing a bottleneck (such as cache-miss time, distillation
queueing delay, interconnect), what resources the system is using, and
similar behaviors of interest.

The {\bf Interconnect} provides a low-latency, high-bandwidth, scalable
interconnect, such as switched 100-Mb/s Ethernet or Myrinet \cite{MYRI_95}.
Its main goal is to prevent the interconnect from becoming the bottleneck
as the system scales.

%\subsubsection{Scalability}

Components in our TACC server architecture may be replicated for fault tolerance or
high availability, but we also use replication to achieve scalability.  When
the offered load to the system saturates the capacity of some component
class, more instances of that component can be launched on incrementally
added nodes.  The duties of our replicated components are largely
independent of each other (because of the nature of the Internet services'
workload), which means the amount of additional resources required is a
linear function of the increase in offered load.

%\subsubsection{Soft State for Fault Tolerance and Availability}

%The technique of constructing robust entities by relying on cached soft
%state refreshed by periodic messages from peers has been enormously
%successful in wide-area TCP/IP networks \cite{tcp_imp, DEER_94,
%MCQUILL_80}, another arena in which transient component failure is a fact
%of life.  Correspondingly, our SNS components operate in this manner, and
%monitor one another using process peer fault tolerance: when a component
%fails, one of its peers restarts it (on a different node if necessary),
%while cached stale state carries the surviving components through the
%failure.  After the component is restarted, it gradually rebuilds its soft
%state, typically by listening to multicasts from other components.

\subsection{Analysis of the TranSend Implementation}\label{meas}

TranSend \cite{transend}, a TACC reimplementation of our earlier
prototype called Pythia \cite{pythia}, performs lossy Web image
compression on the fly.  Each TranSend worker handles compression or
markup for a specific MIME type; objects of unsupported types are passed
through to the user unaltered.\footnote{The PostScript-to-richtext
worker described in section \ref{distill} has not yet been added to
TranSend.}

We took measurements of TranSend using a cluster of 15 Sun SPARC Ultra-1
workstations connected by 100 Mb/s switched Ethernet and isolated from
external load or network traffic.  For measurements requiring Internet
access, the access was via a 10 Mb/s switched Ethernet network connecting
our workstation to the outside world.  Many of the performance tests are
based upon HTTP trace data from the 25,000 UC Berkeley
dialup IP users \cite{grib_97}, played back using a high-performance
playback engine of our own design that can either generate requests at a
constant rate or faithfully play back a
trace according to the timestamps in the trace file.

In the following subsections we
report on experiments that stress TranSend's fault tolerance,
responsiveness, and scalability.

\subsubsection{Self Tuning and Load Balancing}

As mentioned previously, the load balancing and fault tolerance manager is
charged with spawning and reaping workers and distributing internal load
across them.  The mechanisms by which this is accomplished, which include
monitoring worker queue lengths and applying some simple hysteresis, are
described in \cite{sosp16}.

% TranSend uses queue lengths at the workers as a metric for load
% balancing.  As queue lengths grow due to increased load, the moving average
% of the queue length maintained by the manager starts increasing; when the
% average crosses a configurable threshold, $H$, the manager spawns a new
% worker to absorb the load.  The threshold $H$ maps to the greatest delay
% the user is willing to tolerate when the system is under high load.  To
% allow the new worker to stabilize the system, the spawning mechanism is
% disabled for $D$ seconds; the parameter $D$ represents a tradeoff between
% stability (rate of spawning and reaping workers) and user-perceptible
% delay.

\begin{figure}[tbh]
\epsfxsize 0.95\hsize
\begin{center}
\makebox{
\epsfbox[0 0 438 294]{./figures/distq1.eps}
}
\end{center}
\caption{Worker queue lengths observed over time as the load presented
to the system fluctuates, and as workers are manually brought down.}
\label{distq1}
\end{figure}

% \begin{figure}[tbh]
% \epsfxsize 0.95\hsize
% \begin{center}
% \makebox{
% \epsfbox[0 0 438 294]{./figures/distq2.eps}
% }
% \end{center}
% \caption{An enlarged view of figure \ref{distq1}.}
% \label{distq2}
% \end{figure}

Figure \ref{distq1} shows the variation in worker queue lengths over
time.  The system was bootstrapped with one front end and the manager,
and a single demand-spawned worker.  Continuously increasing the load
caused the manager to spawn a second and later a third worker. 
% With increasing load, the worker
% queue gradually increased until the manager decided to spawn a second
% worker, which reduced the queue length of the first worker and balanced
% the load across both workers within five seconds.  Continued increase in
% load caused a third worker to start up, which again reduced and balanced
% the queue lengths within five seconds.
%  Figure \ref{distq2} shows an enlarged view of the
% graph in figure \ref{distq1}.  
% During the experiment, 
We then manually killed
the first two workers; the sudden load increase on the remaining worker 
caused the manager to spawn one and later another new worker, to
stabilize the queue lengths.
% worker.  Even after $D$ seconds, the manager discovered that the system
% was overloaded and started up one more worker, causing the load to
% stabilize.

\subsubsection{Scalability}

%To demonstrate the scalability of the system, we needed to eliminate two
%bottlenecks that limit the load we could offer: the overhead associated
%with having a very large number of open file descriptors, and the
%bottleneck 10Mb/s Ethernet connecting our cluster to the Internet.  To do
%this, we prepared a trace file that repeatedly requested a fixed number of
%JPEG images, all approximately 10KB in size, based on the distributions we
%observed.  These images would then remain resident in the cache partitions,
%eliminating cache miss penalty and the resulting buildup of file
%descriptors in the front end.  We recognize that although a non-zero cache
%miss penalty does not introduce any additional network, stable storage, or
%computational burden on the system, it does result in an increase in the
%amount of state in the front end, which limits the performance of a single
%front end.  On the other hand, by turning off caching of distilled images,
%we force our system to re-distill the image every time it was requested,
%and in that respect our measurements are pessimistic relative to the
%system's normal mode of operation.

To demonstrate the scalability of the system, we performed the following
experiment:

\begin{enumerate} 

\item We began with a minimal instance of the system: one front
end, one worker, the manager, and a fixed number of cache
partitions.  (Since for these experiments we repeatedly requested the same
subset of images, the cache was effectively not tested.)

\item We increased the offered load until some system component saturated
(e.g., worker queues growing too long, front ends no longer accepting
additional connections, etc.).

\item We then added more resources to the system to eliminate this
saturation (in many cases the system does this automatically, as when it
recruits overflow nodes to run more workers), and we recorded the amount of
resources added as a function of the increase in offered load, measured in
requests per second.

\item We continued until the saturated resource could not be replenished
(i.e., we ran out of hardware), or until adding more of the saturated
resource no longer resulted in a linear or close-to-linear improvement in
performance.

\end{enumerate}

\begin{table}[htbp]
\centering
\begin{tabular}{|c||c|c|c|} \hline

{\bf Req./} & {\bf \# } & {\bf \#} & {\bf Element that} \\
{\bf Second} & {\bf FE's} & {\bf Wkrs.} & {\bf saturated} \\ \hline \hline
0-24 & 1 & 1 & workers \\ \hline
25-47 & 1 & 2 & workers \\ \hline
48-72 & 1 & 3 & workers \\ \hline
73-87 & 1 & 4 & FE Ethernet \\ \hline
88-91 & 2 & 4 & workers \\ \hline
92-112 & 2 & 5 & workers \\ \hline
113-135 & 2 & 6 & workers + \\
        &   &   & FE Ethernet \\ \hline
136-159 & 3 & 7 & workers \\ \hline

\end{tabular}
\caption{Results of the scalability experiment.  ``FE'' refers to front end.}
\label{scal_tab}
\end{table}

Table \ref{scal_tab} presents the results of this experiment.  At 24
requests per second, as the offered load exceeded the capacity of the
single available worker, the manager automatically spawned one
additional worker, and then subsequent workers as necessary.  
(In addition to using faster hardware, the performance-engineering of
the cluster-based server has caused a large reduction in the amortized
cost of distillation for a typical 
image, compared to the values suggested by Figure
\ref{dist_figures}.)
At 87
requests per second, the Ethernet segment leading into the front end
saturated, requiring a new front end to be spawned.  We were unable to test
the system at rates higher than 159 requests per second, as all of our
cluster's machines were hosting workers, front ends, or playback
engines.  We did observe nearly perfectly linear growth of the system over
the scaled range: a worker can handle approximately 23 requests per
second, and a 100 Mb/s Ethernet segment into a front-end can handle
approximately 70 requests per second.  We were unable to saturate the front
end, the cache partitions, or fully saturate the interior interconnect
during this
experiment.  We draw two conclusions from this result:

\begin{itemize}

\item  Even with a commodity 100 Mb/s interconnect, linear scaling is limited
primarily by bandwidth into the system rather than bandwidth inside the
system.

\item Although we originally deployed
TranSend on four SPARC 10's, a single Ultra-1 class
machine would suffice to serve the entire dialup IP population of UC
Berkeley (25,000 users officially, over 8000 of whom surfed during the
trace).

\end{itemize}

@


1.24
log
@fixed typos
@
text
@d4 1
a4 1
In order to to accommodate compute-intensive adaptation techniques
@


1.23
log
@Final changes
@
text
@d4 3
a6 3
If our goal is to accommodate compute-intensive adaptation techniques
by putting resources in the network infrastructure, two challenges
immediately arise:
d9 1
a9 1
\item Infrastructural resources are naturally shared, and the sizes of user communities sharing resources such as
d17 1
a17 1
distillation into this infrastructure, we should be prepared to meet
d132 1
a132 1
commodity PC's as the unit
d162 1
a162 1
interfaces.  Application writers program to the TACC API's alluded to in
d423 4
a426 3
(One result of performance-engineering the cluster-based server
is a large reduction in the amortized cost of distillation for a typical
image, compared to the values suggested by figure
@


1.22
log
@*** empty log message ***
@
text
@d9 4
a12 5
\item Infrastructural resources are shared
by nature, and the sizes of user communities sharing resources such as
Internet Points of Presence (POP's) is increasing exponentially.  A
shared infrastructural service must therefore {\em scale} gracefully to
server large numbers of users.
d205 1
a205 1
workers to be managed as a single virtual cache, by providing URL hashing,
d209 1
a209 1
customization of request processing.The Manager balances load across
@


1.21
log
@reviewer B changes
@
text
@d36 1
a36 1
engine (commercialized as HotBot), which performs millions of queries
@


1.20
log
@*** empty log message ***
@
text
@d34 1
a34 1
USR PalmPilot handheld device (commercialized by ProxiNet);
@


1.19
log
@first draft sent to Richard LaMaire
@
text
@d4 18
d25 2
a26 1
representative of the subscription size of a medium-sized ISP).  In
d34 2
a35 1
USR PalmPilot handheld device; and the Inktomi search
d37 1
a37 1
per day against a database of over 50 million web pages.  Although
d236 1
a236 1
The {\bf System-Area Network} provides a low-latency, high-bandwidth, scalable
d423 6
a428 1
additional worker, and then subsequent workers as necessary.  At 87
d437 2
a438 1
end, the cache partitions, or fully saturate the interior SAN during this
d443 1
a443 1
\item  Even with a commodity 100 Mb/s SAN, linear scaling is limited
d447 2
a448 1
\item Although we run TranSend on four SPARC 10's, a single Ultra-1 class
@


1.18
log
@More tersification.
@
text
@d133 1
a133 1
strengths cluster computing  (support for incremental scalability, high
d147 1
a147 1
management provided by the SNS layer.
d181 2
a182 1
The {\bf Worker Pool} consists of caches and service-specific modules
d197 8
a204 5
The {\bf Load Balancing/Fault Tolerance manager} is a centralized
agent that keeps track of what workers are running where, autostarts new
workers as needed, and balances load across workers.  Its detailed
operation is described in section \ref{meas}, in the context of the TranSend
implementation.
d214 1
a214 1
other such figures of interest.
@


1.17
log
@*** empty log message ***
@
text
@d268 5
a272 6
As mentioned previously, the load balancing and fault tolerance manager
is charged with spawning and reaping workers and distributing internal
load across them.  The mechanisms by which this is accomplished, which
include monitoring worker queue lengths and applying some simple
hysteresis, are 
described in \cite{sosp16}.  
@


1.16
log
@*** empty log message ***
@
text
@d31 10
a40 3
{\em transformation, aggregation, caching,}\/ and {\em customization} of
Internet 
content (TACC).  In the TACC model, applications are built from building
d66 2
a67 2
The tasks done by TACC workers can be grouped into four largely
orthogonal categories:
d69 1
a69 1
\begin{itemize}
d71 33
a103 33
\item \textbf{Transformation} is an
operation on a single data object that changes its content; examples
include filtering, transcoding, re-rendering, encryption, and
distillation.

\item \textbf{Aggregation} involves collecting data from several objects
and collating it in a prespecified way; for example, collecting all
listings of cultural events from a prespecified set of Web pages,
extracting the date and event information from each, and composing the
result into a dynamically-generated ``culture this week'' page.  As a
simple example, search engines deliver the result of aggregating over an
offline-generated crawler database.

\item \textbf{Customization}, or the ability to {\em personalize} the
delivery of content and services, represents a fundamental advantage of
the Internet over 
traditional wide-area media such as television.   The TACC API's call for
user-profile information from a customization  database
to be automatically
delivered to workers along with the input data for a particular 
request, allowing the worker's operation to be tailored for each user.
For example, the same image-compression worker can be run with one
set of parameters to reduce image resolution for faster Web browsing,
and a different set of parameters to reduce image size and bit depth for
handheld devices.  Similarly, an HTML markup worker can consider user
profile information in generating a ``novice'' vs. 
``expert'' user interface to a service.

\item \textbf{Caching} is important because even with a cache as small
as $0.5$ GB, storing data (or recomputing it) is cheaper than moving it
across the Internet \cite{uk_cache,singnet}.  In the TACC model, caches
can store post-transformation (or post-aggregation) content 
in addition to original Internet content.
d105 1
a105 1
\end{itemize}
d270 15
a284 13
load across them.  The mechanisms by which this is accomplished are
described in \ref{sosp16}.  

TranSend uses queue lengths at the workers as a metric for load
balancing.  As queue lengths grow due to increased load, the moving average
of the queue length maintained by the manager starts increasing; when the
average crosses a configurable threshold $H$, the manager spawns a new
worker to absorb the load.  The threshold $H$ maps to the greatest delay
the user is willing to tolerate when the system is under high load.  To
allow the new worker to stabilize the system, the spawning mechanism is
disabled for $D$ seconds; the parameter $D$ represents a tradeoff between
stability (rate of spawning and reaping workers) and user-perceptible
delay.
d310 9
a318 8
time.  The system was bootstrapped with one front end and the manager.
On-demand spawning of the first worker was observed as soon as load was
offered.  With increasing load, the worker queue gradually increased
until the manager decided to spawn a second worker, which reduced the
queue length of the first worker and balanced the load across both
workers within five seconds.  Continued increase in load caused a third
worker to start up, which again reduced and balanced the queue lengths
within five seconds.
d321 8
a328 6
During the experiment, we manually killed
the first two workers, causing the load on the remaining worker to
rapidly increase.  The manager immediately reacted and started up a new
worker.  Even after $D$ seconds, the manager discovered that the system
was overloaded and started up one more worker, causing the load to
stabilize.
@


1.15
log
@*** empty log message ***
@
text
@d4 23
a26 22
In this section we discuss a programming model for building interactive
Internet services, and our prototype implementation of
a scalable, cluster-based server that instantiates the model.
Our framework reflects the
implementation of three real services in use today: TranSend, a
scalable transformation proxy for the 25,000 UC Berkeley dialup
users (connecting through a bank of 600 modems); Top Gun Wingman, the
only graphical Web browser for the PalmPilot PDA (Personal Digital
Assistant); and the Inktomi search engine
(commercialized as HotBot), which performs millions of queries per day
against a database of over 50 million web pages.  Although HotBot does
not demonstrate client adaptation, we use it to validate particular
design decisions in the implementation of our server platform, since
it pioneered many of the cluster-based scalability techniques
generalized in our scalable server prototype.

We focus our detailed discussion and measurements on TranSend, which
provides Web caching and data transformation.  In particular, real-time,
datatype-specific distillation and refinement \cite{pythia} of inline Web
images results in an end-to-end latency reduction by a factor of 3--5,
giving the user a much more responsive Web surfing experience with only
modest image quality degradation.  
d30 3
a32 5
We have developed a simple but flexible programming model for
structuring interactive Internet services, including the
datatype-specific distillation services described in section
\ref{distill}.  We focus on a particular subset of services, based on
transformation, aggregation, caching, and customization of Internet
d37 2
a38 1
specific data formats, extracting ``landmark'' information from specific Web pages,
d64 1
a64 1
\item {\textbf Transformation} is an
d69 1
a69 1
\item {\textbf Aggregation} involves collecting data from several objects
d77 3
a79 7
% In this
% sense, aggregation subsumes transformation, but we believe it is worth
% keeping them separate in order to emphasize the interesting and
% less-complex class of pure transformation-based services.)

\item {\textbf Customization}, or the ability to {\em personalize} the
delivery of content and services, represents a fundamental advantage of the Internet over
d92 1
a92 1
\item {\textbf Caching} is important because even with a cache as small
d102 19
d149 2
a150 1
\caption{{\bf Architecture of a cluster-based TACC server}:  Components include front
d153 3
a155 3
a user profile database, a graphical monitor, and a fault-tolerant load manager,
whose functionality logically extends into the manager stubs (MS) and
worker stubs (WS).}\label{arch_sns}
d158 2
a159 1
The software-component block diagram of a scalable TACC server is shown in figure
d166 7
a172 6
{\bf Front Ends} provide the interface to the TACC server as seen by the outside
world (e.g., HTTP server).  They ``shepherd'' incoming requests by matching
them up with the appropriate user profile from the customization database,
and queueing them for service by one or more workers.  Front ends maximize
system throughput by maintaining state for many simultaneous outstanding
requests, and can be replicated for both scalability and availability.
d234 1
a234 32
\subsection{TranSend as a TACC Application}

TranSend is one of the simplest TACC applications we have produced,
essentially a TACC reimplementation of our earlier prototype called
Pythia \cite{pythia}.
Each TranSend worker handles compression or markup for a specific MIME
type; objects of unsupported types are passed through to the user
unaltered.\footnote{The PostScript-to-richtext worker described in
section \ref{distill} has not yet been added to TranSend.}
The dispatch rules simply match the MIME type of the object
returned from the origin server to the list of known workers, which (as
in all TACC applications) can be updated dynamically.  In particular,
TranSend does not exploit TACC's ability to compose workers by chaining
them into a ``pipeline'' or having one worker call others as
coroutines.  

In addition to user-selectable lossy compression levels, TranSend's workers
convert most GIF images to the JPEG format using an off-the-shelf
library \cite{jpeg-6a} (since JPEG often compresses 
more gracefully), reduce colormap size to at most 256 at most levels of
compression, insert HTML markup or JavaScript to allow users to click
through to the original images, use Java and JavaScript to provide a
``floating palette'' user interface for changing user profile settings
\cite{uist97}, and optionally ``sanitize'' request headers to provide
some of the same functionality as Anonymizer Surfing \cite{anonymizer}.

Transformed objects are stored in the cache with ``fat
URL's'' that encode the transformation parameters, saving the work of
re-transforming an original should another user ask for the same
degraded version later.  Each user can select a desired level of
aggressiveness for the lossy compression and turn header sanitizing on
and off.
d236 7
a242 1
\subsection{Analysis of the TranSend Implementation}\label{meas}
d261 4
a264 17
As mentioned previously, when a front end has a task for a worker, the
manager stub code inside the front end contacts the load balancing/fault
tolerance manager, which locates
an appropriate worker, spawning a new one if necessary.  The manager
stub caches the new worker's location for future requests.  Worker stub
code attached to each worker accepts and queues requests on behalf of
the worker and periodically reports load information to the manager.
The manager aggregates load information from all workers, computes
weighted moving averages, and piggybacks the resulting information on its
beacons to the manager stub.  The manager stub at the front end caches
the information in these beacons and uses lottery scheduling
\cite{Wal94} to select a worker for each request.  The cached
information provides a backup so that the system can continue to operate
(using slightly stale load data) even if the manager crashes.  To allow the
system to scale as the load increases, the manager can automatically spawn
a new worker on an unused node if it detects excessive load on
workers of a particular class.
@


1.14
log
@Commented out distillation performance results - redundant with
distill.tex results.
@
text
@d25 1
a25 3
modest image quality degradation.  TranSend was developed at UC Berkeley
and has been deployed for the 25,000 dialup IP users there, and is being
deployed similar communities at UC Davis and various other sites.
a27 1
\label{taccapps}
d38 1
a38 1
data formats, extracting landmark information from specific Web pages,
d47 2
a48 1
A {\em TACC server} is a platform that instantiates TACC workers, routes
a58 2
\subsubsection{TACC Elements}

d67 2
a68 1
compression.  
d92 3
a94 1
handheld devices.  
d104 1
a104 1
\subsection{Cluster-Based Scalable Service Architecture}
d113 2
a114 1
building blocks).  
d116 3
a118 2
In the architecture, the content of network services (i.e., what the
services do) is separated from their implementation, by encapsulating the
d121 1
a121 1
interfaces.  Application writers program to the TACC model described in
d123 3
a125 1
implementation. 
d135 3
a137 2
ends (FE), a pool of TACC workers (W) some of which may be caches (\$), a user
profile database, a graphical monitor, and a fault-tolerant load manager,
d199 1
a199 1
independent of each other (because of the nature of the internet services'
d216 1
a216 1
\subsection{TranSend Implementation}
a217 7
This subsection focuses on the implementation of TranSend.  The goals are
to demonstrate how each component shown in figure \ref{arch_sns} maps into
the layered architecture, to discuss relevant implementation details and
trade-offs, and to provide the necessary context for the measurements we
report in the next section.

\subsubsection{TranSend as a TACC Application}
d223 3
a225 1
unaltered.  The dispatch rules simply match the MIME type of the object
a231 14
One dispatch case is worth mentioning because it illustrates the
flexibility of the TACC API's in constructing responsive services.
TranSend actually contains two HTML parsers: a fast C-language parser
(based on code from the W3C) that does not handle proprietary HTML
extensions such as inline JavaScript, and a much slower but more robust
parser that we wrote in Perl.  All HTML pages are initially passed to
the slower Perl parser, but if it believes (based on page length and tag
density) that processing the page will introduce a delay longer than one
or two seconds, it immediately throws an exception and indicates that
the C parser should take over.  Because the majority of pages of this
length tend to be papers published in HTML rather than complex pages
with weird tags, this scheme handles the common cases well and keeps the
latency for processing HTML barely noticeable.

d242 2
a243 2
Transformed objects are stored in the cache with augmented URL's (``fat
URL's'') that encode the transformation parameters, saving the work of
d255 8
a262 1
our workstation to the outside world.  In the following subsections we
a265 39
\subsubsection{HTTP Traces and the Playback Engine}

Many of the performance tests are based upon HTTP trace data that we
gathered from our intended user population, namely the 25,000 UC Berkeley
dialup IP users, up to 600 of whom may be connected via a bank of 14.4K or
28.8K modems.  This trace data is analyzed in detail in \cite{grib_97}.
In order to realistically stress test TranSend, we created a high
performance trace playback engine.  The engine can generate requests at a
constant (and dynamically tunable) rate, or it can faithfully play back a
trace according to the timestamps in the trace file.  We thus had
fine-grained control over both the amount and nature of the load offered to
our implementation during our experimentation.

%\subsubsection{Worker Performance}

%\begin{figure}[tbh]
%\epsfxsize 0.95\hsize
%\begin{center}
%\makebox{
%\epsfbox[0 0 291 209]{./figures/gif_dist.eps}
%}
%\end{center}
%\caption{Average distillation latency vs. GIF size, based on GIF
%data gathered from the home-IP trace.
%}\label{gif_dist_lat}
%\end{figure}

%If the system is behaving well, the distillation of images is the most
%computationally expensive task performed by TranSend.  We measured the
%performance of our workers by timing distillation latency as a function
%of input data size, calculated across approximately 100,000 items from the
%dialup IP trace file.  Figure \ref{gif_dist_lat} shows that for the GIF
%worker, there is an approximately linear relationship between
%distillation time and input size, although a large variation in
%distillation time is observed for any particular data size.  The slope of
%this relationship is approximately 8 milliseconds per kilobyte of input.
%Similar results were observed for the JPEG and HTML workers, although
%the HTML worker is far more efficient.

d309 10
a318 10
\begin{figure}[tbh]
\epsfxsize 0.95\hsize
\begin{center}
\makebox{
\epsfbox[0 0 438 294]{./figures/distq2.eps}
}
\end{center}
\caption{An enlarged view of figure \ref{distq1}.}
\label{distq2}
\end{figure}
d328 4
a331 2
within five seconds.  Figure \ref{distq2} shows an enlarged view of the
graph in figure \ref{distq1}.  During the experiment, we manually killed
d363 1
a363 1
end, one worker, the manager, and some fixed number of cache
d389 1
a389 1
{\bf Second} & {\bf FE's} & {\bf Dist.} & {\bf saturated} \\ \hline \hline
d401 1
a401 2
\caption{Results of the scalability experiment.  ``FE'' refers to front end,
and ``Dist.'' refers to workers.}
@


1.13
log
@*** empty log message ***
@
text
@d287 1
a287 1
\subsubsection{Worker Performance}
d289 11
a299 11
\begin{figure}[tbh]
\epsfxsize 0.95\hsize
\begin{center}
\makebox{
\epsfbox[0 0 291 209]{./figures/gif_dist.eps}
}
\end{center}
\caption{Average distillation latency vs. GIF size, based on GIF
data gathered from the home-IP trace.
}\label{gif_dist_lat}
\end{figure}
d301 11
a311 11
If the system is behaving well, the distillation of images is the most
computationally expensive task performed by TranSend.  We measured the
performance of our workers by timing distillation latency as a function
of input data size, calculated across approximately 100,000 items from the
dialup IP trace file.  Figure \ref{gif_dist_lat} shows that for the GIF
worker, there is an approximately linear relationship between
distillation time and input size, although a large variation in
distillation time is observed for any particular data size.  The slope of
this relationship is approximately 8 milliseconds per kilobyte of input.
Similar results were observed for the JPEG and HTML workers, although
the HTML worker is far more efficient.
@


1.12
log
@*** empty log message ***
@
text
@d1 2
a2 1
\section{Scalable Infrastructural Adaptation Services}\label{scaling}
d4 9
a12 32
%\begin{quotation}
%One of the overall design goals is to create a computing 
%system which is capable of meeting almost all of the 
%requirements of a large computer utility.  Such systems must 
%run continuously and reliably 7 days a week, 24 hours a 
%day\ldots and must be capable of meeting wide service 
%demands.

%Because the system must ultimately be comprehensive 
%and able to adapt to unknown future requirements, its 
%framework must be general, and capable of evolving over 
%time.

%\flushright{---Corbat\'o and Vyssotsky on Multics, 1965 \cite{CORBAT_65}}
%\end{quotation}

%Although it is normally viewed as an operating system, Multics 
%(Multiplexed Information and Computer Service) was originally 
%conceived as an infrastructural computing service.  The
%primary obstacle to deploying Multics was the absence of the net
%work infrastructure, which is now in place.  Network applications 
%have exploded in popularity in part because they are easier to manage
%and evolve than their desktop application counterparts: they 
%eliminate the need for software distribution, and simplify customer 
%service and bug tracking by avoiding the difficulty of dealing with 
%multiple platforms and versions.  

This section discusses our framework for the implementation of
cluster-based infrastructural services.  Our framework reflects the
implementation of three real network services in use today: TranSend, a
scalable transformation and caching proxy for the 25,000 Berkeley dialup IP
users (connecting through a bank of 600 modems); the Inktomi search engine
d14 5
a18 3
against a database of over 50 million web pages; and Top Gun Wingman, the
only graphical Web browser for the PalmPilot PDA (Personal Digital
Assistant).  
d23 1
a23 1
images results in an end-to-end latency reduction by a factor of 3-5,
d27 1
a27 1
deployed to a similar community at UC Davis.
d29 2
a30 1
%\subsection{Clusters: Advantages and Disadvantages}
d32 70
a101 71
%Particularly in the area of Internet service deployment, clusters of
%workstations provide three primary benefits over single larger machines,
%such as SMPs: incremental scalability, high availability, and the cost/per
%formance and maintenance benefits of commodity PC's.  We elaborate on each
%of these in turn.

%\begin{itemize}

%\item {\bf Scalability}: Clusters are well suited to Internet service
%workloads, which are highly parallel (many independent simultaneous users)
%and for which the grain size typically corresponds to at most a few
%CPU-seconds on a commodity PC.  For these workloads, large clusters can
%dwarf the power of the largest machines.  Furthermore, the ability to grow
%clusters incrementally over time is a tremendous advantage in areas such as
%Internet service deployment, where capacity planning depends on a large
%number of unknown variables.  

%\item {\bf High Availability}: Clusters have natural redundancy due to the
%independence of the nodes: Each node has its own busses, power supply,
%disks, etc., so it is ``merely'' a matter of software to mask (possibly
%multiple simultaneous) transient faults. Such capabilities are essential for
%network services, whose users have come to expect 24-hour uptime despite
%the inevitable reality of hardware and software faults due to rapid system
%evolution.

%\item {\bf Commodity Building Blocks}: The final set of advantages of
%clustering follows from the use of commodity building blocks rather than
%high-end, low-volume machines.  The obvious advantage is cost/performance,
%since memory, disks, and nodes can all track the leading edge.
%Furthermore, since many commodity vendors compete on service (particularly
%for PC hardware), it is easy to get high-quality configured nodes in 48
%hours or less.  

%\end{itemize}

%There are a number of areas in which clusters are at a disadvantage
%relative to SMP's.  In this section we describe some of these challenges and
%how they influenced the architecture we will propose.

%\begin{itemize}

%\item {\bf Administration}: Administration is a serious concern for sys
%tems of many nodes.  We leverage ideas in prior work \cite{ANDERS_96}, which 
%describes how a unified monitoring/reporting framework with data 
%visualization support was an effective tool for simplifying cluster 
%administration.

%\item {\bf Component vs.  system replication}: Each commodity PC in a
%cluster is not usually powerful enough to support an entire service, but
%can probably support some components of the service.  Component-level rather
%than whole-system replication therefore allows commodity PCs to serve as
%the unit of incremental scaling, provided the software can be naturally
%decomposed into loosely coupled modules.  We address this challenge by
%proposing an architecture in which each component has well-circumscribed
%functional responsibilities and is largely ``interchangeable'' with other
%components of the same type.  For example, a cache node can run anywhere
%that a disk is available, and a worker that performs a specific kind of
%data compression can run anywhere that significant CPU cycles are
%available.

%\item {\bf Partial failures}: Component-level replication leads directly to
%the fundamental issue separating clusters from SMPs: the need to handle
%partial failures (i.e., the ability to survive and adapt to failures of
%subsets of the system).  Traditional workstations and SMPs never face this
%issue, since the machine is either up or down.

%\item {\bf Shared state}: Unlike SMPs, clusters have no shared state.  
%Although much work has been done to emulate global shared state through
%software distributed shared memory \cite{KELEH_92, KELEH_94, LI_86}, we can
%improve performance and reduce complexity if we can avoid or minimize the
%need for shared state across the cluster.
d103 1
a103 1
%\end{itemize}
d107 8
a114 9
We now propose a system architecture and service-programming model for
building scalable network services on clusters of workstations.  The
architecture attempts to address both the challenges of cluster computing
(unwieldy administration, the difficulties of achieving component
replication and suffering partial failures, and the lack of shared state
across components) and the challenges of deploying network services, while
exploiting clusters' strengths (the support for incremental scalability,
high availability through redundancy, and the ability to use commodity
building blocks).
d120 3
a122 1
interfaces.
d131 2
a132 2
\caption{{\bf Architecture of a generic SNS}:  Components include front
ends (FE), a pool of workers (W) some of which may be caches (\$), a user
d138 1
a138 1
The software-component block diagram of a generic SNS is shown in figure
d145 1
a145 1
{\bf Front Ends} provide the interface to the SNS as seen by the outside
d152 6
a157 4
The {\bf Worker Pool} consists of caches and service-specific modules that
implement the actual service (data transformation/filtering, content
aggregation, etc.) Each type of module may be instantiated zero or more
times, depending on offered load.
d167 15
a181 4
The {\bf Graphical Monitor} for system management supports tracking and
visualization of the system's behavior, asynchronous error notification via
email or pager, and temporary disabling of system components for hot
upgrades.
d183 1
a183 1
The {\bf System-Area Network} provides a low-latency, high-bandwidth
d190 1
a190 1
Components in our SNS architecture may be replicated for fault tolerance or
d212 1
a212 1
\subsection{Service Implementation}
d220 43
a262 30
The TranSend implementation consists of the following SNS components:

\begin{description}

\item \textbf{Front Ends.}  The TranSend front end presents an HTTP interface to the
client population.  A thread is assigned to each arriving TCP connection.
Request processing involves fetching Web data from the caching subsystem
(or from the Internet on a cache miss), pairing up the request with the
user's customization preferences, sending the request and preferences to a
pipeline of one or more distillers (the TranSend lossy-compression workers)
to perform the appropriate transformation, and returning the result to the
client.

\item \textbf{Load Balancing Manager.}  For internal load balancing, TranSend uses a
centralized manager.  When the front end has a task for a distiller, the
manager stub code inside the front end contacts the manager, which locates
an appropriate distiller, spawning a new one if necessary.  The manager
stub caches the new distiller's location for future requests.  Worker stub
code attached to each distiller accepts and queues requests on behalf of
the distiller and periodically reports load information to the manager.
The manager aggregates load information from all distillers, computes
weighted moving averages, and piggybacks the resulting information on its
beacons to the manager stub.  The manager stub (at the front end) caches
the information in these beacons and uses lottery scheduling
\cite{Wal94} to select a distiller for each request.  The cached
information provides a backup so that the system can continue to operate
(using slightly stale load data) even if the manager crashes.  To allow the
system to scale as the load increases, the manager can automatically spawn
a new distiller on an unused node if it detects excessive load on
distillers of a particular class.
d264 1
a264 35
\item \textbf{User Profile Database.} The service interface to TranSend allows each
user to register a series of customization settings, using either HTML
forms or a Java/JavaScript combination applet.  The actual database is
implemented using gdbm because it is freely available and its performance
is adequate for our needs: user preference reads are much more frequent
than writes, and the reads are absorbed by a write-through cache in the
front end.

\item {\textbf TACC workers.}  Transformation, aggregation, and caching
are performed by TACC workers.  TranSend includes three parameterizable
transformation workers: scaling and low-pass filtering of JPEG images
using the off-the-shelf {\em jpeg-6a} library \cite{jpeg-6a}, GIF-to-JPEG
conversion followed by JPEG degradation, and a Perl HTML parser that
implements the user interface to TranSend.  In addition,
TranSend runs four Harvest object cache workers
\cite{harvest} on four 
separate nodes.  The TACC API allows the cache workers to be managed as
a single virtual cache, including URL hashing, automatic failover,
and dynamic growth of the cache pool.

\item {\textbf Graphical Monitor.}
Our extensible Tcl/Tk \cite{tcltk} graphical monitor presents a unified
view of the system as a single virtual entity.  Components of the system
report state information to the monitor using a multicast group, allowing
multiple monitors to run at geographically dis persed locations for remote
management.  The benefits of visualization are not just cosmetic: We can
immediately detect by looking at the visualization panel what state the
system as a whole is in, whether any component is currently causing a
bottleneck (such as cache-miss time, distillation queueing delay,
interconnect), what resources the system is using, and other such figures
of interest.

\end{itemize}

\subsection{Measurements of the TranSend Implementation}\label{meas}
d287 1
a287 1
\subsubsection{Distiller Performance}
d303 1
a303 1
performance of our distillers by timing distillation latency as a function
d306 1
a306 1
distiller, there is an approximately linear relationship between
d310 2
a311 2
Similar results were observed for the JPEG and HTML distillers, although
the HTML distiller is far more efficient.
d315 19
a333 1
TranSend uses queue lengths at the distillers as a metric for load
d337 1
a337 1
distiller to absorb the load.  The threshold $H$ maps to the greatest delay
d339 1
a339 1
allow the new distiller to stabilize the system, the spawning mechanism is
d341 1
a341 1
stability (rate of spawning and reaping distillers) and user-perceptible
d351 2
a352 2
\caption{Distiller queue lengths observed over time as the load presented
to the system fluctuates, and as distillers are manually brought down.}
d367 1
a367 1
Figure \ref{distq1} shows the variation in distiller queue lengths over
d369 6
a374 6
On-demand spawning of the first distiller was observed as soon as load was
offered.  With increasing load, the distiller queue gradually increased
until the manager decided to spawn a second distiller, which reduced the
queue length of the first distiller and balanced the load across both
distillers within five seconds.  Continued increase in load caused a third
distiller to start up, which again reduced and balanced the queue lengths
d377 1
a377 1
the first two distillers, causing the load on the remaining distiller to
d379 2
a380 2
distiller.  Even after $D$ seconds, the manager discovered that the system
was overloaded and started up one more distiller, causing the load to
d408 1
a408 1
end, one distiller, the manager, and some fixed number of cache
d413 1
a413 1
(e.g., distiller queues growing too long, front ends no longer accepting
d435 3
a437 3
0-24 & 1 & 1 & distillers \\ \hline
25-47 & 1 & 2 & distillers \\ \hline
48-72 & 1 & 3 & distillers \\ \hline
d439 3
a441 3
88-91 & 2 & 4 & distillers \\ \hline
92-112 & 2 & 5 & distillers \\ \hline
113-135 & 2 & 6 & distillers + \\
d443 1
a443 1
136-159 & 3 & 7 & distillers \\ \hline
d447 1
a447 1
and ``Dist.'' refers to distillers.}
d453 2
a454 2
single available distiller, the manager automatically spawned one
additional distiller, and then subsequent distillers as necessary.  At 87
d458 1
a458 1
cluster's machines were hosting distillers, front ends, or playback
d460 1
a460 1
the scaled range: a distiller can handle approximately 23 requests per
@


1.11
log
@more chopping.
@
text
@d230 1
a230 1
\item[Front Ends]  The TranSend front end presents an HTTP interface to the
d239 1
a239 1
\item[Load Balancing Manager]  For internal load balancing, TranSend uses a
d257 1
a257 1
\item[User Profile Database] The service interface to TranSend allows each
d265 11
a275 17
\item[Cache Nodes] TranSend runs Harvest object cache workers [10] on four
separate nodes.  For both scalability and improved fault tolerance, the
manager stub can manage a number of separate cache nodes as a single
virtual cache, hashing URL space across the separate caches and
automatically re-hashing when cache nodes are added or removed.e

\item[Datatype-Specific Distillers]  Distillers are workers which perform
transformation and aggregation.  We have built three parameterizable
distillers for TranSend: scaling and low-pass filtering of JPEG images
using the off-the-shelf jpeg-6a library \cite{jpeg-6a}, GIF-to-JPEG
conversion followed by JPEG degradation, and a Perl HTML ``munger'' that
marks up inline image references with distillation preferences, adds extra
links next to distilled images so that users can retrieve the original
content.  The user interface for TranSend is thus controlled by the HTML
distiller, under the direction of the user preferences from the front end.

\item[Graphical Monitor]
d277 1
d289 1
a289 1
\end{description}
@


1.10
log
@More stuff.
@
text
@d3 26
a28 26
\begin{quotation}
One of the overall design goals is to create a computing 
system which is capable of meeting almost all of the 
requirements of a large computer utility.  Such systems must 
run continuously and reliably 7 days a week, 24 hours a 
day\ldots and must be capable of meeting wide service 
demands.

Because the system must ultimately be comprehensive 
and able to adapt to unknown future requirements, its 
framework must be general, and capable of evolving over 
time.

\flushright{---Corbat\'o and Vyssotsky on Multics, 1965 \cite{CORBAT_65}}
\end{quotation}

Although it is normally viewed as an operating system, Multics 
(Multiplexed Information and Computer Service) was originally 
conceived as an infrastructural computing service.  The
primary obstacle to deploying Multics was the absence of the net
work infrastructure, which is now in place.  Network applications 
have exploded in popularity in part because they are easier to manage
and evolve than their desktop application counterparts: they 
eliminate the need for software distribution, and simplify customer 
service and bug tracking by avoiding the difficulty of dealing with 
multiple platforms and versions.  
d49 1
a49 1
\subsection{Clusters: Advantages and Disadvantages}
d51 71
a121 71
Particularly in the area of Internet service deployment, clusters of
workstations provide three primary benefits over single larger machines,
such as SMPs: incremental scalability, high availability, and the cost/per
formance and maintenance benefits of commodity PC's.  We elaborate on each
of these in turn.

\begin{itemize}

\item {\bf Scalability}: Clusters are well suited to Internet service
workloads, which are highly parallel (many independent simultaneous users)
and for which the grain size typically corresponds to at most a few
CPU-seconds on a commodity PC.  For these workloads, large clusters can
dwarf the power of the largest machines.  Furthermore, the ability to grow
clusters incrementally over time is a tremendous advantage in areas such as
Internet service deployment, where capacity planning depends on a large
number of unknown variables.  

\item {\bf High Availability}: Clusters have natural redundancy due to the
independence of the nodes: Each node has its own busses, power supply,
disks, etc., so it is ``merely'' a matter of software to mask (possibly
multiple simultaneous) transient faults. Such capabilities are essential for
network services, whose users have come to expect 24-hour uptime despite
the inevitable reality of hardware and software faults due to rapid system
evolution.

\item {\bf Commodity Building Blocks}: The final set of advantages of
clustering follows from the use of commodity building blocks rather than
high-end, low-volume machines.  The obvious advantage is cost/performance,
since memory, disks, and nodes can all track the leading edge.
Furthermore, since many commodity vendors compete on service (particularly
for PC hardware), it is easy to get high-quality configured nodes in 48
hours or less.  

\end{itemize}

There are a number of areas in which clusters are at a disadvantage
relative to SMP's.  In this section we describe some of these challenges and
how they influenced the architecture we will propose.

\begin{itemize}

\item {\bf Administration}: Administration is a serious concern for sys
tems of many nodes.  We leverage ideas in prior work \cite{ANDERS_96}, which 
describes how a unified monitoring/reporting framework with data 
visualization support was an effective tool for simplifying cluster 
administration.

\item {\bf Component vs.  system replication}: Each commodity PC in a
cluster is not usually powerful enough to support an entire service, but
can probably support some components of the service.  Component-level rather
than whole-system replication therefore allows commodity PCs to serve as
the unit of incremental scaling, provided the software can be naturally
decomposed into loosely coupled modules.  We address this challenge by
proposing an architecture in which each component has well-circumscribed
functional responsibilities and is largely ``interchangeable'' with other
components of the same type.  For example, a cache node can run anywhere
that a disk is available, and a worker that performs a specific kind of
data compression can run anywhere that significant CPU cycles are
available.

\item {\bf Partial failures}: Component-level replication leads directly to
the fundamental issue separating clusters from SMPs: the need to handle
partial failures (i.e., the ability to survive and adapt to failures of
subsets of the system).  Traditional workstations and SMPs never face this
issue, since the machine is either up or down.

\item {\bf Shared state}: Unlike SMPs, clusters have no shared state.  
Although much work has been done to emulate global shared state through
software distributed shared memory \cite{KELEH_92, KELEH_94, LI_86}, we can
improve performance and reduce complexity if we can avoid or minimize the
need for shared state across the cluster.
d123 1
a123 1
\end{itemize}
d128 14
a141 7
building scalable network services on clusters.  The architecture attempts
to address both the challenges of cluster computing and the challenges of
deploying network services, while exploiting clusters' strengths.  In the
architecture, the content of network services (i.e., what the services do)
is separated from their implementation, by encapsulating the ``scalable
network service'' (SNS) requirements of high availability, scalability, and
fault tolerance in a reusable layer with narrow interfaces.
d194 1
a194 1
\subsubsection{Scalability}
d205 1
a205 1
\subsubsection{Soft State for Fault Tolerance and Availability}
d207 10
a216 10
The technique of constructing robust entities by relying on cached soft
state refreshed by periodic messages from peers has been enormously
successful in wide-area TCP/IP networks \cite{tcp_imp, DEER_94,
MCQUILL_80}, another arena in which transient component failure is a fact
of life.  Correspondingly, our SNS components operate in this manner, and
monitor one another using process peer fault tolerance: when a component
fails, one of its peers restarts it (on a different node if necessary),
while cached stale state carries the surviving components through the
failure.  After the component is restarted, it gradually rebuilds its soft
state, typically by listening to multicasts from other components.
d399 16
a414 16
To demonstrate the scalability of the system, we needed to eliminate two
bottlenecks that limit the load we could offer: the overhead associated
with having a very large number of open file descriptors, and the
bottleneck 10Mb/s Ethernet connecting our cluster to the Internet.  To do
this, we prepared a trace file that repeatedly requested a fixed number of
JPEG images, all approximately 10KB in size, based on the distributions we
observed.  These images would then remain resident in the cache partitions,
eliminating cache miss penalty and the resulting buildup of file
descriptors in the front end.  We recognize that although a non-zero cache
miss penalty does not introduce any additional network, stable storage, or
computational burden on the system, it does result in an increase in the
amount of state in the front end, which limits the performance of a single
front end.  On the other hand, by turning off caching of distilled images,
we force our system to re-distill the image every time it was requested,
and in that respect our measurements are pessimistic relative to the
system's normal mode of operation.
d416 2
a417 1
Our strategy for the experiment was as follows:
d421 1
a421 1
\item Begin with a minimal instance of the system: one front
d426 14
a439 12
\item Increase the offered load until some system component saturates
(e.g., distiller queues grow too long, front ends cannot accept additional
connections, etc.).

\item Add more resources to the system to eliminate this saturation (in
many cases the system does this automatically, as when it recruits overflow
nodes to run more workers), and record the amount of resources added as a
function of the increase in offered load, measured in requests per second.

\item Continue until the saturated resource cannot be replenished (i.e., we
run out of hardware), or until adding more of the saturated resource no
longer results in a linear or close-to-linear improvement in performance.
@


1.9
log
@Aaagh.
@
text
@a389 9
When we first ran this experiment, we noticed rapid oscillations in queue
lengths.  Inspection revealed that since the front end's manager stubs only
periodically received distiller queue length reports, they were making load
balancing decisions based on stale data.  To repair this, we changed the
manager stub to keep a running estimate of the change in distiller queue
lengths between successive reports; these estimates were sufficient to
eliminate the oscillations.  The data in figures \ref{distq1} and
\ref{distq2} reflects the modified load balancing functionality.

a481 17

Ultimately, the scalability of our system is limited by the shared or
centralized components of the system, namely the user profile database, the
manager, and the SAN.  In our experience, neither the database nor the
manager have ever been close to saturation.  The main task of the manager
(in steady state) is to accumulate load announcements from all distillers
and multicast this information to the front ends.  We conducted an
experiment to test the capability of the manager to handle these load
announcements.  Nine hundred distillers were created on four machines.  Each
of these distillers generated a load announcement packet for the manager
every half a second.  The manager was easily able to handle this aggregate
load of 1800 announcements per second.  With each distiller capable of
processing over 20 front end requests per second, the manager is
computationally capable of sustaining a total number of distillers
equivalent to 18000 requests per second.  This number is nearly three orders
of magnitude greater than the peak load ever seen on UC Berkeley's modem
pool (which is comparable to a modest-sized ISP).
@


1.8
log
@Fixed up multics reference
@
text
@d30 18
a47 44


Pervasive throughout our design and implementation strategies 
is the observation that much of the data manipulated by a network 
service can tolerate semantics weaker than ACID\cite{GRAY_81}.  We combine 
ideas from prior work on tradeoffs between availability and consis
tency, and the use of soft state for robust fault-tolerance to charac
terize the data semantics of many network services, which we refer 
to as BASE semantics (basically available, soft state, eventual con
sistency).  In addition to demonstrating how BASE simplifies the 
implementation of our architecture, we present a programming 
model for service authoring that is a good fit for BASE semantics 
and that maps well onto our cluster-based service framework.

\subsection{Validation: Three Real Services}

Our framework reflects the implementation of three real network services
in use today: TranSend, a scalable transformation and caching proxy for
the 25,000 Berkeley dialup IP users (connecting through a bank of 600
modems); the Inktomi search engine (commercialized as HotBot), which
performs millions of queries per day against a database of over 50
million web pages; and Top Gun Wingman, the only graphical Web browser
for the PalmPilot PDA (Personal Digital Assistant).  The Inktomi search
engine is an aggregation server that was initially developed to explore
the use of cluster technology to han dle the scalability and
availability requirements of network ser vices.  The commercial version,
HotBot, handles several million queries per day against a full-text
database of 54 million web pages.  It has been incrementally scaled up to
60 nodes, provides high availability, and is extremely cost
effective.  Inktomi predates the framework we describe, and thus differs
from it in some respects.  However, it strongly influenced the
framework's design, and we will use it to validate particular design
decisions.  We focus our detailed discussion on TranSend, which provides
Web caching and data transformation.  In particular, real-time,
datatype-specific distillation and refinement \cite{pythia} of inline
Web images results in an end-to-end latency reduction by a factor of
3-5, giving the user a much more responsive Web surfing experience with
only modest image quality degradation.  TranSend was devel oped at UC
Berkeley and has been deployed for the 25,000 dialup IP users there, and
is being deployed to a similar community at UC Davis.  We also discuss
Top Gun Wingman in some depth, since it represents an extreme point in
the design space for small devices with poor networks and provides a
vivid demonstration of how the utility of such devices is enhanced by
aggressive adoption of active proxies.
d51 3
a53 3
Particularly in the area of Internet service deployment, clusters 
provide three primary benefits over single larger machines, such as 
SMPs: incremental scalability, high availability, and the cost/per
d66 1
a66 5
number of unknown variables.  Incremental scalability replaces capacity
planning with relatively fluid reactionary scaling.  Clusters
correspondingly eliminate the ``forklift upgrade'', in which you must throw
out the current machine (and related investments) and replace it via
forklift with an even larger one.  
d71 1
a71 3
multiple simultaneous) transient faults.  A natural extension of this
capability is to temporarily disable a subset of nodes and then upgrade
them in place (``hot upgrade'').  Such capabilities are essential for
d82 1
a82 6
hours or less.  In contrast, large SMPs typically have a lead time of 45
days, are more cumbersome to purchase, install, and upgrade, and are
supported by a single vendor, so it is much harder to get help when
difficulties arise.  Once again, it is a ``simple matter of software'' to
tie together a collection of possibly heterogeneous commodity building
blocks.
a124 1

d131 1
a131 1
architecutre, the content of network services (i.e., what the services do)
d196 1
a196 36
linear function of the increase in offered load.  Although the components
are mostly independent, they do have some dependence on the shared,
non-replicated system components: the SAN, the resource manager, and
possibly the user profile database.  Our measurements in section \ref{meas}
confirm that even for very large systems, these shared components do not
become a bottleneck.

The static partitioning of functionality between front ends and 
workers reflects our desire to keep workers as simple as possible, 
by localizing in the front ends the control decisions associated with 
satisfying user requests.  In addition to managing the network state 
for outstanding requests, a front end encapsulates service-specific 
worker dispatch logic, accesses the profile database to pass the 
appropriate parameters to the workers, notifies the end user in a 
service-specific way (e.g., constructing an HTML page describing 
the error) when one or more workers fails unrecoverably, provides 
the user interface to the profile database, and so forth.  This division 
of responsibility allows workers to remain simple and stateless, and 
allows the behavior of the service as a whole to be defined almost 
entirely in the front end.  If the workers are analogous to processes 
in a Unix pipeline, the front end is analogous to an interactive shell.

\subsubsection{Centralized Load Balancing}

Load balancing is controlled by a centralized policy implemented in the
manager.  The manager collects load information from the workers,
synthesizes load balancing hints based on the policy, and periodically
transmits the hints to the front ends, which make local scheduling
decisions based on the most recent hints.  The load balancing and overflow
policies are left to the system operator.

The decision to centralize rather than distribute load balancing is
intentional: If the load balancer can be made fault tolerant, and if we can
ensure it does not become a performance bottleneck, centralization makes it
easier to implement and reason about the behavior of the load balancing
policy.  
a210 31
We use timeouts as an additional fault-tolerance mechanism, to infer
certain failure modes that cannot be otherwise detected.  If the condition
that caused the timeout can be automatically resolved (e.g., if workers
lost because of a SAN partition can be restarted on still-visible nodes),
the manager performs the necessary actions.  Otherwise, the SNS layer
reports the suspected failure condition, and the service layer determines
how to proceed (e.g., report the error or fall back to a simpler task that
does not require the failed worker).

\subsubsection{Narrow Interface to Service-Specific Workers}

To allow new services to reuse all these facilities, the manager and front
ends provide a narrow API, shown as the Manager Stubs and Worker Stubs in
figure \ref{arch_sns}, for communicating with the workers, the manager, and the
graphical system monitor.  The worker stub provides mechanisms for workers
to implement some required behaviors for participating in the system (e.g.,
supplying load data to assist the manager in load balancing decisions and
reporting detectable failures in their own operation).  The worker stub
hides fault tolerance, load balancing, and multithreading considerations
from the worker code, which may use all the facilities of the operating
system, need not be thread-safe, and can, in fact, crash without taking the
system down.  The minimal restrictions on worker code allow worker authors
to focus instead on the content of the service, even using off-the-shelf
code (as we have in TranSend) to implement the worker modules.

The manager stub linked to the front ends provides support for implementing
the dispatch logic that selects which worker type(s) are needed to satisfy
a request; since the dispatch logic is independent of the core load
balancing and fault tolerance mechanisms, a variety of services can be
built using the same set of workers.

d213 5
a217 5
This subsection focuses on the implementation of TranSend, a scalable Web
distillation proxy.  The goals are to demonstrate how each component shown
in figure \ref{arch_sns} maps into the layered architecture, to discuss
relevant implementation details and trade-offs, and to provide the
necessary context for the measurements we report in the next section.
d223 8
a230 10
\item[Front Ends]  TranSend runs on a cluster of SPARCstation 10 and 20
machines, interconnected by switched 10 Mb/s Ethernet and connected to the
dialup IP pool by a single 10 Mb/s segment.  The TranSend front end presents
an HTTP interface to the client population.  A thread is assigned to each
arriving TCP connection.  Request processing involves fetching Web data from
the caching subsystem (or from the Internet on a cache miss), pairing up
the request with the user's customization preferences, sending the request
and preferences to a pipeline of one or more distillers (the TranSend
lossy-compression workers) to perform the appropriate transformation, and
returning the result to the client.  
d233 11
a243 17
centralized manager whose responsibilities include tracking the location of
distillers, spawning new distillers on demand, balancing load across
distillers of the same class, and providing the assurance of fault
tolerance and system tuning.  The manager periodically beacons its existence
on an IP multicast group to which the other components subscribe.  The use
of IP multicast provides a level of indirection and relieves components of
having to explicitly locate each other.  When the front end has a task for a
distiller, the manager stub code inside the front end contacts the manager,
which locates an appropriate distiller, spawning a new one if
necessary.  The manager stub caches the new distiller's location for future
requests.  Worker stub code attached to each distiller accepts and queues
requests on behalf of the distiller and periodically reports load
information to the manager.  The manager aggregates load information from
all distillers, computes weighted moving averages, and piggybacks the
resulting information on its beacons to the manager stub.  The manager stub
(at the front end) caches the information in these beacons and uses lottery
scheduling \cite{Wal94} to select a distiller for each request.  The cached
d259 14
a272 26
separate nodes.  Harvest suffers from three functional/performance
deficiencies,two of which we resolved.  First, although a collection of
Harvest caches can be treated as ``siblings'', by default all siblings are
queried on each request, so that the cache service time would increase as
the load increases even if more cache nodes were added.  Therefore, for
both scalability and improved fault tolerance, the manager stub can manage
a number of separate cache nodes as a single virtual cache, hashing the key
space across the separate caches and automatically re-hashing when cache
nodes are added or removed.  Second, we modified Harvest to allow data to
be injected into it, allowing distillers (via the worker stub) to store
post-transformed or intermediate-state data into the large virtual cache.
Finally, because the interface to each cache node is HTTP, a separate TCP
connection is required for each cache request.  We did not repair this
deficiency due to the complexity of the Harvest code, and as a result some
of the measurements reported in section \ref{meas} are slightly pessimistic.

\item[Datatype-Specific Distillers]  The second group of workers is the
distillers, which perform transformation and aggregation.  As a result, we
were able to leverage a large amount of off-the-shelf code for our
distillers.  We have built three parameterizable distillers for TranSend:
scaling and low-pass filtering of JPEG images using the off-the-shelf
jpeg-6a library \cite{jpeg-6a}, GIF-to-JPEG conversion followed by JPEG
degradation, and a Perl HTML ``munger'' that marks up inline image
references with distillation preferences, adds extra links next to
distilled images so that users can retrieve the original content.
The user interface for TranSend is thus controlled by the HTML distiller, under the direction of the user preferences from the front end.
d276 5
a280 7
Our extensible Tcl/Tk \cite{tcltk} graphical monitor presents a unified view of
the system as a single virtual entity.  Components of the system report
state information to the monitor using a multicast group, allowing multiple
monitors to run at geographically dis persed locations for remote
management.  The monitor can page or email the system operator if a serious
error occurs, for example, if it stops receiving reports from some
component.  The benefits of visualization are not just cosmetic: We can
d296 2
a297 5
analyze the size distribution and burstiness characteristics of TranSend's
expected workload, describe the performance of two throughput-critical
components (the cache nodes and data-transformation workers) in isolation,
and report on experiments that stress TranSend's fault tolerance,
responsiveness to bursts, and scalability.
d304 1
a304 9
28.8K modems.  The modems' connection to the Internet passes through a
single 10 Mb/s Ethernet segment; we placed a tracing machine running an IP
packet filter on this segment for a month and a half, and unobtrusively
gathered a trace of approximately 20 million (anonymized) HTTP
requests.  GIF, HTML, and JPEG were by far the three most common MIME types
observed in our traces (50\%, 22\%, and 18\%, respectively), and hence our
three implemented distillers cover these common cases.  Data for which no
distiller exists is passed unmodified to the user.

a311 23
\subsubsection{Burstiness}

\begin{figure}[tbh]
\epsfxsize 0.95\hsize
\begin{center}
\makebox{
\epsfbox[0 0 315 368]{./figures/all_burst.eps}
}
\end{center}
\caption{The number of requests per second for traced home-IP users,
showing burstiness across different time scales.
}\label{allburst}
\end{figure}

Burstiness is a fundamental property of a great variety of computing
systems, and can be observed across all time scales \cite{crov95, grib98,
mah97}.  Our HTTP traces show that the offered load to our implementation
will contain bursts - figure \ref{allburst} shows the request rate observed
from the user base across a 24 hour, 3.5 hour, and 3.5 minute time
interval.  The 24 hour interval exhibits a strong 24 hour cycle that is
overlaid with shorter time-scale bursts.  The 3.5 hour and 3.5 minute
intervals reveal finer grained bursts.

a325 1

a337 43
\subsubsection{Cache Partition Performance}

In \cite{bow94}, a detailed performance analysis of the Harvest caching
system is presented.  We summarize the results here: The average cache hit
takes 27 ms to service, including network and OS overhead, implying a
maximum average service rate from each partitioned cache instance of 37
requests per second.  TCP connection and tear-down overhead is attributed
to 15 ms of this service time.  95\% of all cache hits take less than 100
ms to service, implying cache hit rate has low variation.  The miss penalty
(i.e., the time to fetch data from the Internet) varies widely, from 100 ms
through 100 seconds.  This implies that should a cache miss occur, it is
likely to dominate the end-to-end latency through the system, and therefore
much effort should be expended to minimize cache miss rate.

As a supplement to these results, we ran a number of cache simulations to
explore the relationship between user population size, cache size, and
cache hit rate, using LRU replacement.  We observed that the size of the
user population greatly affects the attainable hit rate.  Cache hit rate
increases monotonically as a function of cache size, but plateaus out at a
level that is a function of the user population size.  For the user
population observed across the traces (approximately 8000 people over the
1.5 month period), six gigabytes of cache space (in total, partitioned over
all instances) gave us a hit rate of 56\%.  Similarly, we observed that for
a given cache size, increasing the size of the user population increases
the hit rate in the cache (due to an increase in locality across the
users), until the point at which the sum of the users' working sets exceeds
the cache size, causing the cache hit rate to fall.From these results, we
can deduce that the capacity of a single front end will be limited by the
high cache miss penalties.  The number of simultaneous, outstanding requests
at a front end is equal to, where N is the number of requests arriving per
second, and T is the average service time of a request.  A high cache miss
penalty implies that T will be large.  Because two TCP connections (one
between the client and front end, the other between the front end and a
cache partition) and one thread context are maintained in the front end for
each outstanding request, implying that front ends are vulnerable to state
management and context switching overhead.  As an example, for offered loads
of 15 requests per second to a front end, we have observed 150-350
outstanding requests and therefore up to 700 open TCP connections and 300
active thread contexts at any given time.  As a result, the front end spends
more than 70\% of its time in the kernel (as reported by the top utility)
under this load.  Eliminating this overhead is the subject of ongoing
research.

d343 2
a344 2
average crosses a configurable threshold H, the manager spawns a new
distiller to absorb the load.  The threshold H maps to the greatest delay
d347 1
a347 1
disabled for D seconds; the parameter D represents a tradeoff between
d375 9
a383 9
time.  The system was bootstrapped with one front end and the
manager.  On-demand spawning of the first distiller was observed as soon as
load was offered.  With increasing load, the distiller queue gradually
increased until the manager decided to spawn a second distiller, which
reduced the queue length of the first distiller and balanced the load
across both distillers within five seconds.  Continued increase in load
caused a third distiller to start up, which again reduced and balanced the
queue lengths within five seconds.  Figure \ref{distq2} shows an enlarged view
of the graph in figure \ref{distq1}.  During the experiment, we manually killed
d386 2
a387 2
distiller.  Even after D seconds, the manager discovered that the system was
overloaded and started up one more distiller, causing the load to
d507 1
a507 40
pool which is comparable to a modest-sized ISP.  

On the other hand, SAN saturation is a potential concern for
communication-intensive workloads such as TranSend's.  The problem of
optimizing component placement given a specific network topology,
technology, and workload is an important topic for future research.  As a
preliminary exploration of how TranSend behaves as the SAN saturates, we
repeated the scalability experiments using a 10 Mb/s switched Ethernet.  As
the network was driven closer to saturation, we noticed that most of our
(unreliable) multicast traffic was being dropped, crippling the ability of
the manager to balance load and the ability of the monitor to report system
conditions.  One possible solution to this problem is the addition of a
low- speed utility network to isolate control traffic from data traffic,
allowing the system to more gracefully handle (and perhaps avoid) SAN
saturation.  Another possibility is to use a higher-performance SAN
interconnect: a Myrinet \cite{MYRI_95} microbenchmark measured 32 MBytes/s
all-pairs traffic between 40 nodes, far greater than the traffic
experienced during the normal use of the system, suggesting that Myrinet
will support systems of at least several tens of nodes.

\subsection{Economic Feasibility}

Given the improved quality of service provided by TranSend, 
an interesting question is the additional cost required to operate this 
service.  From our performance data, a US\$5000 Pentium Pro 
server should be able to support about 750 modems, or about 
15,000 subscribers (assuming a 20:1 subscriber to modem ratio).  
Amortized over 1 year, the marginal cost per user is an amazing 25 
cents/month.
If we include the savings to the ISP due to a cache hit rate of 
50\% or more, as we observed in our cache experiments, then we 
can eliminate the equivalent of 1-2 T1 lines per TranSend installa
tion, which reduces operating costs by about US\$3000 per month.  
Thus, we expect that the server would pay for itself in only two 
months.  In this argument we have ignored the cost of administra
tion, which is nontrivial, but we believe administration costs for 
TranSend would be minimal---we run TranSend at Berkeley with 
essentially no administration except for feature upgrades and bug 
fixes, both of which are performed without bringing the service 
down.
@


1.7
log
@Added a final set of graphs for the scalability section, as well as some
tables and cleanup of text.
@
text
@d16 1
a16 1
\flushright{---Corbat\'o and Vyssotsky on Multics, 1965 [17]}
@


1.6
log
@Added closing itemize to taccapps, working on references and figures in
scaling.
@
text
@d174 20
a193 6
The software-component block diagram of a generic SNS is shown in Figure
**1**.  Each physical workstation in a network of workstations (NOW **[2]**)
supports one or more software components in the figure, but each component
in the diagram is confined to one node.  In general, the components whose
tasks are naturally parallelizable are replicated for scalability, fault
tolerance, or both.  
d221 3
a223 3
interconnect, such as switched 100-Mb/s Ethernet or Myrinet **[43]**.  Its
main goal is to prevent the interconnect from becoming the bottleneck as
the system scales.
d237 1
a237 1
possibly the user profile database.  Our measurements in **Section 4**
d263 1
a263 2
policies are left to the system operator.  We describe our experiments with
load balancing and overflow in **Section 4.5.**
d275 8
a282 8
successful in wide-area TCP/IP networks **[5,20,39]**, another arena in
which transient component failure is a fact of life.  Correspondingly, our
SNS components operate in this manner, and monitor one another using
process peer fault tolerance: when a component fails, one of its peers
restarts it (on a different node if necessary), while cached stale state
carries the surviving components through the failure.  After the component
is restarted, it gradually rebuilds its soft state, typically by listening
to multicasts from other components.  
d297 1
a297 1
**Figure 1**, for communicating with the workers, the manager, and the
d319 3
a321 3
in **Figure 1** maps into the layered architecture, to discuss relevant
implementation details and trade-offs, and to provide the necessary context
for the measurements we report in the next section.
d355 1
a355 1
scheduling **[63]** to select a distiller for each request.  The cached
d371 15
a385 1
separate nodes.  Harvest suffers from three functional/performance deficiencies,two of which we resolved.  First, although a collection of Harvest caches can be treated as ``siblings'', by default all siblings are queried on each request, so that the cache service time would increase as the load increases even if more cache nodes were added.  Therefore, for both scalability and improved fault tolerance, the manager stub can manage a number of separate cache nodes as a single virtual cache, hashing the key space across the separate caches and automatically re-hashing when cache nodes are added or removed.  Second, we modified Harvest to allow data to be injected into it, allowing distillers (via the worker stub) to store post-transformed or intermediate-state data into the large virtual cache.  Finally, because the interface to each cache node is HTTP, a separate TCP connection is required for each cache request.  We did not repair this deficiency due to the complexity of the Harvest code, and as a result some of the measurements reported in **Section 4.4** are slightly pessimistic.
d392 1
a392 1
jpeg-6a library **[29]**, GIF-to-JPEG conversion followed by JPEG
d400 1
a400 1
Our extensible Tcl/Tk **[48]* graphical monitor presents a unified view of
d415 1
a415 1
\subsection{Measurements of the TranSend Implementation}
d451 20
a470 8
Burstiness is a fundamental property of a great variety of computing 
systems, and can be observed across all time scales **[18,27,35]**.  Our
HTTP traces show that the offered load to our implementation will contain
bursts - **Figure 6** shows the request rate observed from the user base
across a 24 hour, 3.5 hour, and 3.5 minute time interval.  The 24 hour
interval exhibits a strong 24 hour cycle that is overlaid with shorter
time-scale bursts.  The 3.5 hour and 3.5 minute intervals reveal finer
grained bursts.
d474 13
d491 7
a497 7
dialup IP trace file.  **Figure 7** shows that for the GIF distiller, there
is an approximately linear relationship between distillation time and input
size, although a large variation in distillation time is observed for any
particular data size.  The slope of this relationship is approximately 8
milliseconds per kilobyte of input.  Similar results were observed for the
JPEG and HTML distillers, although the HTML distiller is far more
efficient.
d501 11
a511 8
In **[10]**, a detailed performance analysis of the Harvest caching sys
tem is presented.  We summarize the results here: The average cache hit takes 27 ms to service, including network and OS overhead, implying a maximum average service rate from each partitioned cache instance of 37 requests per second.  TCP connection and tear-down overhead is attributed to 15 ms of this service time.
95\% of all cache hits take less than 100 ms to service, implying cache hit
rate has low variation.  The miss penalty (i.e., the time to fetch data from
the Internet) varies widely, from 100 ms through 100 seconds.  This implies
that should a cache miss occur, it is likely to dominate the end-to-end
latency through the system, and therefore much effort should be expended to
minimize cache miss rate.
d555 24
a578 1
**Figure 8(a)** shows the variation in distiller queue lengths over
d586 2
a587 2
queue lengths within five seconds.  **Figure 8(b)** shows an enlarged view
of the graph in **Figure 8(a)**.  During the experiment, we manually killed
d600 2
a601 2
eliminate the oscillations.  The data in **Figure 8** reflects the modified
load balancing functionality.
d646 36
a681 14
**Table 2** presents the results of this experiment.  At 24 requests per
second, as the offered load exceeded the capacity of the single available
distiller, the manager automatically spawned one additional distiller, and
then subsequent distillers as necessary.  At 87 requests per second, the
Ethernet segment leading into the front end saturated, requiring a new
front end to be spawned.  We were unable to test the system at rates higher
than 159 requests per second, as all of our cluster's machines were hosting
distillers, front ends, or playback engines.  We did observe nearly
perfectly linear growth of the system over the scaled range: a distiller
can handle approximately 23 requests per second, and a 100 Mb/s Ethernet
segment into a front-end can handle approximately 70 requests per
second.  We were unable to saturate the front end, the cache partitions, or
fully saturate the interior SAN during this experiment.  We draw two
conclusions from this result:
d726 1
a726 1
interconnect: a Myrinet **[43]** microbenchmark measured 32 MBytes/s
@


1.5
log
@*** empty log message ***
@
text
@d34 1
a34 1
service can tolerate semantics weaker than ACID **[26]**.  We combine 
d130 1
a130 1
tems of many nodes.  We leverage ideas in prior work **[1]**, which 
d156 3
a158 3
software distributed shared memory **[33,34,36]**, we can improve
performance and reduce complexity if we can avoid or minimize the need for
shared state across the cluster.
@


1.4
log
@argh.
@
text
@d1 30
a30 1
\section{Building Scalable Infrastructural Services}\label{scal_serv}
d34 1
a34 1
service can tolerate semantics weaker than ACID **[26]**. We combine 
d39 1
a39 1
sistency). In addition to demonstrating how BASE simplifies the 
d55 1
a55 1
availability requirements of network ser vices. The commercial version,
d57 1
a57 1
database of 54 million web pages. It has been incrementally scaled up to
d59 2
a60 2
effective. Inktomi predates the framework we describe, and thus differs
from it in some respects. However, it strongly influenced the
d63 1
a63 1
Web caching and data transformation. In particular, real-time,
d67 1
a67 1
only modest image quality degradation. TranSend was devel oped at UC
d80 1
a80 1
formance and maintenance benefits of commodity PC's. We elaborate on each
d88 1
a88 1
CPU-seconds on a commodity PC. For these workloads, large clusters can
d92 2
a93 2
number of unknown variables. Incremental scalability replaces capacity
planning with relatively fluid reactionary scaling. Clusters
d96 1
a96 1
forklift with an even larger one. 
d101 1
a101 1
multiple simultaneous) transient faults. A natural extension of this
d103 1
a103 1
them in place (``hot upgrade''). Such capabilities are essential for
d110 1
a110 1
high-end, low-volume machines. The obvious advantage is cost/performance,
d114 1
a114 1
hours or less. In contrast, large SMPs typically have a lead time of 45
d117 1
a117 1
difficulties arise. Once again, it is a ``simple matter of software'' to
d124 1
a124 1
relative to SMP's. In this section we describe some of these challenges and
d130 1
a130 1
tems of many nodes. We leverage ideas in prior work **[1]**, which 
d135 1
a135 1
\item {\bf Component vs. system replication}: Each commodity PC in a
d137 1
a137 1
can probably support some components of the service. Component-level rather
d140 1
a140 1
decomposed into loosely coupled modules. We address this challenge by
d143 1
a143 1
components of the same type. For example, a cache node can run anywhere
d151 1
a151 1
subsets of the system). Traditional workstations and SMPs never face this
d154 1
a154 1
\item {\bf Shared state}: Unlike SMPs, clusters have no shared state. 
d166 1
a166 1
building scalable network services on clusters. The architecture attempts
d168 1
a168 1
deploying network services, while exploiting clusters' strengths. In the
d175 1
a175 1
**1**. Each physical workstation in a network of workstations (NOW **[2]**)
d177 1
a177 1
in the diagram is confined to one node. In general, the components whose
d179 1
a179 1
tolerance, or both. 
d182 1
a182 1
world (e.g., HTTP server). They ``shepherd'' incoming requests by matching
d184 1
a184 1
and queueing them for service by one or more workers. Front ends maximize
d196 1
a196 1
occur. When necessary, it may assign work to machines in the overflow pool,
d207 1
a207 1
interconnect, such as switched 100-Mb/s Ethernet or Myrinet **[43]**. Its
d214 1
a214 1
high availability, but we also use replication to achieve scalability. When
d217 1
a217 1
added nodes. The duties of our replicated components are largely
d220 1
a220 1
linear function of the increase in offered load. Although the components
d223 1
a223 1
possibly the user profile database. Our measurements in **Section 4**
d230 1
a230 1
satisfying user requests. In addition to managing the network state 
d236 1
a236 1
the user interface to the profile database, and so forth. This division 
d239 1
a239 1
entirely in the front end. If the workers are analogous to processes 
d245 1
a245 1
manager. The manager collects load information from the workers,
d248 2
a249 2
decisions based on the most recent hints. The load balancing and overflow
policies are left to the system operator. We describe our experiments with
d256 1
a256 1
policy. 
d263 1
a263 1
which transient component failure is a fact of life. Correspondingly, our
d267 1
a267 1
carries the surviving components through the failure. After the component
d269 1
a269 1
to multicasts from other components. 
d272 1
a272 1
certain failure modes that cannot be otherwise detected. If the condition
d275 1
a275 1
the manager performs the necessary actions. Otherwise, the SNS layer
d285 1
a285 1
graphical system monitor. The worker stub provides mechanisms for workers
d288 1
a288 1
reporting detectable failures in their own operation). The worker stub
d292 1
a292 1
system down. The minimal restrictions on worker code allow worker authors
d316 3
a318 3
dialup IP pool by a single 10 Mb/s segment. The TranSend front end presents
an HTTP interface to the client population. A thread is assigned to each
arriving TCP connection. Request processing involves fetching Web data from
d323 1
a323 1
returning the result to the client. 
d329 2
a330 2
tolerance and system tuning. The manager periodically beacons its existence
on an IP multicast group to which the other components subscribe. The use
d332 1
a332 1
having to explicitly locate each other. When the front end has a task for a
d335 1
a335 1
necessary. The manager stub caches the new distiller's location for future
d338 1
a338 1
information to the manager. The manager aggregates load information from
d340 1
a340 1
resulting information on its beacons to the manager stub. The manager stub
d342 1
a342 1
scheduling **[63]** to select a distiller for each request. The cached
d351 1
a351 1
forms or a Java/JavaScript combination applet. The actual database is
d358 1
a358 1
separate nodes. Harvest suffers from three functional/performance deficiencies,two of which we resolved. First, although a collection of Harvest caches can be treated as ``siblings'', by default all siblings are queried on each request, so that the cache service time would increase as the load increases even if more cache nodes were added. Therefore, for both scalability and improved fault tolerance, the manager stub can manage a number of separate cache nodes as a single virtual cache, hashing the key space across the separate caches and automatically re-hashing when cache nodes are added or removed. Second, we modified Harvest to allow data to be injected into it, allowing distillers (via the worker stub) to store post-transformed or intermediate-state data into the large virtual cache. Finally, because the interface to each cache node is HTTP, a separate TCP connection is required for each cache request. We did not repair this deficiency due to the complexity of the Harvest code, and as a result some of the measurements reported in **Section 4.4** are slightly pessimistic.
d361 1
a361 1
distillers, which perform transformation and aggregation. As a result, we
d363 1
a363 1
distillers. We have built three parameterizable distillers for TranSend:
d374 1
a374 1
the system as a single virtual entity. Components of the system report
d377 1
a377 1
management. The monitor can page or email the system operator if a serious
d392 1
a392 1
external load or network traffic. For measurements requiring Internet
d394 1
a394 1
our workstation to the outside world. In the following subsections we
d406 1
a406 1
28.8K modems. The modems' connection to the Internet passes through a
d410 1
a410 1
requests. GIF, HTML, and JPEG were by far the three most common MIME types
d412 1
a412 1
three implemented distillers cover these common cases. Data for which no
d416 1
a416 1
performance trace playback engine. The engine can generate requests at a
d418 1
a418 1
trace according to the timestamps in the trace file. We thus had
d425 1
a425 1
systems, and can be observed across all time scales **[18,27,35]**. Our
d428 1
a428 1
across a 24 hour, 3.5 hour, and 3.5 minute time interval. The 24 hour
d430 1
a430 1
time-scale bursts. The 3.5 hour and 3.5 minute intervals reveal finer
d436 1
a436 1
computationally expensive task performed by TranSend. We measured the
d439 1
a439 1
dialup IP trace file. **Figure 7** shows that for the GIF distiller, there
d442 2
a443 2
particular data size. The slope of this relationship is approximately 8
milliseconds per kilobyte of input. Similar results were observed for the
d450 1
a450 1
tem is presented. We summarize the results here: The average cache hit takes 27 ms to service, including network and OS overhead, implying a maximum average service rate from each partitioned cache instance of 37 requests per second. TCP connection and tear-down overhead is attributed to 15 ms of this service time.
d452 2
a453 2
rate has low variation. The miss penalty (i.e., the time to fetch data from
the Internet) varies widely, from 100 ms through 100 seconds. This implies
d460 2
a461 2
cache hit rate, using LRU replacement. We observed that the size of the
user population greatly affects the attainable hit rate. Cache hit rate
d463 1
a463 1
level that is a function of the user population size. For the user
d466 1
a466 1
all instances) gave us a hit rate of 56\%. Similarly, we observed that for
d472 1
a472 1
high cache miss penalties. The number of simultaneous, outstanding requests
d474 2
a475 2
second, and T is the average service time of a request. A high cache miss
penalty implies that T will be large. Because two TCP connections (one
d479 1
a479 1
management and context switching overhead. As an example, for offered loads
d482 1
a482 1
active thread contexts at any given time. As a result, the front end spends
d484 1
a484 1
under this load. Eliminating this overhead is the subject of ongoing
d490 1
a490 1
balancing. As queue lengths grow due to increased load, the moving average
d493 2
a494 2
distiller to absorb the load. The threshold H maps to the greatest delay
the user is willing to tolerate when the system is under high load. To
d501 3
a503 3
time. The system was bootstrapped with one front end and the
manager. On-demand spawning of the first distiller was observed as soon as
load was offered. With increasing load, the distiller queue gradually
d506 1
a506 1
across both distillers within five seconds. Continued increase in load
d509 1
a509 1
of the graph in **Figure 8(a)**. During the experiment, we manually killed
d511 2
a512 2
rapidly increase. The manager immediately reacted and started up a new
distiller. Even after D seconds, the manager discovered that the system was
d517 1
a517 1
lengths. Inspection revealed that since the front end's manager stubs only
d519 1
a519 1
balancing decisions based on stale data. To repair this, we changed the
d522 1
a522 1
eliminate the oscillations. The data in **Figure 8** reflects the modified
d530 1
a530 1
bottleneck 10Mb/s Ethernet connecting our cluster to the Internet. To do
d533 1
a533 1
observed. These images would then remain resident in the cache partitions,
d535 1
a535 1
descriptors in the front end. We recognize that although a non-zero cache
d539 1
a539 1
front end. On the other hand, by turning off caching of distilled images,
d550 1
a550 1
partitions. (Since for these experiments we repeatedly requested the same
d568 1
a568 1
**Table 2** presents the results of this experiment. At 24 requests per
d571 1
a571 1
then subsequent distillers as necessary. At 87 requests per second, the
d573 1
a573 1
front end to be spawned. We were unable to test the system at rates higher
d575 1
a575 1
distillers, front ends, or playback engines. We did observe nearly
d579 2
a580 2
second. We were unable to saturate the front end, the cache partitions, or
fully saturate the interior SAN during this experiment. We draw two
d598 2
a599 2
manager, and the SAN. In our experience, neither the database nor the
manager have ever been close to saturation. The main task of the manager
d601 1
a601 1
and multicast this information to the front ends. We conducted an
d603 1
a603 1
announcements. Nine hundred distillers were created on four machines. Each
d605 2
a606 2
every half a second. The manager was easily able to handle this aggregate
load of 1800 announcements per second. With each distiller capable of
d609 1
a609 1
equivalent to 18000 requests per second. This number is nearly three orders
d611 1
a611 1
pool which is comparable to a modest-sized ISP. 
d614 1
a614 1
communication-intensive workloads such as TranSend's. The problem of
d618 1
a618 1
repeated the scalability experiments using a 10 Mb/s switched Ethernet. As
d625 1
a625 1
saturation. Another possibility is to use a higher-performance SAN
d635 1
a635 1
service. From our performance data, a US\$5000 Pentium Pro 
d637 1
a637 1
15,000 subscribers (assuming a 20:1 subscriber to modem ratio). 
d643 1
a643 1
tion, which reduces operating costs by about US\$3000 per month. 
d645 1
a645 1
months. In this argument we have ignored the cost of administra
@


1.3
log
@More mods
@
text
@d1 1
a1 1
\section{Building Scalable Infrastructural Services}
@


1.2
log
@*** empty log message ***
@
text
@d1 1
a1 5
%  scalability and stuff, from SOSP
% right now, contains the entire text of the SOSP paper


[stuff deleted and moved to Motivation]
d5 1
a5 1
service can tolerate semantics weaker than ACID [26]. We combine 
d46 1
a46 14
In the remainder of this section we argue that clusters are an excellent
fit for Internet services, provided the challenges we describe for
cluster software development can be surmounted. In Section 2 we describe
the proposed layered architecture for build ing new services, and a
programming model for creating services that maps well onto the
architecture. We show how TranSend and HotBot map onto this
architecture, using HotBot to justify specific design decisions within
the architecture. Sections 3 and 4 describe the TranSend implementation
and its measured performance, including experiments on its scalability
and fault tolerance properties. Section 5 discusses related work and the
continuing evolution of this work, and we summarize our observations and
contributions in Section 6.

Advantages of Clusters
d51 51
a101 56
formance and maintenance benefits of commodity PC's. We elabo
rate on each of these in turn.
Scalability: Clusters are well suited to Internet service work
loads, which are highly parallel (many independent simultaneous 
users) and for which the grain size typically corresponds to at most 
a few CPU-seconds on a commodity PC. For these workloads, 
large clusters can dwarf the power of the largest machines. For 
example, Inktomi's HotBot cluster contains 60 nodes with 120 pro
cessors, 30 GB of physical memory, and hundreds of commodity 
disks. Wal-Mart uses a cluster from TeraData with 768 processors 
and 16 terabytes of online storage.
Furthermore, the ability to grow clusters incrementally over 
time is a tremendous advantage in areas such as Internet service 
deployment, where capacity planning depends on a large number 
of unknown variables. Incremental scalability replaces capacity 
planning with relatively fluid reactionary scaling. Clusters corre
spondingly eliminate the "forklift upgrade", in which you must 
throw out the current machine (and related investments) and 
replace it via forklift with an even larger one. 
High Availability: Clusters have natural redundancy due to the 
independence of the nodes: Each node has its own busses, power 
supply, disks, etc., so it is "merely" a matter of software to mask 
(possibly multiple simultaneous) transient faults. A natural exten
sion of this capability is to temporarily disable a subset of nodes 
and then upgrade them in place ("hot upgrade"). Such capabilities 
are essential for network services, whose users have come to 
expect 24-hour uptime despite the inevitable reality of hardware 
and software faults due to rapid system evolution.
Commodity Building Blocks: The final set of advantages of 
clustering follows from the use of commodity building blocks 
rather than high-end, low-volume machines. The obvious advan
tage is cost/performance, since memory, disks, and nodes can all 
track the leading edge; for example, we changed the building block 
every time we grew the HotBot cluster, each time picking the reli
able high volume previous-generation commodity units, helping to 
ensure stability and robustness. Furthermore, since many commod
ity vendors compete on service (particularly for PC hardware), it is 
easy to get high-quality configured nodes in 48 hours or less. In 
contrast, large SMPs typically have a lead time of 45 days, are 
more cumbersome to purchase, install, and upgrade, and are sup
ported by a single vendor, so it is much harder to get help when dif
ficulties arise. Once again, it is a "simple matter of software" to tie 
together a collection of possibly heterogeneous commodity build
ing blocks.
To summarize, clusters have significant advantages in scalabil
ity, growth, availability, and cost. Although fundamental, these 
advantages are not easy to realize.

Challenges of Cluster Computing

There are a number of areas in which clusters are at a disadvan
tage relative to SMP's. In this section we describe some of these 
challenges and how they influenced the architecture we will pro
pose in Section 2.
Administration: Administration is a serious concern for sys
tems of many nodes. We leverage ideas in prior work [1], which 
d105 93
a197 192
Component vs. system replication: Each commodity PC in a 
cluster is not usually powerful enough to support an entire service, 
but can probably support some components of the service. Compo
nent-level rather than whole-system replication therefore allows 
commodity PCs to serve as the unit of incremental scaling, pro
vided the software can be naturally decomposed into loosely cou
pled modules. We address this challenge by proposing an 
architecture in which each component has well-circumscribed 
functional responsibilities and is largely "interchangeable" with 
other components of the same type. For example, a cache node can 
run anywhere that a disk is available, and a worker that performs a 
specific kind of data compression can run anywhere that significant 
CPU cycles are available.
Partial failures: Component-level replication leads directly to 
the fundamental issue separating clusters from SMPs: the need to 
handle partial failures (i.e., the ability to survive and adapt to fail
ures of subsets of the system). Traditional workstations and SMPs 
never face this issue, since the machine is either up or down.
Shared state: Unlike SMPs, clusters have no shared state. 
Although much work has been done to emulate global shared state 
through software distributed shared memory [33,34,36], we can 
improve performance and reduce complexity if we can avoid or 
minimize the need for shared state across the cluster.
These last two concerns, partial failure and shared state, lead us 
to focus on the sharing semantics actually required by network ser
vices.

BASE Semantics

We believe that the design space for network services can be 
partitioned according to the data semantics that each service 
demands. At one extreme is the traditional transactional database 
model with the ACID properties (atomicity, consistency, isolation, 
durability) [26], providing the strongest semantics at the highest 
cost and complexity. ACID makes no guarantees regarding avail
ability; indeed, it is preferable for an ACID service to be unavail
able than to function in a way that relaxes the ACID constraints. 
ACID semantics are well suited for Internet commerce transac
tions, billing users, or maintaining user profile information for per
sonalized services.
For other Internet services, however, the primary value to the 
user is not necessarily strong consistency or durability, but rather 
high availability of data:
Stale data can be temporarily tolerated as long as all copies 
of data eventually reach consistency after a short time (e.g., 
DNS servers do not reach consistency until entry timeouts 
expire [41]).
Soft state, which can be regenerated at the expense of 
additional computation or file I/O, is exploited to improve 
performance; data is not durable.
Approximate answers (based on stale data or incomplete 
soft state) delivered quickly may be more valuable than exact 
answers delivered slowly.
We refer to the data semantics resulting from the combination 
of these techniques as BASE-Basically Available, Soft State, 
Eventual Consistency. By definition, any data semantics that are 
not strictly ACID are BASE. BASE semantics allow us to handle 
partial failure in clusters with less complexity and cost. Like pio
neering systems such as Grapevine [9] , BASE reduces the com
plexity of the service implementation, essentially trading 
consistency for simplicity; like later systems such as Bayou [21] 
that allow trading consistency for availability, BASE provides 
opportunities for better performance. For example, where ACID 
requires durable and consistent state across partial failures, BASE 
semantics often allows us to avoid communication and disk activity 
or to postpone it until a more convenient time.
In practice, it is simplistic to categorize every service as either 
ACID or BASE; instead, different components of services demand 
varying data semantics. Directories such as Yahoo! [64] maintain a 
database of soft state with BASE semantics, but keep user customi
zation profiles in an ACID database. Transformation proxies [23,57] 
interposed between clients and servers transform Internet content 
on-the-fly; the transformed content is BASE data that can be regen
erated by computation, but if the service bills the user per session, 
the billing should certainly be delegated to an ACID database.
We focus on services that have an ACID component, but 
manipulate primarily BASE data. Web servers, search/aggregation 
servers [58], caching proxies [14,44], and transformation proxies are 
all examples of such services; our framework supports a superset of 
these services by providing integrated support for the requirements 
of all four. As we will show, BASE semantics greatly simplify the 
implementation of fault tolerance and availability and permit per
formance optimizations within our framework that would be pre
cluded by ACID.

Cluster-Based Scalable Service Architecture

In this section we propose a system architecture and service-
programming model for building scalable network services on clus
ters. The architecture attempts to address both the challenges of 
cluster computing and the challenges of deploying network ser
vices, while exploiting clusters' strengths. We view our contribu
tions as follows:
A proposed system architecture for scalable network services 
that exploits the strengths of cluster computing, as 
exemplified by cluster-based servers such as TranSend and 
HotBot.
Separation of the content of network services (i.e., what the 
services do) from their implementation, by encapsulating the 
"scalable network service" (SNS) requirements of high 
availability, scalability, and fault tolerance in a reusable layer 
with narrow interfaces. 
A programming model based on composition of stateless 
worker modules into new services. The model maps well 
onto our system architecture and numerous existing services 
map directly onto it.
Detailed measurements of a production service that 
instantiates the architecture and validates our performance 
and reliability claims.
In the remainder of this section we review the benefits and 
challenges of cluster computing and propose a network service 
architecture that exploits these observations and allows encapsula
tion of the SNS requirements. We then describe a programming 
model that minimizes service development effort by allowing 
implementation of new services entirely at the higher layers.

Proposed Functional Organization of an SNS

The above observations lead us to the software-component 
block diagram of a generic SNS shown Figure 1. Each physical 
workstation in a network of workstations (NOW [2]) supports one 
or more software components in the figure, but each component in 
the diagram is confined to one node. In general, the components 
whose tasks are naturally parallelizable are replicated for scalabil
ity, fault tolerance, or both. In our measurements (Section 4), we 
will argue that the performance demands on the non-replicated 
components are not significant for the implementation of a large 
class of services, and that the practical bottlenecks are bandwidth 
into and out of the system and bandwidth in the system area net
work (SAN).
Front Ends provide the interface to the SNS as seen by the 
outside world (e.g., HTTP server). They "shepherd" 
incoming requests by matching them up with the appropriate 
user profile from the customization database, and queueing 
them for service by one or more workers. Front ends 
maximize system throughput by maintaining state for many 
simultaneous outstanding requests, and can be replicated for 
both scalability and availability.
The Worker Pool consists of caches and service-specific 
modules that implement the actual service (data 
transformation/filtering, content aggregation, etc.) Each type 
of module may be instantiated zero or more times, depending 
on offered load.
The Customization Database stores user profiles that allow 
mass customization of request processing.
The Manager balances load across workers and spawns 
additional workers as offered load fluctuates or faults occur. 
When necessary, it may assign work to machines in the 
overflow pool, a set of backup machines (perhaps on 
desktops) that can be harnessed to handle load bursts and 
provide a smooth transition during incremental growth. The 
overflow pool is discussed in Section 2.2.3.
The Graphical MC (transformation, aggregation, caching, 
customization), and Service. The key contributions of our architec
ture are the reusability of the SNS layer, and the ability to add sim
ple, stateless "building blocks" at the TACC layer and compose 
them in the Service layer. We discuss TACC in Section 2.3. The 
SNS layer, which we describe here, provides scalability, load bal
ancing, fault tolerance, and high availability; it comprises the front 
ends, manager, SAN, and monitor in Figure 1.

Scalability

Components in our SNS architecture may be replicated for 
fault tolerance or high availability, but we also use replication to 
achieve scalability. When the offered load to the system saturates 
the capacity of some component class, more instances of that com
ponent can be launched on incrementally added nodes. The duties 
of our replicated components are largely independent of each other 
(because of the nature of the internet services' workload), which 
means the amount of additional resources required is a linear func
tion of the increase in offered load. Although the components are 
mostly independent, they do have some dependence on the shared, 
non-replicated system components: the SAN, the resource man
ager, and possibly the user profile database. Our measurements in 
Section 4 confirm that even for very large systems, these shared 
components do not become a bottleneck.
d213 1
a213 1
Centralized Load Balancing
d215 301
a515 250
Load balancing is controlled by a centralized policy imple
mented in the manager. The manager collects load information 
from the workers, synthesizes load balancing hints based on the 
policy, and periodically transmits the hints to the front ends, which 
make local scheduling decisions based on the most recent hints. 
The load balancing and overflow policies are left to the system 
operator. We describe our experiments with load balancing and 
overflow in Section 4.5. 
The decision to centralize rather than distribute load balancing 
is intentional: If the load balancer can be made fault tolerant, and if 
we can ensure it does not become a performance bottleneck, cen
tralization makes it easier to implement and reason about the 
behavior of the load balancing policy. In Section 3.1.3 we discuss 
the evolution that led to this design decision and its implications 
for performance, fault tolerance, and scalability.
Prolonged Bursts and Incremental Growth
Although we would like to assume that there is a well-defined 
average load and that arriving traffic follows a Poisson distribution, 
burstiness has been demonstrated for Ethernet traffic [35], file sys
tem traffic [27], and Web requests [18], and is confirmed by our 
traces of web traffic (discussed later). In addition, Internet services 
can experience relatively rare but prolonged bursts of high load: 
after the recent landing of Pathfinder on Mars, its web site served 
over 220 million hits in a 4-day period [45]. Often, it is during such 
bursts that uninterrupted operation is most critical.
Our architecture includes the notion of an overflow pool for 
absorbing these bursts. The overflow machines are not dedicated to 
the service, and normally do not have workers running on them, but 
the manager can spawn workers on the overflow machines on 
demand when unexpected load bursts arrive, and release the 
machines when the burst subsides. In an institutional or corporate 
setting, the overflow pool could consist of workstations on individ
uals' desktops. Because worker nodes are already interchangeable, 
workers do not need to know whether they are running on a dedi
cated or an overflow node, since load balancing is handled exter
nally. In addition to absorbing sustained bursts, the ability to 
temporarily harness overflow machines eases incremental growth: 
when the overflow machines are being recruited unusually often, it 
is time to purchase more dedicated nodes for the service.
Soft State for Fault Tolerance and Availability
The technique of constructing robust entities by relying on 
cached soft state refreshed by periodic messages from peers has 
been enormously successful in wide-area TCP/IP networks 
[5,20,39], another arena in which transient component failure is a 
fact of life. Correspondingly, our SNS components operate in this 
manner, and monitor one another using process peer fault toler
ance: when a component fails, one of its peers restarts it (on a dif
ferent node if necessary), while cached stale state carries the 
surviving components through the failure. After the component is 
restarted, it gradually rebuilds its soft state, typically by listening to 
multicasts from other components. We give specific examples of 
this mechanism in Section 3.1.3.
We use timeouts as an additional fault-tolerance mechanism, to 
infer certain failure modes that cannot be otherwise detected. If the 
condition that caused the timeout can be automatically resolved 
(e.g., if workers lost because of a SAN partition can be restarted on 
still-visible nodes), the manager performs the necessary actions. 
Otherwise, the SNS layer reports the suspected failure condition, 
and the service layer determines how to proceed (e.g., report the 
error or fall back to a simpler task that does not require the failed 
worker).

Narrow Interface to Service-Specific Workers

To allow new services to reuse all these facilities, the manager 
and front ends provide a narrow API, shown as the Manager Stubs 
and Worker Stubs in Figure 1, for communicating with the workers, 
the manager, and the graphical system monitor. The worker stub 
provides mechanisms for workers to implement some required 
behaviors for participating in the system (e.g., supplying load data 
to assist the manager in load balancing decisions and reporting 
detectable failures in their own operation). The worker stub hides 
fault tolerance, load balancing, and multithreading considerations 
from the worker code, which may use all the facilities of the operat
ing system, need not be thread-safe, and can, in fact, crash without 
taking the system down. The minimal restrictions on worker code 
allow worker authors to focus instead on the content of the service, 
even using off-the-shelf code (as we have in TranSend) to imple
ment the worker modules.
The manager stub linked to the front ends provides support for 
implementing the dispatch logic that selects which worker type(s) 
are needed to satisfy a request; since the dispatch logic is indepen
dent of the core load balancing and fault tolerance mechanisms, a 
variety of services can be built using the same set of workers.


Service Implementation

This section focuses on the implementation of TranSend, a 
scalable Web distillation proxy, and compares it with HotBot. The 
goals of this section are to demonstrate how each component 
shown in Figure 1 maps into the layered architecture, to discuss rel
evant implementation details and trade-offs, and to provide the nec
essary context for the measurements we report in the next section.
TranSend SNS Components
Front Ends
TranSend runs on a cluster of SPARCstation 10 and 20 
machines, interconnected by switched 10 Mb/s Ethernet and con
nected to the dialup IP pool by a single 10 Mb/s segment. The 
TranSend front end presents an HTTP interface to the client popu
lation. A thread is assigned to each arriving TCP connection. 
Request processing involves fetching Web data from the caching 
subsystem (or from the Internet on a cache miss), pairing up the 
request with the user's customization preferences, sending the 
request and preferences to a pipeline of one or more distillers (the 
TranSend lossy-compression workers) to perform the appropriate 
transformation, and returning the result to the client. Alternatively, 
if an appropriate distilled representation is available in the cache, it 
can be sent directly to the client. A large thread pool allows the 
front end to sustain throughput and maximally exploit parallelism 
despite the large number of potentially long, blocking operations 
associated with each task, and provides a clean programming 
model. The production TranSend runs with a single front-end of 
about 400 threads.
Load Balancing Manager
Client-side JavaScript support [46] balances load across multi
ple front ends and masks transient front end failures, although 
other mechanisms such as round-robin DNS [12] or commercial 
routers [16] could also be used. For internal load balancing, 
TranSend uses a centralized manager whose responsibilities 
include tracking the location of distillers, spawning new distillers 
on demand, balancing load across distillers of the same class, and 
providing the assurance of fault tolerance and system tuning. We 
argue for a centralized as opposed to distributed manager because 
it is easier to change the load balancing policy and reason about its 
behavior; the next section discusses the fault-tolerance implications 
of this decision.
The manager periodically beacons its existence on an IP multi
cast group to which the other components subscribe. The use of IP 
multicast provides a level of indirection and relieves components 
of having to explicitly locate each other. When the front end has a 
task for a distiller, the manager stub code contacts the manager, 
which locates an appropriate distiller, spawning a new one if neces
sary. The manager stub caches the new distiller's location for future 
requests.
The worker stub attached to each distiller accepts and queues 
requests on behalf of the distiller and periodically reports load 
information to the manager. The manager aggregates load informa
tion from all distillers, computes weighted moving averages, and 
piggybacks the resulting information on its beacons to the manager 
stub. The manager stub (at the front end) caches the information in 
these beacons and uses lottery scheduling [63] to select a distiller 
for each request. The cached information provides a backup so that 
the system can continue to operate (using slightly stale load data) 
even if the manager crashes. Eventually, the fault tolerance mecha
nisms (discussed in Section 3.1.3) restart the manager and the sys
tem returns to normal.
To allow the system to scale as the load increases, the manager 
can automatically spawn a new distiller on an unused node if it 
detects excessive load on distillers of a particular class. (The 
spawning and load balancing policies are described in detail in 
Section 4.5.) Another mechanism used for adjusting to bursts in 
load is overflow: if all the nodes in the system are used up, the 
manager can resort to starting up temporary distillers on a set of 
overflow nodes. Once the burst subsides, the distillers may be 
reaped.

Fault Tolerance and Crash Recovery

In the original prototype for the manager, information about 
distillers was kept as hard state, using a log file and crash recovery 
protocols similar to those used by ACID databases. Resilience 
against crashes was via process-pair fault tolerance, as in [6]: the 
primary manager process was mirrored by a secondary whose role 
was to maintain a current copy of the primary's state, and take over 
the primary's tasks if it detects that the primary has failed. In this 
scenario, crash recovery is seamless, since all state in the second
ary process is up-to-date.
However, by moving entirely to BASE semantics, we were able 
to simplify the manager greatly and increase our confidence in its 
correctness. In TranSend, all state maintained by the manager is 
explicitly designed to be soft state. When a distiller starts up, it reg
isters itself with the manager, whose existence it discovers by sub
scribing to a well-known multicast channel. If the distiller crashes 
before de-registering itself, the manager detects the broken connec
tion; if the manager crashes and restarts, the distillers detect bea
cons from the new manager and re-register themselves. Timeouts 
are used as a backup mechanism to infer failures. Since all state is 
soft and is periodically beaconed, no explicit crash recovery or 
state mirroring mechanisms are required to regenerate lost state. 
Similarly, the front end does not require any special crash recovery 
code, since it can reconstruct its state as it receives the next few 
beacons from the manager. 
With this use of soft state, each "watcher" process only needs 
to detect that its peer is alive (rather than mirroring the peer's state) 
and, in some cases, be able to restart the peer (rather than take over 
the peer's duties). Broken connections, timeouts, or loss of beacons 
are used to infer component failures and restart the failed process. 
The manager, distillers, and front ends are process peers:
The manager reports distiller failures to the manager stubs, 
which update their caches of where distillers are running.
The manager detects and restarts a crashed front end.
The front end detects and restarts a crashed manager.
This process peer functionality is encapsulated within the man
ager stub code. Simply by linking against the stub, front ends are 
automatically recruited as process peers of the manager.
User Profile Database
The service interface to TranSend allows each user to register a 
series of customization settings, using either HTML forms or a 
Java/JavaScript combination applet. The actual database is imple
mented using gdbm because it is freely available and its perfor
mance is adequate for our needs: user preference reads are much 
more frequent than writes, and the reads are absorbed by a write-
through cache in the front end.

Cache Nodes

TranSend runs Harvest object cache workers [10] on four separate
nodes. Harvest suffers from three functional/performance deficiencies,
two of which we resolved. 
First, although a collection of Harvest caches can be treated as 
"siblings", by default all siblings are queried on each request, so 
that the cache service time would increase as the load increases 
even if more cache nodes were added. Therefore, for both scalabil
ity and improved fault tolerance, the manager stub can manage a 
number of separate cache nodes as a single virtual cache, hashing 
the key space across the separate caches and automatically re-hash
ing when cache nodes are added or removed. Second, we modified 
Harvest to allow data to be injected into it, allowing distillers (via 
the worker stub) to store post-transformed or intermediate-state 
data into the large virtual cache. Finally, because the interface to 
each cache node is HTTP, a separate TCP connection is required 
for each cache request. We did not repair this deficiency due to the 
complexity of the Harvest code, and as a result some of the mea
surements reported in Section 4.4 are slightly pessimistic.
Caching in TranSend is only an optimization. All cached data 
can be thrown away at the cost of performance-cache nodes are 
workers whose only job is the management of BASE data.

Datatype-Specific Distillers

The second group of workers is the distillers, which perform 
transformation and aggregation. As a result, we were able to lever
age a large amount of off-the-shelf code for our distillers. We have 
built three parameterizable distillers for TranSend: scaling and low-
pass filtering of JPEG images using the off-the-shelf jpeg-6a library 
[29], GIF-to-JPEG conversion followed by JPEG degradation, and 
a Perl HTML "munger" that marks up inline image references with 
distillation preferences, adds extra links next to distilled images so 
that users can retrieve the original content, and adds a "toolbar" 
(Figure 4) to each page that allows users to control various aspects 
of TranSend's operation. The user interface for TranSend is thus 
controlled by the HTML distiller, under the direction of the user 
preferences from the front end.
Each of these distillers took approximately 5-6 hours to imple
ment, debug, and optimize. Although pathological input data occa
sionally causes a distiller to crash, the process-peer fault tolerance 
guaranteed by the SNS layer means that we don't have to worry 
about eliminating all such possible bugs and corner cases from the 
system.
d517 1
a517 1
Graphical Monitor
d519 34
a552 160
Our extensible Tcl/Tk [48] graphical monitor presents a unified 
view of the system as a single virtual entity. Components of the 
system report state information to the monitor using a multicast 
group, allowing multiple monitors to run at geographically dis
persed locations for remote management. The monitor can page or 
email the system operator if a serious error occurs, for example, if it 
stops receiving reports from some component.
The benefits of visualization are not just cosmetic: We can 
immediately detect by looking at the visualization panel what state 
the system as a whole is in, whether any component is currently 
causing a bottleneck (such as cache-miss time, distillation queueing 
delay, interconnect), what resources the system is using, and other 
such figures of interest.

How TranSend Exploits BASE

Distinguishing ACID vs. BASE semantics in the design of ser
vice components greatly simplifies TranSend's fault-tolerance and 
improves its availability. Only the user-profile database is ACID; 
everything else exploits some aspect of BASE semantics, both in 
manipulating application data (i.e., Web content) and in the imple
mentation of the system components themselves.
Stale load balancing data: The load balancing data in the 
manager stub is slightly stale between updates from the 
manager, which arrive a few seconds apart. The use of stale 
data for the load balancing and overflow decisions improves 
performance and helps to hide faults, since using cached data 
avoids communicating with the source. Timeouts are used to 
recover from cases where stale data causes an incorrect load 
balancing choice. For example, if a request is sent to a 
worker that no longer exists, the request will time out and 
another worker will be chosen. From the standpoint of 
performance, as we will show in our measurements, the use 
of slightly stale data is not a problem in practice.
Soft state: The two advantages of soft state are improved 
performance from avoiding (blocking) commits and trivial 
recovery. Transformed content is cached and can be 
regenerated from the original (which may be also cached).
Approximate answers: Users of TranSend request objects that 
are named by the object URL and the user preferences, 
which are used to derive distillation parameters. However, if 
the system is too heavily loaded to perform distillation, it can 
return a somewhat different version from the cache; if the 
user clicks the "Reload" button later, they will get the 
distilled representation they asked for if the system now has 
sufficient resources to perform the distillation. If the required 
distiller has temporarily or permanently failed, the system 
can return the original content. In all cases, an approximate 
answer delivered quickly is more useful than the exact 
answer delivered slowly. 

HotBot Implementation

In this section we highlight the principal differences between 
the implementations of TranSend and HotBot.The original Inktomi 
work, which is the basis of HotBot, predates the layered model and 
scalable server architecture presented here and uses ad hoc rather 
than generalized mechanisms in some places.
Front ends and service interface: HotBot runs on a mixture 
of single- and multiple-CPU SPARCstation server nodes, intercon
nected by Myrinet [43]. The HTTP front ends in HotBot run 50-80 
threads per node and handle the presentation and customization of 
results based on user preferences and browser type. The presenta
tion is performed using a form of "dynamic HTML" based on Tcl 
macros [54].
Load balancing: HotBot workers statically partition the 
search-engine database for load balancing. Thus each worker han
dles a subset of the database proportional to its CPU power, and 
every query goes to all workers in parallel.
Failure management: Unlike the workers in TranSend, Hot
Bot worker nodes are not interchangeable, since each worker uses a 
local disk to store its part of the database. The original Inktomi 
nodes cross-mounted databases, so that there were always multiple 
nodes that could reach any database partition.  Thus, when a node 
when down, other nodes would automatically take over responsi
bility for that data, maintaining 100% data availability with grace
ful degradation in performance.  
Since the database partitioning distributes documents ran
domly and it is acceptable to lose part of the database temporarily, 
HotBot moved to a model in which RAID storage handles disk fail
ures, while fast restart minimizes the impact of node failures. For 
example, with 26 nodes the loss of one machine results in the data
base dropping from 54M to about 51M documents, which is still 
significantly larger than other search engines (such as Alta Vista at 
30M).
The success of the fault management of HotBot is exemplified 
by the fact that during February 1997, HotBot was physically 
moved (from Berkeley to San Jose) without ever being down, by 
moving half of the cluster at a time and changing DNS resolution 
in the middle. Although various parts of the database were unavail
able at different times during the move, the overall service was still 
up and useful-user feedback indicated that few people were 
affected by the transient changes.
User profile database: We expect commercial systems to use 
a real database for ACID components. HotBot uses Informix with 
primary/backup failover for the user profile and ad revenue track
ing database, with each front end linking in an Informix SQL cli
ent. However, all other HotBot data is BASE, and as in TranSend, 
timeouts are used to recover from stale cluster-state data.
Summary
The TranSend implementation quite closely maps into the lay
ered architecture presented in Section 2, while the HotBot imple
mentation differs in the use of a distributed manager, static load 
balancing by data partitioning, and workers that are tied to particu
lar machines. The careful separation of responsibility into different 
components of the system, and the layering of components accord
ing to the architecture, made the implementation complexity man
ageable.

Measurements of the TranSend Implementation

We took measurements of TranSend using a cluster of 15 Sun 
SPARC Ultra-1 workstations connected by 100 Mb/s switched 
Ethernet and isolated from external load or network traffic. For 
measurements requiring Internet access, the access was via a 
10Mb/s switched Ethernet network connecting our workstation to 
the outside world. In the following subsections we analyze the size 
distribution and burstiness characteristics of TranSend's expected 
workload, describe the performance of two throughput-critical 
components (the cache nodes and data-transformation workers) in 
isolation, and report on experiments that stress TranSend's fault 
tolerance, responsiveness to bursts, and scalability.
HTTP Traces and the Playback Engine
Many of the performance tests are based upon HTTP trace data 
that we gathered from our intended user population, namely the 
25,000 UC Berkeley dialup IP users, up to 600 of whom may be 
connected via a bank of 14.4K or 28.8K modems. The modems' 
connection to the Internet passes through a single 10 Mb/s Ethernet 
segment; we placed a tracing machine running an IP packet filter 
on this segment for a month and a half, and unobtrusively gathered 
a trace of approximately 20 million (anonymized) HTTP requests. 
GIF, HTML, and JPEG were by far the three most common MIME 
types observed in our traces (50%, 22%, and 18%, respectively), 
and hence our three implemented distillers cover these common 
cases. Data for which no distiller exists is passed unmodified to the 
user.
Figure 5 illustrates the distribution of sizes occurring for these 
three MIME types. Although most content accessed on the web is 
small (considerably less than 1 KB), the average byte transferred is 
part of large content (3-12 KB). This means that the users' modems 
spend most of their time transferring a few, large files. It is the goal 
of TranSend to eliminate this bottleneck by distilling this large con
tent into smaller, but still useful representations; data under 1 KB is 
transferred to the client unmodified, since distillation of such small 
content rarely results in a size reduction. 
Figure 5 also reveals a number of interesting properties of the 
individual data types. The GIF distribution has two plateaus-one 
for data sizes under 1KB (which correspond to icons, bullets, etc.) 
and one for data sizes over 1KB (which correspond to photos or 
cartoons). Our 1KB distillation threshold therefore exactly sepa
rates these two classes of data, and deals with each correctly. 
JPEGs do not show this same distinction: the distribution falls of 
rapidly under the 1KB mark.
In order to realistically stress test TranSend, we created a high 
performance trace playback engine. The engine can generate 
requests at a constant (and dynamically tunable) rate, or it can 
faithfully play back a trace according to the timestamps in the trace 
file. We thus had fine-grained control over both the amount and 
nature of the load offered to our implementation during our experi
mentation.
d554 1
a554 1
Burstiness
d556 3
a558 232
Burstiness is a fundamental property of a great variety of computing 
systems, and can be observed across all time scales [18,27,35]. Our HTTP 
traces show that the offered load to our implementation will contain 
bursts-Figure 6 shows the request rate observed from the user base 
across a 24 hour, 3.5 hour, and 3.5 minute time interval. The 24 hour 
interval exhibits a strong 24 hour cycle that is overlaid with shorter time-
scale bursts. The 3.5 hour and 3.5 minute intervals reveal finer grained 
bursts.
We described in Section 2.2.3 how our architecture allows an arbi
trary subset of machines to be managed as an overflow pool during tem
porary but prolonged periods of high load. The overflow pool can also be 
used to absorb bursts on shorter time scales. We argue that there are two 
possible administrative avenues for managing the overflow pool:
Select an average desired utilization level for the dedicated 
worker pool. Since we can observe a daily cycle, this amounts to 
drawing a line across Figure 6a (i.e., picking a number of tasks/
sec) such that the fraction of black under the line is the desired 
utilization level.
Select an acceptable percentage of time that the system will 
resort to the overflow pool. This amounts to drawing a line 
across Figure 6a such that the fraction of columns that cross the 
line is this percentage.
Since we have measured the average number of requests/s that a dis
tiller of a given class can handle, the number of tasks /s that we picked 
(from step 1 or 2 above) dictates how many distillers will need to be in 
the dedicated (non-overflow) pool.

Distiller Performance

If the system is behaving well, the distillation of images is the most 
computationally expensive task performed by TranSend. We measured 
the performance of our distillers by timing distillation latency as a func
tion of input data size, calculated across approximately 100,000 items 
from the dialup IP trace file. Figure 7 shows that for the GIF distiller, 
there is an approximately linear relationship between distillation time 
and input size, although a large variation in distillation time is observed 
for any particular data size. The slope of this relationship is approxi
mately 8 milliseconds per kilobyte of input. Similar results were 
observed for the JPEG and HTML distillers, although the HTML dis
tiller is far more efficient.

Cache Partition Performance

In [10], a detailed performance analysis of the Harvest caching sys
tem is presented. We summarize the results here:
The average cache hit takes 27 ms to service, including network 
and OS overhead, implying a maximum average service rate from 
each partitioned cache instance of 37 requests per second. TCP 
connection and tear-down overhead is attributed to 15 ms of this 
service time.
95% of all cache hits take less than 100 ms to service, implying 
cache hit rate has low variation.
The miss penalty (i.e., the time to fetch data from the Internet) 
varies widely, from 100 ms through 100 seconds. This implies that 
should a cache miss occur, it is likely to dominate the end-to-end 
latency through the system, and therefore much effort should be 
expended to minimize cache miss rate.
As a supplement to these results, we ran a number of cache simula
tions to explore the relationship between user population size, cache 
size, and cache hit rate, using LRU replacement. We observed that the 
size of the user population greatly affects the attainable hit rate. Cache 
hit rate increases monotonically as a function of cache size, but plateaus 
out at a level that is a function of the user population size. For the user 
population observed across the traces (approximately 8000 people over 
the 1.5 month period), six gigabytes of cache space (in total, partitioned 
over all instances) gave us a hit rate of 56%. Similarly, we observed that 
for a given cache size, increasing the size of the user population 
increases the hit rate in the cache (due to an increase in locality across 
the users), until the point at which the sum of the users' working sets 
exceeds the cache size, causing the cache hit rate to fall.
From these results, we can deduce that the capacity of a single front 
end will be limited by the high cache miss penalties. The number of 
simultaneous, outstanding requests at a front end is equal to , 
where N is the number of requests arriving per second, and T is the aver
age service time of a request. A high cache miss penalty implies that T 
will be large. Because two TCP connections (one between the client and 
front end, the other between the front end and a cache partition) and one 
thread context are maintained in the front end for each outstanding 
request, implying that front ends are vulnerable to state management and 
context switching overhead. As an example, for offered loads of 15 
requests per second to a front end, we have observed 150-350 outstand
ing requests and therefore up to 700 open TCP connections and 300 
active thread contexts at any given time. As a result, the front end 
spends more than 70% of its time in the kernel (as reported by the 
top utility) under this load. Eliminating this overhead is the subject 
of ongoing research.

Self Tuning and Load Balancing

TranSend uses queue lengths at the distillers as a metric for 
load balancing. As queue lengths grow due to increased load, the 
moving average of the queue length maintained by the manager 
starts increasing; when the average crosses a configurable threshold 
H, the manager spawns a new distiller to absorb the load. The 
threshold H maps to the greatest delay the user is willing to tolerate 
when the system is under high load. To allow the new distiller to 
stabilize the system, the spawning mechanism is disabled for D 
seconds; the parameter D represents a tradeoff between stability 
(rate of spawning and reaping distillers) and user-perceptible delay.
Figure 8(a) shows the variation in distiller queue lengths over 
time. The system was bootstrapped with one front end and the 
manager. On-demand spawning of the first distiller was observed 
as soon as load was offered. With increasing load, the distiller 
queue gradually increased until the manager decided to spawn a 
second distiller, which reduced the queue length of the first distiller 
and balanced the load across both distillers within five seconds. 
Continued increase in load caused a third distiller to start up, which 
again reduced and balanced the queue lengths within five seconds.
Figure 8(b) shows an enlarged view of the graph in Figure 8(a). 
During the experiment, we manually killed the first two distillers, 
causing the load on the remaining distiller to rapidly increase. The 
manager immediately reacted and started up a new distiller. Even 
after D seconds, the manager discovered that the system was over
loaded and started up one more distiller, causing the load to stabi
lize.
When we first ran this experiment, we noticed rapid oscilla
tions in queue lengths. Inspection revealed that since the front 
end's manager stubs only periodically received distiller queue 
length reports, they were making load balancing decisions based on 
stale data. To repair this, we changed the manager stub to keep a 
running estimate of the change in distiller queue lengths between 
successive reports; these estimates were sufficient to eliminate the 
oscillations. The data in Figure 8 reflects the modified load balanc
ing functionality.
Scalability
To demonstrate the scalability of the system, we needed to 
eliminate two bottlenecks that limit the load we could offer: the 
overhead associated with having a very large number of open file 
descriptors, and the bottleneck 10Mb/s Ethernet connecting our 
cluster to the Internet. To do this, we prepared a trace file that 
repeatedly requested a fixed number of JPEG images, all approxi
mately 10KB in size, based on the distributions we observed (Sec
tion 4.1). These images would then remain resident in the cache 
partitions, eliminating cache miss penalty and the resulting buildup 
of file descriptors in the front end. We recognize that although a 
non-zero cache miss penalty does not introduce any additional net
work, stable storage, or computational burden on the system, it 
does result in an increase in the amount of state in the front end, 
which as we mentioned in Section 4.4 limits the performance of a 
single front end. On the other hand, by turning off caching of dis
tilled images, we force our system to re-distill the image every time 
it was requested, and in that respect our measurements are pessi
mistic relative to the system's normal mode of operation.
Our strategy for the experiment was as follows:
Begin with a minimal instance of the system: one front 
end, one distiller, the manager, and some fixed number of 
cache partitions. (Since for these experiments we repeat
edly requested the same subset of images, the cache was 
effectively not tested.)
Increase the offered load until some system component sat
urates (e.g., distiller queues grow too long, front ends 
cannot accept additional connections, etc.).
Add more resources to the system to eliminate this satura
tion (in many cases the system does this automatically, as 
when it recruits overflow nodes to run more workers), and 
record the amount of resources added as a function of the 
increase in offered load, measured in requests per second.
Continue until the saturated resource cannot be replenished 
(i.e., we run out of hardware), or until adding more of the 
saturated resource no longer results in a linear or close-to-
linear improvement in performance.
Table2 presents the results of this experiment. At 24 requests 
per second, as the offered load exceeded the capacity of the single 
available distiller, the manager automatically spawned one addi
tional distiller, and then subsequent distillers as necessary. At 87 
requests per second, the Ethernet segment leading into the front 
end saturated, requiring a new front end to be spawned. We were 
unable to test the system at rates higher than 159 requests per sec
ond, as all of our cluster's machines were hosting distillers, front 
ends, or playback engines. We did observe nearly perfectly linear 
growth of the system over the scaled range: a distiller can handle 
approximately 23 requests per second, and a 100 Mb/s Ethernet 
segment into a front-end can handle approximately 70 requests per 
second. We were unable to saturate the front end, the cache parti
tions, or fully saturate the interior SAN during this experiment. We 
draw two conclusions from this result:
Even with a commodity 100 Mb/s SAN, linear scaling is 
limited primarily by bandwidth into the system rather than 
bandwidth inside the system.
Although we run TranSend on four SPARC 10's, a single 
Ultra-1 class machine would suffice to serve the entire dialup 
IP population of UC Berkeley (25,000 users officially, over 
8000 of whom surfed during the trace).
Ultimately, the scalability of our system is limited by the 
shared or centralized components of the system, namely the user 
profile database, the manager, and the SAN. In our experience, nei
ther the database nor the manager have ever been close to satura
tion. The main task of the manager (in steady state) is to 
accumulate load announcements from all distillers and multicast 
this information to the front ends. We conducted an experiment to 
test the capability of the manager to handle these load announce
ments. Nine hundred distillers were created on four machines. Each 
of these distillers generated a load announcement packet for the 
manager every half a second. The manager was easily able to han
dle this aggregate load of 1800 announcements per second. With 
each distiller capable of processing over 20 front end requests per 
second, the manager is computationally capable of sustaining a 
total number of distillers equivalent to 18000 requests per second. 
This number is nearly three orders of magnitude greater than the 
peak load ever seen on UC Berkeley's modem pool which is com
parable to a modest-sized ISP. Similarly, HotBot's ACID database 
(parallel Informix server), used for ad revenue tracking and user 
profiles, can serve about 400 requests per second, significantly 
greater than HotBot's load. 
On the other hand, SAN saturation is a potential concern for 
communication-intensive workloads such as TranSend's. The prob
lem of optimizing component placement given a specific network 
topology, technology, and workload is an important topic for future 
research. As a preliminary exploration of how TranSend behaves as 
the SAN saturates, we repeated the scalability experiments using a 
10 Mb/s switched Ethernet. As the network was driven closer to 
saturation, we noticed that most of our (unreliable) multicast traffic 
was being dropped, crippling the ability of the manager to balance 
load and the ability of the monitor to report system conditions. 
One possible solution to this problem is the addition of a low-
speed utility network to isolate control traffic from data traffic, 
allowing the system to more gracefully handle (and perhaps avoid) 
SAN saturation. Another possibility is to use a higher-performance 
SAN interconnect: a Myrinet [43] microbenchmark run on the Hot
Bot implementation measured 32 MBytes/s all-pairs traffic 
between 40 nodes, far greater than the traffic experienced during 
the normal use of the system, suggesting that Myrinet will support 
systems of at least several tens of nodes.

Discussion

In previous sections we presented detailed measurements of a 
scalable network service implementation that confirmed the effec
tiveness of our layered architecture. In this section, we discuss 
some of the more interesting and novel aspects of our architecture, 
reflect on further potential applications of this research, and com
pare our work with others' efforts.
d560 41
@


1.1
log
@Initial revision
@
text
@d4 3
a6 62
"One of the overall design goals is to create a computing 
system which is capable of meeting almost all of the 
requirements of a large computer utility. Such systems must 
run continuously and reliably 7 days a week, 24 hours a 
day... and must be capable of meeting wide service 
demands.
"Because the system must ultimately be comprehensive 
and able to adapt to unknown future requirements, its 
framework must be general, and capable of evolving over 
time."
- Corbat and Vyssotsky on Multics, 1965 [17]

Although it is normally viewed as an operating system, Multics 
(Multiplexed Information and Computer Service) was originally 
conceived as an infrastructural computing service, so it is not sur
prising that its goals as stated above are similar to our own. The 
primary obstacle to deploying Multics was the absence of the net
work infrastructure, which is now in place. Network applications 
have exploded in popularity in part because they are easier to man
age and evolve than their desktop application counterparts: they 
eliminate the need for software distribution, and simplify customer 
service and bug tracking by avoiding the difficulty of dealing with 
multiple platforms and versions. Also, basic queueing theory 
shows that a large central (virtual) server is more efficient in both 
cost and utilization than a collection of smaller servers; standalone 
desktop systems represent the degenerate case of one "server" per 
user. All of these support the argument for Network Computers 
[28].
However, network services remain difficult to deploy because 
of three fundamental challenges: scalability, availability and cost 
effectiveness.
By scalability, we mean that when the load offered to the 
service increases, an incremental and linear increase in 
hardware can maintain the same per-user level of service. 
By availability, we mean that the service as a whole must be 
available 24x7, despite transient partial hardware or software 
failures.
By cost effectiveness, we mean that the service must be 
economical to administer and expand, even though it 
potentially comprises many workstation nodes. 
We observe that clusters of workstations have some fundamen
tal properties that can be exploited to meet these requirements: 
using commodity PCs as the unit of scaling allows the service to 
ride the leading edge of the cost/performance curve, the inherent 
redundancy of clusters can be used to mask transient failures, and 
"embarrassingly parallel" network service workloads map well 
onto networks of workstations. However, developing cluster soft
ware and administering a running cluster remain complex. The pri
mary contributions of this work are the design, analysis, and 
implementation of a layered framework for building network ser
vices that addresses this complexity. New services can use this 
framework as an off-the-shelf solution to scalability, availability, 
and several other problems, and focus instead on the content of the 
service being developed. The lower layer handles scalability, avail
ability, load balancing, support for bursty offered load, and system 
monitoring and visualization, while the middle layer provides 
extensible support for caching, transformation among MIME 
types, aggregation of information from multiple sources, and per
sonalization of the service for each of a large number of users 
(mass customization). The top layer allows composition of trans
formation and aggregation into a specific service, such as acceler
ated Web browsing or a search engine.
d18 44
a61 39
Validation: Two Real Services
Our framework reflects the implementation of two real network 
services in use today: TranSend, a scalable transformation and 
caching proxy for the 25,000 Berkeley dialup IP users (connecting 
through a bank of 600 modems), and the Inktomi search engine 
(commercialized as HotBot), which performs millions of queries 
per day against a database of over 50 million web pages.
The Inktomi search engine is an aggregation server that was 
initially developed to explore the use of cluster technology to han
dle the scalability and availability requirements of network ser
vices. The commercial version, HotBot, handles several million 
queries per day against a full-text database of 54 million web 
pages. It has been incrementally scaled up to 60 nodes, provides 
high availability, and is extremely cost effective. Inktomi predates 
the framework we describe, and thus differs from it in some 
respects. However, it strongly influenced the framework's design, 
and we will use it to validate particular design decisions. 
We focus our detailed discussion on TranSend, which provides 
Web caching and data transformation. In particular, real-time, 
datatype-specific distillation and refinement [22] of inline Web 
images results in an end-to-end latency reduction by a factor of 3-5, 
giving the user a much more responsive Web surfing experience 
with only modest image quality degradation. TranSend was devel
oped at UC Berkeley and has been deployed for the 25,000 dialup 
IP users there, and is being deployed to a similar community at UC 
Davis.
In the remainder of this section we argue that clusters are an 
excellent fit for Internet services, provided the challenges we 
describe for cluster software development can be surmounted. In 
Section 2 we describe the proposed layered architecture for build
ing new services, and a programming model for creating services 
that maps well onto the architecture. We show how TranSend and 
HotBot map onto this architecture, using HotBot to justify specific 
design decisions within the architecture. Sections 3 and 4 describe 
the TranSend implementation and its measured performance, 
including experiments on its scalability and fault tolerance proper
ties. Section 5 discusses related work and the continuing evolution 
of this work, and we summarize our observations and contributions 
in Section 6.
d986 2
a987 1
Economic Feasibility
d990 1
a990 1
service. From our performance data, a US$5000 Pentium Pro 
d996 1
a996 1
50% or more, as we observed in our cache experiments, then we 
d998 1
a998 1
tion, which reduces operating costs by about US$3000 per month. 
d1002 1
a1002 1
TranSend would be minimal- we run TranSend at Berkeley with 
@


1.1.1.1
log
@Client and Network Adaptation paper for IEEE Pers Comms special issue,
to be submitted 2/98
@
text
@@
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              