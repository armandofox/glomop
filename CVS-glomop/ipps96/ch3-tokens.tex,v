head	1.3;
access;
symbols
	initial:1.1.1.1
	initial:1.1.1;
locks; strict;
comment	@% @;


1.3
date	96.01.25.01.59.33;	author fox;	state Exp;
branches;
next	1.2;

1.2
date	96.01.19.03.21.24;	author fox;	state Exp;
branches;
next	1.1;

1.1
date	96.01.19.02.01.11;	author fox;	state Exp;
branches
	1.1.1.1;
next	;

1.1.1.1
date	96.01.19.02.01.11;	author fox;	state Exp;
branches;
next	;


desc
@@


1.3
log
@final edits for ipps96
@
text
@ 
\subsection{Tokens}

To address this problem,
we implemented an alternate scheme for enforcing 1-on-1 communication
based on token passing.  A token is associated with each node in the system.
The holder of node $i$'s token has the exclusive right to send to node
$i$.  Since the communication is being structured as a sequence of
permutations, the movement of the token among the senders can easily
be determined in advance, and each sender passes the receiver
token to the next sender when it finishes sending.

For our experiments we implemented
a sequence of cyclic shifts as the permutations. At round $a$
each node $i$ sends to node ${(i+a)\%N}$, where $N$ is the
number of nodes. At the end of a round, each node $i$  
passes the token it is holding to node ${(i-1)\%N}$, i.e. the node
that will be sending to the same destination in the following
round.   

Figure \ref{tokens} illustrates the token passing mechanism in
action. Four nodes of an MPP are represented
by circles, the superscripted numbers represent the tokens held by each
node, solid arrows represent data transfer and dashed arrows represent
token passing messages.  Node 2 finishes sending early.  As soon as it
receives 4's token (from 3) it can begin sending to 4.  Eventually all
nodes advance to the next round, but if we had used barriers, 2 would
not have been able to move ahead until the last send of the round was
complete.
Token passing is effected
using active messages \cite{actmsg}, a low latency, low overhead
communications layer supported on all three
machines we used.  


\begin{figure}
\centerline{\psfig{figure=./graphs/tokens.eps,height=2.5in}}

\caption{The token mechanism
 with cyclic shift permutations.}
\label{tokens}
\end{figure}


        %Implementation is straightforward.  At the start of each round each
        %node blocks waiting for the arrival of the token corresponding to its
        %destination for the round.  After completing the send, the node
        %forwards the token to the next sender.  

\subsection{Discussion of Tokens}

Figures \ref{tokcm5} through \ref{tokalewife}
illustrate the performance of tokens relative to
permutations separated by barriers.  Note that on the Alewife and
Paragon, tokens always perform better than software barriers, for the reasons
outlined above.  However, on the CM-5, the hardware barriers
provide better performance for bulk-transfer sizes
up to about 1.5Kbytes.  For larger transfers, the relative overhead of
both barriers and tokens approaches zero, and we start to see
the benefit of relaxing the condition that all nodes wait at the
barrier.  

As described previously, the overhead and latency of software barriers
grows as nodes are added.  
The small number of nodes in the Alewife and Paragon systems we used
makes the barrier cost comparable to the token passing cost, 
but with more nodes one would expect tokens to outperform barriers by a
wider margin.

\begin{figure}
\centerline{\psfig{figure=./graphs/tokens.cm5.eps,height=\gheight}}
\caption{Tokens and barriers compared on the 64-node CM-5.}
\label{tokcm5}
\end{figure}

\begin{figure}
\centerline{\psfig{figure=./graphs/tokens.paragon.eps,height=\gheight}}
\caption{Tokens and barriers compared on the 8-node Paragon.}
\label{tokparagon}
\end{figure}

\begin{figure}
\centerline{\psfig{figure=./graphs/tokens.alewife.eps,height=\gheight}}
\caption{Tokens and barriers compared on the 4-node Alewife.}
\label{tokalewife}
\end{figure}


We experimented with and discarded an alternative scheme which we called
Receive Queue tokens (RQ tokens).  In this scheme each receiver
explicitly manages its own token by keeping a queue of token requests.
However, without barriers to enforce lock-step advancement, nodes drift
out of sync with each other, causing multiple senders queue up for a
single token.  This means that even if they have other data available
for an idle receiver, they cannot send it.  As a result, the network
becomes underutilized and overall performance is disappointing.  Figure
\ref{rq-tokens} compares the performance of this scheme with the other
schemes described so far, in a simple experiment on the Intel Paragon.

\begin{figure}
\centerline{\psfig{figure=./graphs/rq-tokens.eps,height=\gheight}}
\caption{Receiver-Queue token performance compared to other permutation
	methods on the 8-node Paragon.}
\label{rq-tokens}
\end{figure}
@


1.2
log
@first round edits for IPPS96 camera-ready
@
text
@d19 11
a29 2
round.   Figure \ref{tokens} illustrates the token passing mechanism in
action. 
d37 1
a37 1
\centerline{\psfig{figure=./graphs/tokens.eps,width=3in}}
d39 2
a40 9
\caption{{\bf The token mechanism
 with cyclic shift permutations.}  Four nodes of an MPP are represented
by circles, the superscripted numbers represent the tokens held by each
node, solid arrows represent data transfer and dashed arrows represent
token passing messages.  Node 2 finishes sending early.  As soon as it
receives 4's token (from 3) it can begin sending to 4.  Eventually all
nodes advance to the next round, but if we had used barriers, 2 would
not have been able to move ahead until the last send of the round was
complete.}
@


1.1
log
@Initial revision
@
text
@d1 1
a1 1

d4 2
a5 1
We implemented an alternate scheme for enforcing 1-on-1 communication
d19 2
a20 1
round. 
d53 3
a55 3
Paragon, tokens always perform better than barriers, for the reasons
outlined above.  However, on the CM-5, barriers are so inexpensive
that using them results in better performance for bulk-transfer sizes
d57 2
a58 2
both barriers and tokens becomes effectively zero, and we start to see
the effect of relaxing the condition that all nodes wait at the
d61 6
a66 9
The small number of nodes in the Alewife and Paragon systems
provides only a small bias against conducting a barrier.
The cost of a barrier is actually very close to the cost of
passing a token. With more nodes one would expect
the relative performance of tokens to
improve as compared to the barriers, as the overhead and
latency of the barrier increases.


@


1.1.1.1
log
@initial import matches original submission to IPPS 96.
@
text
@@
