head	1.7;
access;
symbols
	pre-camera-edits:1.5
	sigmetrics:1.1.1.1
	sigmetrics:1.1.1;
locks; strict;
comment	@% @;


1.7
date	98.03.16.19.15.48;	author gribble;	state Exp;
branches;
next	1.6;

1.6
date	98.03.13.23.53.50;	author gribble;	state Exp;
branches;
next	1.5;

1.5
date	97.11.03.10.03.12;	author gribble;	state Exp;
branches;
next	1.4;

1.4
date	97.11.02.00.19.13;	author gribble;	state Exp;
branches;
next	1.3;

1.3
date	97.11.01.00.05.27;	author gribble;	state Exp;
branches;
next	1.2;

1.2
date	97.10.31.23.38.37;	author gribble;	state Exp;
branches;
next	1.1;

1.1
date	97.10.30.21.43.25;	author gribble;	state Exp;
branches
	1.1.1.1;
next	;

1.1.1.1
date	97.10.30.21.43.25;	author gribble;	state Exp;
branches;
next	;


desc
@@


1.7
log
@Multo-changes for camera ready.
@
text
@\section{Theory of Self-Similarity}\label{theory_sec}

The theory behind self-similar processes is briefly presented in this
section.  A more thorough treatment can be found in \cite{Lela94},
\cite{Will95}, or \cite{Paxs94};  the goal of this section is to
outline enough of the theory to motivate the methodology
discussed in section \ref{measure_sec}.

Consider a stochastic process $X = (X_1, X_2, \ldots)$ with mean
$\mu = E\{X_i\}$, variance $\sigma^2 = E\{{(X_i - \mu)}^2\}$, and
autocorrelation function:
\begin{equation}
r(k) = \frac{E\{(X_i - \mu)(X_{i+k} - \mu)\}}{E\{{(X_i - \mu)}^2\}},
\; \; \makebox{for} \; k \geq 0.
\end{equation}
This process is said to exhibit {\it long-range dependence} if
\begin{equation}
r(k) \sim k^{-\beta}L(t) \; \; \makebox{for} \;\;  (0 < \beta < 2), \; \;
\makebox{as} \; k
\rightarrow \infty
\label{autodecay}
\end{equation}
and $L(t)$ is a slowly varying function with the property
\begin{equation}
\lim_{t\rightarrow\infty}L(tx)/L(t) = 1 \; \; \forall \; x > 0.
\end{equation}

A new aggregated time series $X^{(m)} = (X_k^{(m)} : k =
1,2,3,\ldots)$ for each $m = 1,2,3,\ldots$ is obtained by averaging
non-overlapping blocks of size $m$ from the original series $X$.
In other words,
\begin{equation}
X_k^{(m)} = \frac{(X_{km-m+1} + \cdots + X_{km})}{m}, \; \; k\geq1.
\end{equation}
The process $X$ is said to be {\it exactly second-order self-similar}
with Hurst parameter
\begin{equation}
H = 1 - \frac{\beta}{2} \; \; (0<\beta<2)
\label{beta-H}
\end{equation}
if, for any $m = 1,2,3,\ldots$,
\begin{equation}
{\displaystyle Var}\left(X^{(m)}\right) \propto m^{-\beta}
\label{variance-aggregation}
\end{equation}
and
\begin{equation}
r^{(m)}(k) = r(k) \; \; \; (k = 1,2,3,\ldots).
\end{equation}

If the weaker condition
\begin{equation}
r^{(m)}(k) \rightarrow r(k) \; \; \makebox{as} \; m \rightarrow \infty
\end{equation}
is true, then the process $X$ is said to be {\it asymptotically
second-order self-similar}, with Hurst parameter $H$ again given
by equation \ref{beta-H}.

% function of a self-similar process decays hyperbolically, as compared
% with the exponential decay of typical Poisson models used in network
% modeling.  This hyperbolic decay implies that the autocorrelation
% function is {\it non-summable}, that is $\sum_kr(k) = \infty$.

As described in \cite{Lela94}, self-similar processes provide an
explanation for an empirical law known as the {\it Hurst effect}.  The
{\it rescaled adjusted range (R/S) statistic}
for a set of observations $(X_k : k = 1,2,\ldots,n)$ having mean $\bar{X}(n)$
and sample variance $S^2(n)$
is given by the relation
\begin{eqnarray}
R(n)/S(n) = & (1/S(n)) [\max(0, W_1, W_2, \ldots, W_n) - \nonumber \\
   & \min(0, W_1, W_2, \ldots, W_n) ]
\end{eqnarray}
where
\begin{equation}
W_k = (X_1 + X_2 + \cdots + X_k) - k\bar{X}(n) \; \; (k \geq 1).
\end{equation}
Short-range dependent sets of observations seem to satisfy
$E[R(n)/S(n)] \sim c_0n^{\frac{1}{2}}$, while long-range dependent
processes such as self-similar process are observed to follow
\begin{equation}
E\left[\frac{R(n)}{S(n)}\right] \sim c_0n^H, \; \; (0 < H < 1).
\label{hurst-effect}
\end{equation}
This is known as the Hurst effect, and it can be used to differentiate
between self-similar and non-self-similar time series, as will be
demonstrated in section \ref{variance_plots}.\footnote{
The Hurst parameter can be broken down into three distinct categories; 
$H < 0.5$,
$H=0.5$, and $H > 0.5$. Ordinary Brownian motion (e.g., a random walk) is produced
when $H=0.5$. If $H>0.5$, the values produced are
self-similar with a positive correlation. If $H<0.5$, the values produced are
self-similar with a negative correlation. Negative correlated data tends to
reverse itself instead of continuing along the same path. Positive and
negative correlated data is also called persistent and anti-persistent data,
respectively. \cite{mand83} Most observed self-similar data
to date is persistent.}

@


1.6
log
@attaaack!
@
text
@d96 1
a96 1
respectively. \cite{mand83,peit92} Most observed self-similar data
@


1.5
log
@Added confidence intervals.  Added On/off model section.
@
text
@d7 1
a7 1
discussed in \ref{measure_sec}.
d86 1
a86 1
between self-similar and non self-similar time series, as will be
d95 1
a95 1
negative correlated data is also called persistent and anti-persistent data
@


1.4
log
@fixed up as per Tim's comments - minor wordsmithing, only.
@
text
@a58 1
% From equation \ref{autodecay}, we see that the autocorrelation
@


1.3
log
@More minor edits.
@
text
@d3 1
a3 1
The theory behind self-similar process is briefly presented in this
d38 1
a38 1
H = 1 - \frac{\beta}{2} \; \; (0<\beta<1)
@


1.2
log
@Another round of edits.
@
text
@d89 5
a93 4
The Hurst parameter can be broken down into three distinct categories; H<0.5,
H=0.5, and H>0.5. Ordinary Brownian motion (e.g., a random walk) is produced
when H=0.5. If H>0.5, the values produced are
self-similar with a positive correlation. If H<0.5, the values produced are
@


1.1
log
@Initial revision
@
text
@d6 1
a6 1
outline the enough of the theory to motivate the methodology
d18 1
a18 1
r(k) \sim k^{-\beta}L(t) \; \; \makebox{for} \;\;  (0 < \beta < 1), \; \;
d83 1
a83 1
E\left[\frac{R(n)}{S(n)}\right] \sim c_0n^H, \; \; (0.5 < H < 1).
d88 10
a97 2
demonstrated in section \ref{variance_plots}.

@


1.1.1.1
log
@Sigmetrics paper

@
text
@@
