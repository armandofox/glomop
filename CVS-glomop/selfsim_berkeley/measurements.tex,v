head	1.10;
access;
symbols
	pre-camera-edits:1.6
	sigmetrics:1.1.1.1
	sigmetrics:1.1.1;
locks; strict;
comment	@% @;


1.10
date	98.03.16.19.15.44;	author gribble;	state Exp;
branches;
next	1.9;

1.9
date	98.03.13.23.53.46;	author gribble;	state Exp;
branches;
next	1.8;

1.8
date	98.03.13.11.41.38;	author gribble;	state Exp;
branches;
next	1.7;

1.7
date	98.03.13.04.43.50;	author gribble;	state Exp;
branches;
next	1.6;

1.6
date	97.11.03.10.03.07;	author gribble;	state Exp;
branches;
next	1.5;

1.5
date	97.11.02.00.19.08;	author gribble;	state Exp;
branches;
next	1.4;

1.4
date	97.11.01.00.05.26;	author gribble;	state Exp;
branches;
next	1.3;

1.3
date	97.10.31.23.38.34;	author gribble;	state Exp;
branches;
next	1.2;

1.2
date	97.10.31.01.28.06;	author gribble;	state Exp;
branches;
next	1.1;

1.1
date	97.10.30.21.43.25;	author gribble;	state Exp;
branches
	1.1.1.1;
next	;

1.1.1.1
date	97.10.30.21.43.25;	author gribble;	state Exp;
branches;
next	;


desc
@@


1.10
log
@Multo-changes for camera ready.
@
text
@\section{File System Traces}\label{fs_traces}

We examine a total of four file system traces in this paper.  The first set
of traces was collected in 1991 for a study of the file access patterns and
caching behaviour of the Sprite distributed file system \cite{Bake91}.  The
traces were collected on a Sprite cluster of approximately 40 workstations
sharing a single Ethernet, over eight separate 24 hour intervals.  Because
of the nature of the Sprite distributed file system and the manner in which
the traces were gathered, all file system events required communication
between the client and the server; thus, all file system events could be
gathered at the servers themselves.  The events within the file trace were
collected through kernel call instrumentation on client machines with a
precision of 10 milliseconds.  All file system requests (including those
that are satisfied by a client-side cache) are present within the trace.
The Sprite traces suffer from some minor limitations, such as a lack of
fully complete information on file read and write events.  The total number
of bytes read from and written to files could be deduced from the seek and
close events recorded in the trace.  However, each individual file read and
write event could not be recovered --- these events were recorded in our
second set of traces.

The second set of traces used in this paper was gathered from a relatively
large NFS installation served by a single Auspex NFS server.  These traces
were collected in 1994 to study the impact of different cache policies on
scalable network file system performance \cite{Dahl94}.  The Auspex NFS
server straddled four separate Ethernets, and served a total of 237
clients.  The traces were collected by monitoring network activity on each
of the four Ethernets.  This implies that any file system request satisfied
by a client-side NFS cache was not present in the trace.  The NFS traces
available to us had been post-processed in order to remove NFS-related
artifacts and deficiencies.  For example, file open and close events were
not visible from the network, and had to be added to the traces using a
heuristic method.  Postprocessing was also performed on NFS attribute
operations; it was assumed that most attribute read operations within the
trace were really NFS cache consistency validation operations, and thus
some were removed from the trace.  Anomalies from this postprocessing
likely introduced some minor distortion into our results, but this
distortion is only on a very fine time scale (on the order of
milliseconds), and the fact that the statistical methods used to detect
self-similarity take into account many time scales (up to many hours)
implies that the effects of such distortion are minimal.

The third set of traces was collected on a cluster of twenty instructional
UNIX workstations used by undergraduate classes at UC Berkeley
\cite{ros_trace}. These long term traces (referred to as the INS traces
throughout the paper) were gathered by using the auditing system to log all
system calls relevant to the file system on each workstation.  Because the
traces were collected on the client workstations, requests that are
absorbed by the local cache are present in the traces.  File system calls
recorded in the raw traces include individual data operations, such as read
and write requests, as well as metadata operations, such as open, stat, and
chmod. However, in the processed version of these traces that is analyzed
in this paper, some calls (such as lseek and dup) have been removed from
the traces, and consecutive read or write operations made by the same
process during the same second are coalesced.  Time is recorded on the
granularity of one second.  These traces were collected continuously for
six and one half months over the fall of 1996 and spring of 1997.

\begin{footnotesize}
\begin{table}[tbph]
\centering
\begin{tabular}{|c|c|c|c|c|c|} \hline
{\bf Event} & 
{\#Sprite} &
\multicolumn{3}{c|}{\# NFS Events} &
{\# INS} \\ 
{\bf Type}  & {Events} & \multicolumn{1}{c|}{R} & W & RW & {Events} \\ \hline\hline

{\bf Open} & 38668 & 66845 & 8457 & 520 & 229968287 \\ \hline

{\bf Close} & 48412 & 66714 & 8455 & 518 & 232488430 \\ \hline

{\bf Block Xfer} & n/a & 344564 & 71399 & 0 & 633158006 \\ \hline

{\bf Delete} & 5764 & 0 & 3327 & 0 & 3147359 \\ \hline

{\bf Attribute} & unused & 1374402 & 144336 & 0 & 857738997 \\ \hline

{\bf Directory} & unused & 1890262 & 0 & 20201 & 32176538 \\ \hline

{\bf Seeks} & 28371 & \multicolumn{3}{c|}{n/a} & n/a \\ \hline\hline

%%%%%%%%%% Total events
{\bf Total \#:} &
121215 &
\multicolumn{3}{c|}{4000000} &
1988677617
\\ \hline

%%%%%%%%%% Total time of trace
{\bf Time:} &
14413s&
\multicolumn{3}{c|}{91337s} &
17448960s\\ 

 & (4h) & \multicolumn{3}{c|}{(25.37h)}
 & (6 mnth) \\ \hline


\end{tabular}
\caption{{\bf Trace summaries:}  this table summarizes the number and
type of events available in the Sprite, Auspex NFS, and INS traces.}
\label{trace_summary}
\end{table}
\end{footnotesize}

\begin{footnotesize}
\begin{table}[tbph]
\centering
\begin{tabular}{|c|c|c|} \hline
{\bf Trace Attribute} & \multicolumn{2}{c|}{\bf Value} \\ \hline\hline

{FS type} & \multicolumn{2}{c|}{NFS} \\ \hline
FS size & \multicolumn{2}{c|}{7.5 GB} \\ \hline
\# files & \multicolumn{2}{c|}{~250,000} \\ \hline
\# users & \multicolumn{2}{c|}{~500} \\ \hline
\# workstation clients & \multicolumn{2}{c|}{~150} \\ \hline
Trace duration & \multicolumn{2}{c|}{287 days} \\ \hline
Trace start & \multicolumn{2}{c|}{October 23rd, 1996} \\ \hline
\# Creations & \multicolumn{2}{c|}{443,516 (1,545/day)} \\
\# Deletions & \multicolumn{2}{c|}{314,333 (1,100/day)} \\
\# File Accesses & \multicolumn{2}{c|}{807,968 (2,815/day)} \\
\# Modifications & \multicolumn{2}{c|}{108,262 (377/day)} \\ \hline
\end{tabular}
\caption{{\bf Summary of the UMBC trace data}}
\label{ustar_summary}
\end{table}
\end{footnotesize}

The final set of traces were collected daily on a portion of the University
of Maryland Baltimore County's file systems to generate data for a
tape-migration simulation.  The tracing period spanned 287 days. These
traces (referred to as the UMBC traces) were not collected with kernel
modifications; instead, a modified version of the ``find'' utility was run
nightly to collect information from a number of file systems in the
University.  This tracing process did not provide the fine-grained data the
Sprite and NFS traces possess --- the tape-migration simulator did not
require such fine-grained measurements.  For example, on the Sprite system
traces, a user modifying a file will generate an open, a close, and many
intervening reads and writes. The UMBC traces only show one action, a file
modification. Thus, the long-term traces collapse many events into
one. Similarly, the trace collection process does not track how many times
a file is used during a day.  Finally, it misses all the temporary files
the system creates and deletes during the day --- Ousterhout \cite{oust85}
noted that 80\% of all file creations have a lifetime of less than three
minutes; all of these are missed.

Tables \ref{trace_summary} and \ref{ustar_summary} summarizes the number of
events within the portions of the traces analyzed in the paper.  For the
Sprite file system traces, four hours worth of events from the most heavily
used server, which had over 75\% of the Sprite cluster's file system
events.  Access privileges (i.e., read, write, or read/write access) were
not considered for Sprite events in this section of the paper.  Four
million events were analyzed from the Auspex traces --- this represents
approximately 25 hours worth of trace data.  All events from the INS and
the UMBC traces were considered.

\subsection{ Visualizing the Traces }\label{vis_trace_sec}

Figure \ref{vis_demo_ss} illustrates the INS traces across 6 orders of
magnitude of time scales.  Each plot within the figure represents file
system event activity, enumerated as the number of events per time unit.
Successive plots are refinements of the previous plots; the top plot in
each column has a time unit of 32,768 seconds, the second of 4,096 seconds,
and so on.  The left column illustrates file system read or write events
(summarized as XFER events), and the right column illustrates attribute
read or write events (ATTR).

\begin{figure*}[tbh]
\epsfxsize .75\hsize
\epsfysize .55\vsize
\begin{center}
\makebox{
\epsfbox[0 0 513 568]{./figures/newvis.eps}
}
\end{center}
\caption{{\bf Visualization of the traces:}  INS file
system events per unit time (or ``bucket'') are plotted for 6 increasingly
refined time units, varying by a total factor of 32,768.  Column (a) shows
read and write operations, and column (b) shows file attribute read or
modification events.  The x-axis of all graphs is the number of seconds
since the start of the trace that each bucket begins; the y-axis shows the
number events that occurred during that bucket.}\label{vis_demo_ss}
\end{figure*}

The most obvious feature of the XFER events is that they appear bursty at
all time scales, although the burstiness appears to smooth out at coarser
time granularities.  For example, at the 500-second time scale, the
peak-to-average ratio is about 60:1, but by the 6 month time scale, the
ratio drops to about 5:1.  For the ATTR events, a similar but even more
pronounced effect can be observed.  At fine time granularities, very
bursty, unpredictable behaviour occurs.  For coarse granularities (3-day
and above), the traces become extremely smooth and predictable, although a
small number of large spikes can be seen to interrupt this
smoothness.\footnote{Closer examination of these large spikes at the coarse
time scales revealed that they were caused by misbehaving or extremely
poorly written programs.  For example, a runaway program that was
repeatedly reading a {\tt .logout} file dominated all other file system
activities for 2 days, causing a large spike to appear in figure
\ref{vis_demo_ss}.} A very strong diurnal cycle can also be seen in both
event traces at the 3-day and 24-day scales.

Process models that are typically used while modeling systems events (such
as those having Poisson or Markov modulated Poisson interarrival times)
tend to look uniform at large time scales, primarily because of their
finite variance and short-tailed distributions.  In contrast, self-similar
processes exhibit scale invariance;  the processes will appear to be bursty
at all time scales, and will not degrade to uniformity.  Figure
\ref{vis_demo_ss} contains characteristics of both process models;
across short time scales (less than a day), the process appears bursty,
while across larger time scales (more than a few days), the process smooths
out to some degree.

@


1.9
log
@attaaack!
@
text
@d150 4
a153 5
Sprite file system traces, four hours worth of events gathered at the
``allspice'' file server were considered, since this server was the most
heavily burdened, receiving over 75\% of the entire Sprite cluster's file
system events.  Access privileges (i.e., read, write, or read/write access)
were not considered for Sprite events in this section of the paper.  Four
d160 8
a167 8
Figure \ref{vis_demo_ss} illustrates the INS traces across more than 5
orders of magnitude of time scales.  Each plot within the figure represents
file system event activity, enumerated as the number of events per time
unit.  Successive plots are refinements of the previous plots; the top plot
in each column has a time unit of 32,768 seconds, the second of 4,096
seconds, and so on.  The left column illustrates file system read or write
events (summarized as XFER events), and the right column illustrates
attribute read or write events (ATTR).
@


1.8
log
@Many changes to modify spin of paper.
@
text
@d5 1
a5 1
caching behaviour of the Sprite distributed file system\cite{Bake91}.  The
d12 1
a12 1
collected through kernel calls instrumentation on client machines with a
d19 1
a19 1
write event could not be recovered -- these events were recorded in our
d25 1
a25 1
scalable network file system performance\cite{Dahl94}.  The Auspex NFS
a58 28
The final set of traces were collected daily on a portion of the University
of Maryland, Baltimore County's file systems to generate data for a
tape-migration simulation.  The tracing period spanned 287 days. These
traces (referred to as the UMBC traces) were not collected with kernel
modifications; instead, a modified version of the ``find'' utility was run
nightly to collect information from a number of file systems in the
University.  This tracing process did not provide the fine-grained data the
Sprite and NFS traces possess - the tape-migration simulator did not
require such fine-grained measurements.  For example, on the Sprite system
traces, a user modifying a file will generate an open, a close, and many
intervening reads and writes. The UMBC traces only show one action, a file
modification. Thus, the long-term traces collapse many events into
one. Similarly, the trace collection process does not track how many times
a file is used during a day.  Finally, it misses all the temporary files
the system creates and deletes during the day - Ousterhout \cite{oust85}
noted that 80\% of all file creations have a lifetime of less than three
minutes; all of these are missed.

Tables \ref{trace_summary} and \ref{ustar_summary} summarizes the number of
events within the portions of the traces analyzed in the paper.  For the
Sprite file system traces, 4 hours worth of events gathered at the
``allspice'' file server were considered, since this server was the most
heavily burdened, receiving over 75\% of the entire Sprite cluster's file
system events.  Access privileges were not considered for Sprite events in
this section of the paper.  Four million events were analyzed from the
Auspex traces - this represents approximately 25 hours worth of trace data.
All events from the INS and the UMBC traces were considered.

d130 28
d191 2
a192 2
ratio has dropped to about 5:1.  For the ATTR events, a similar but even
more pronounced effect can be observed.  At fine time granularities, very
d208 1
a208 1
processes exhibit scale-invariance;  the processes will appear to be bursty
d210 1
a210 1
\ref{vis_demo_ss} contains characteristics of both classes of processes;
@


1.7
log
@Hacking away, and the world comes crumbling down.
@
text
@d1 1
a1 1
\section{File System Traces}
d44 1
a44 1
workstations used by undergraduate classes at UC Berkeley
d97 1
a97 1
{\bf Open} & 38668 & 66845 & 8457 & 520 & XXX \\ \hline
d99 1
a99 1
{\bf Close} & 48412 & 66714 & 8455 & 518 & XXX \\ \hline
d101 1
a101 1
{\bf Block Xfer} & n/a & 344564 & 71399 & 0 & XXX \\ \hline
d103 1
a103 1
{\bf Delete} & 5764 & 0 & 3327 & 0 & XXX \\ \hline
d105 1
a105 1
{\bf Attribute} & unused & 1374402 & 144336 & 0 & XXX \\ \hline
d107 1
a107 1
{\bf Directory} & unused & 1890262 & 0 & 20201 & XXX \\ \hline
d109 1
a109 1
{\bf Seeks} & 28371 & \multicolumn{3}{c|}{n/a} & XXX \\ \hline\hline
d115 1
a115 1
XXX
d159 1
a159 1
\subsection{ Visualizing the Traces }
d199 4
a202 3
repeatedly reading a {\tt .logout} file caused a 2-day long spike that
dominated all other file system traffic.} A very strong diurnal cycle can
also be seen in both event traces at the 3-day and 24-day scales.
@


1.6
log
@Added confidence intervals.  Added On/off model section.
@
text
@d1 1
a1 16
\section{Self-Similarity in File Systems (Short-term)}\label{measure_sec}

In this section we analyze two previously gathered sets of file system
traces in order to detect self-similarity in high-level file system events.
The first set of traces was collected in 1991 for a study of the file
access patterns and caching behaviour of the Sprite distributed file
system\cite{Bake91}.  The traces were collected on a Sprite cluster of
approximately 40 workstations sharing a single Ethernet, over eight
separate 24 hour intervals.  Because of the nature of the Sprite
distributed file system and the manner in which the traces were gathered,
all file system events required communication between the client and the
server; thus, all file system events could be gathered at the servers
themselves.  The events within the file trace were collected through kernel
calls instrumentation on client machines with a precision of 10
milliseconds.  All file system requests (including those that are satisfied
by a client-side cache) are present within the trace.
d3 12
d29 10
a38 11
by a client-side NFS cache was not present in the trace.

The NFS traces available to us had been post-processed in order to remove
NFS-related artifacts and deficiencies.  For example, file open and close
events were not visible from the network, and had to be added to the traces
using a heuristic method.  Postprocessing was also performed on NFS
attribute operations; it was assumed that most attribute read operations
within the trace were really NFS cache consistency validation operations,
and thus some were removed from the trace.  Anomalies from this
postprocessing likely introduced some minor distortion into our results,
but this distortion is only on a very fine time scale (on the order of
d43 43
a85 8
Table \ref{trace_summary} summarizes the number of events within the
portions of the traces analyzed in the paper.  For the Sprite file system
traces, 4 hours worth of events gathered at the ``allspice'' file server
were considered, since this server was the most heavily burdened, receiving
over 75\% of the entire Sprite cluster's file system events.  Access
privileges were not considered for Sprite events in this section of the
paper.  Four million events were analyzed from the Auspex traces - this
represents approximately 25 hours worth of trace data.
d87 1
d90 1
a90 1
\begin{tabular}{|c|c|c|c|c|} \hline
d93 3
a95 2
\multicolumn{3}{c|}{\# NFS Events} \\ 
{\bf Type}  & {Events} & \multicolumn{1}{c|}{R} & W & RW \\ \hline\hline
d97 1
a97 1
{\bf Open} & 38668 & 66845 & 8457 & 520 \\ \hline
d99 1
a99 1
{\bf Close} & 48412 & 66714 & 8455 & 518 \\ \hline
d101 1
a101 1
{\bf Block Xfer} & n/a & 344564 & 71399 & 0 \\ \hline
d103 1
a103 1
{\bf Delete} & 5764 & 0 & 3327 & 0 \\ \hline
d105 1
a105 1
{\bf Attribute} & unused & 1374402 & 144336 & 0 \\ \hline
d107 1
a107 1
{\bf Directory} & unused & 1890262 & 0 & 20201 \\ \hline
d109 1
a109 1
{\bf Seeks} & 28371 & \multicolumn{3}{c|}{n/a} \\ \hline\hline
d114 3
a116 1
\multicolumn{3}{c|}{4000000} \\ \hline
d121 2
a122 1
\multicolumn{3}{c|}{91337s} \\ 
d124 2
a125 1
 & (4h) & \multicolumn{3}{c|}{(25.37h)} \\ \hline
d130 1
a130 1
type of events available in each of the file system traces.}
d133 1
a133 100

\subsection{ Detection of Self-Similarity }

\begin{figure*}[tbh]
\epsfxsize 0.75\hsize
\begin{center}
\makebox{
\epsfbox[0 0 413 307]{./figures/picstot.eps}
}
\end{center}
\caption{{\bf Visual demonstration of self-similarity:}  Sprite file
system events per unit time are plotted for 6 increasingly refined
time units, varying by a total factor of 150.}\label{vis_demo_ss}
\end{figure*}

The intuitive definition of a self-similar process is that the process
looks similar across all time-scales.  Figure \ref{vis_demo_ss}
provides pictorial evidence of the self-similarity of Sprite file
system events.  Each plot within the figure represents total file
system event activity (enumerated as the number of events per time
unit).  Successive plots are refinements of the previous plots; the
first plot (\ref{vis_demo_ss}a) has a time unit of 15 seconds, the
second (\ref{vis_demo_ss}b) of 8.1 seconds, and so on.  The domains of
the refined plots were chosen by selecting some arbitrary interval
from its more detailed plot.

The most obvious feature of these plots is the fact that it is difficult to
distinguish among them.  The file system event process appears to exhibit
a characteristically bursty behaviour that is independent of the time scale
at which it is displayed.  This burstiness provides strong evidence that
the process is in fact self-similar.  In comparison, the process models
that are typically used while modeling systems events (such as those having
Poisson or Markov modulated Poisson interarrival times) tend to look
uniform at large time scales, primarily because of their finite variance
and short-tailed distributions.  It is because self-similar processes have
heavy-tailed distributions that burstiness can be observed at all time
scales.

Note that the plot for the smallest time scale (\ref{vis_demo_ss}f) shows
discretization artifacts, due to the small number of events arriving per
time unit.  The finite capabilities of computer systems intrinsically limit
the file system event arrival rate; file system events are thus only {\it
asymptotically self-similar} (as defined in \cite{Geor94}) in the limit of
large time scales.

\subsection{Variance-time plots}\label{variance_plots}

We can take advantage of equation \ref{variance-aggregation} to
more rigorously verify the self-similar nature of a
process, and to estimate the value of the Hurst parameter $H$.
Taking the logarithm of both sides of the equation results in the
relation
\begin{equation}
 \log\left({\displaystyle Var}\left(X^{(m)}\right)\right) = c_1
- \beta\log\left(m\right)
\end{equation}
for some constant $c_1$.  Thus, plotting $\log\left({\displaystyle
Var}\left(X^{(m)}\right)\right)$ versus $\log\left(m\right)$ for many
values of $m$ of a self-similar process will result in a linear series
of points with slope $-\beta$;  this plot is known as a {\it
variance-time plot}.

\begin{figure}[tbh]
\epsfxsize 0.85\hsize
\begin{center}
\makebox{
\epsfbox[0 0 256 333]{./figures/varplots.eps}
}
\end{center}
\caption{{\bf Variance-time plots:}  Variance plots for (a) Sprite
file open events and (b) NFS file open (W) events are illustrated, and
linear curve fits displayed.}\label{varplots}
\end{figure}

Given a variance-time plot, an estimate of $H$ can be obtained by
calculating the slope $\beta$ and using equation \ref{beta-H}.  Slopes
between -2 and 0 correspond to Hurst parameters $H$ between 0.5 and 1;
if $0.5 < H < 1$, then the process is self-similar.  Figure
\ref{varplots} illustrates the variance-time plot for Sprite file open
events (\ref{varplots}a) and NFS file open (W) events (\ref{varplots}b).

Two characteristics of the Sprite and NFS variance-time plots are
noteworthy - first, both are extremely linear, and have Hurst
parameters of 0.874 and 0.749, respectively, which verifies
the self-similar nature of the processes.  Secondly, the NFS plot
noticeably tapers away from the best-fit line for small values of
$\log(m)$.  There are two factors that contribute to this divergence.

The file open and close events within the NFS traces were
heuristically added during the postprocessing.  The placement of these
events could potentially be inaccurate on an order of hundreds of
milliseconds or even a small number of seconds.  For low values of the
aggregation parameter $m$, the reported events per unit time in the
trace thus contain errors.  However, since the time unit increases
with the aggregation parameter, the contribution of the errors falls
as the level of aggregation gets larger.  A second source of error is
the noticeable discretization for low time units.  As previously
mentioned, this means that the process is really asymptotically
self-similar, and variance plots therefore should become linear in the
limit of large $m$.
d136 1
a136 1
\begin{table*}[tbph]
d138 14
a151 37
\begin{tabular}{|c|c|c|c|c|} \hline
{\bf Event Type} &
{\bf Sprite} &
\multicolumn{3}{c|}{\bf NFS $H$} \\
 & $H$ & \multicolumn{1}{c|}{R} & W & RW \\ \hline\hline

{\bf Open} & $0.8739 \pm 0.0012$ & 
	$0.8686 \pm 0.0055$ & 
	$0.7489 \pm 0.0039$ & 
	$0.5220 \pm 0.0016$ \\ \hline

{\bf Close} & $0.8695 \pm 0.0012$ & 
	$0.8753 \pm 0.0070$ & 
	$0.7355 \pm 0.0035$ & 
	$0.5268 \pm 0.0023$ \\ \hline

{\bf Block Xfer} & n/a & 
	$0.8647 \pm 0.0012$ & 
	$0.8681 \pm 0.0050$ & n/a \\ \hline

{\bf Delete} & $0.7773 \pm 0.0045$ & n/a & 
	$0.8489 \pm 0.0070$ & n/a \\ \hline

{\bf Attribute} & unused & 
	$0.8156 \pm 0.0040$ & 
	$0.9899 \pm 0.0007$ & n/a \\ \hline

{\bf Directory} & unused & 
	$0.9832 \pm 0.0005$ & n/a & 
	$0.7329 \pm 0.0039$ \\ \hline

{\bf Seeks} & $0.8323 \pm 0.0073$ & \multicolumn{3}{c|}{n/a} \\ \hline\hline

%%%%%%%%%% All events
{\bf All events} &
$0.9033 \pm 0.0011$ &
\multicolumn{3}{c|}{$0.9688 \pm 0.0008$} \\ \hline
d153 3
a155 5
\caption{{\bf Estimates of $H$ from variance-time plots:} This table
summarizes the estimated values of the Hurst parameters $H$ derived
from variance-time plots of the Sprite and NFS trace data.}
\label{var_summary}
\end{table*}
a157 47
Variance-time plots were generated for all of the events listed in table
\ref{trace_summary}; the resulting Hurst parameter estimates are listed in
table \ref{var_summary}.  In all cases except for open and close RW events
within the NFS traces, all estimated Hurst parameters are well above 0.5,
indicating that specific file system event types are self-similar.  The two
cases for which the estimated Hurst parameter is low (NFS open and close
events with RW privileges) correspond to events for which scant amounts of
trace data was available (refer to table \ref{trace_summary}).  These two
estimated Hurst parameters are thus unreliable.

Somewhat surprisingly, the estimated values of $H$ for the 1991 Sprite file
system events seem to match their 1994 NFS counterparts.  For example, the
estimated value of 0.8734 for Sprite open events is quite close to the NFS
open (R) value of 0.8413, although it is greater than the NFS open (W) and
open (RW) values of 0.7491 and 0.5286, respectively.  Considering that the
number of NFS open (R) events dominate the NFS open (W) and open (RW)
events, this is not unreasonable.  However, as indicated by the confidence
intervals on these $H$ estimates, the NFS and Sprite values' differences
are statistically significant.

Another observation that can be made is that the measured Hurst parameters
for open events closely match the measured values for close events.  Open
events are quickly followed by close events; the open and close event
processes should therefore be equally bursty and asymptotically
self-similar.

In summary, variance-time analysis has revealed that:

\begin{itemize}
\item short-term file system traffic is self-similar in nature.
\item the Hurst parameters (a measure of burstiness) measured from the 
      NFS and the Sprite traces are closely correlated (but not in a
      statistically significant manner), suggesting the
      possibility that this parameter is to some degree independent of the
      type of file system measured.
\end{itemize}

\subsection{R/S-Analysis and Pox plots}\label{pox_plots}

A second estimate of the Hurst parameter can be obtained through R/S
analysis (originally presented in \cite{Mand69}, and fully explained in
\cite{Bera95}).  Given a set of observations
$(X_k : k = 1, 2, \ldots, N)$, that set is subdivided into $K$ disjoint,
contiguous subsets of length $(N/K)$.  The R/S statistic $R(t_i, n)/S(t_i, n)$
(equation \ref{hurst-effect}) is then computed for the starting
points $t_i$ of the $K$ disjoint subsets and for values of $n$
satisfying the relationship $(t_i - 1) + n \leq N$.  
d159 14
a172 2
\begin{figure}[tbh]
\epsfxsize 0.85\hsize
d175 1
a175 1
\epsfbox[0 0 312 333]{./figures/Pox.eps}
d178 8
a185 13
\caption{{\bf Pox plots:} Pox plots for (a) Sprite file open events
and (b) NFS file open (W) events are illustrated, and linear curve
fits displayed.}\label{poxplots}
\end{figure}

From equation \ref{hurst-effect} can derived:
\begin{equation}
\log\left( \frac{R(t_i, n)}{S(t_i, n)}\right) = c_2 + H\log(n).
\end{equation}
Plotting $\log\left(R(t_i, n)/S(t_i, n)\right)$ versus
$\log(n)$ should therefore result in a roughly linear graph with a
slope equal to the Hurst parameter $H$; such a graph is known as a
{\it Pox plot}.
d187 26
a212 40
\begin{footnotesize}
\begin{table*}[tbph]
\centering
\begin{tabular}{|c|c|c|c|c|} \hline
{\bf Event Type} &
{\bf Sprite} &
\multicolumn{3}{c|}{\bf NFS $H$} \\
 & $H$ & \multicolumn{1}{c}{R} & W & RW \\ \hline\hline

{\bf Open} & $0.8898 \pm 0.0047$ & 
	$0.8521 \pm 0.0052$ & 
	$0.7472 \pm 0.0059$ & 
	$0.5696 \pm 0.0039$ \\ \hline
{\bf Close} & $0.8750 \pm 0.0042$ & 
	$0.8464 \pm 0.0050$ & 
	$0.7324 \pm 0.0054$ & 
	$0.5675 \pm 0.0039$ \\ \hline
{\bf Block Xfer} & n/a & 
	$0.8071 \pm 0.0042$ & 
	$0.8424 \pm 0.0077$ & n/a \\ \hline
{\bf Delete} & $0.7463 \pm 0.0040$ & n/a & 
	$0.7973 \pm 0.0091$ & n/a \\ \hline
{\bf Attribute} & unused & 
	$0.7578 \pm 0.0048$ & 
	$0.7345 \pm 0.0100$ & n/a \\ \hline
{\bf Directory} & unused & 
	$0.9316 \pm 0.0057$ & n/a & 
	$0.6659 \pm 0.0054$ \\ \hline
{\bf Seeks} & $0.7712 \pm 0.0049$ & \multicolumn{3}{c|}{n/a} \\ \hline\hline

{\bf All events} &
$0.9098 \pm 0.0044$ &
\multicolumn{3}{c|}{$0.8960 \pm 0.0048$} \\ \hline
\end{tabular}
\caption{{\bf Estimates of $H$ from Pox plots:} This table
summarizes the estimated values of the Hurst parameters $H$ derived
from Pox plots of the Sprite and NFS trace data.}
\label{pox_summary}
\end{table*}
\end{footnotesize}
a213 45
Figure \ref{poxplots} contains Pox plots for the same Sprite file open
and NFS file open (W) events as were used to produce the variance-time
plots of figure \ref{varplots}.  A least-squares linear fit of the
data produced estimated Hurst parameter values of 0.890 and 0.747,
respectively.  These estimates are extremely close to the previously
estimated values of 0.874 and 0.749, which gives confidence to
the variance and Pox plot analysis techniques as well as to the
estimated values of $H$.

Estimates of $H$ were generated through R/S analysis for all of the
available file system events.  Table \ref{pox_summary} summarizes the
results of this analysis.  Once again, all estimated Hurst parameters are
well above 0.5, confirming the presence of self-similarity in the file
system traces.  Furthermore, the correlation between all Pox plot generated
$H$ estimates and the previously presented variance-time plot generate $H$
estimates is extremely high, although the values' confidence intervals do
not overlap.  With the exception of NFS attribute (W) events, all Pox plot
and variance-time plot estimates were between 5-10\% percent of each other.

The difference between the two measured H estimates for NFS attribute
(W) events is startling, and cannot be easily explained.  The
uncharacteristically high value of the variance-time plot estimate
(0.9934) suggests the possibility of an error in either the attribute
(W) event traces, or perhaps is a result of the anomalies introduced
during the NFS trace post-processing stage.

To summarize,

\begin{itemize}
\item R/S analysis of short-term file system traffic has 
      confirmed that this traffic is self-similar.
\item R/S analysis and variance-time analysis have produced consistent
      measures of the degree of burstiness of the two traces.
\item there is once again a noticeable correlation between the Hurst
      parameters produced for the NFS and Sprite traces via R/S analysis, 
      although the parameter estimates' confidence intervals don't overlap.
\end{itemize}

We have convincingly shown that short-term file system traffic is
self-similar. By the very nature of self-similarity, file system traffic
over a longer period should also be self-similar.  However, to convincingly
prove that file system traffic is self-similar requires long-term data that
the Sprite and NFS traces do not provide.  The next section of the paper
discusses a set of long-term file system traces that overcome this
shortcoming.
@


1.5
log
@fixed up as per Tim's comments - minor wordsmithing, only.
@
text
@a0 1

a98 11
The intuitive definition of a self-similar process is that the process
looks similar across all time-scales.  Figure \ref{vis_demo_ss}
provides pictorial evidence of the self-similarity of Sprite file
system events.  Each plot within the figure represents total file
system event activity (enumerated as the number of events per time
unit).  Successive plots are refinements of the previous plots; the
first plot (\ref{vis_demo_ss}a) has a time unit of 15 seconds, the
second (\ref{vis_demo_ss}b) of 8.1 seconds, and so on.  The domains of
the refined plots were chosen by selecting some arbitrary interval
from its more detailed plot.

d111 11
a157 7
Given a variance-time plot, an estimate of $H$ can be obtained by
calculating the slope $\beta$ and using equation \ref{beta-H}.  Slopes
between -2 and 0 correspond to Hurst parameters $H$ between 0.5 and 1;
if $0.5 < H < 1$, then the process is self-similar.  Figure
\ref{varplots} illustrates the variance-time plot for Sprite file open
events (\ref{varplots}a) and NFS file open (W) events (\ref{varplots}b).

d170 7
d198 1
a198 1
\begin{table}[tbph]
d206 24
a229 9
{\bf Open} & 0.8739 & 0.8413 & 0.7491 & 0.5286 \\ \hline

{\bf Close} & 0.8595 & 0.8432 & 0.7478 & 0.5363 \\ \hline

{\bf Block Xfer} & n/a & 0.8599 & 0.8478 & n/a \\ \hline

{\bf Delete} & 0.7773 & n/a & 0.8292 & n/a \\ \hline

{\bf Attribute} & unused & 0.8363 & 0.9934 & n/a \\ \hline
d231 1
a231 3
{\bf Directory} & unused & 0.9857 & n/a & 0.7413 \\ \hline

{\bf Seeks} & 0.8322 & \multicolumn{3}{c|}{n/a} \\ \hline\hline
d235 2
a236 2
0.9033 &
\multicolumn{3}{c|}{0.973} \\ \hline
d242 1
a242 1
\end{table}
d261 3
a263 1
events, this is not unreasonable.
d276 2
a277 1
      NFS and the Sprite traces are closely correlated, suggesting the
a292 9
From equation \ref{hurst-effect} can derived:
\begin{equation}
\log\left( \frac{R(t_i, n)}{S(t_i, n)}\right) = c_2 + H\log(n).
\end{equation}
Plotting $\log\left(R(t_i, n)/S(t_i, n)\right)$ versus
$\log(n)$ should therefore result in a roughly linear graph with a
slope equal to the Hurst parameter $H$; such a graph is known as a
{\it Pox plot}.

d305 8
a312 8
Figure \ref{poxplots} contains Pox plots for the same Sprite file open
and NFS file open (W) events as were used to produce the variance-time
plots of figure \ref{varplots}.  A least-squares linear fit of the
data produced estimated Hurst parameter values of 0.890 and 0.747,
respectively.  These estimates are extremely close to the previously
estimated values of 0.874 and 0.749, which gives confidence to
the variance and Pox plot analysis techniques as well as to the
estimated values of $H$.
d315 1
a315 1
\begin{table}[tbph]
d323 20
a342 7
{\bf Open} & 0.8898 & 0.8521 & 0.7472 & 0.5696 \\ \hline
{\bf Close} & 0.8750 & 0.8464 & 0.7324 & 0.5675 \\ \hline
{\bf Block Xfer} & n/a & 0.8071 & 0.8424 & n/a \\ \hline
{\bf Delete} & 0.7463 & n/a & 0.7973 & n/a \\ \hline
{\bf Attribute} & unused & 0.7578 & 0.7345 & n/a \\ \hline
{\bf Directory} & unused & 0.9316 & n/a & 0.6659 \\ \hline
{\bf Seeks} & 0.7712 & \multicolumn{3}{c|}{n/a} \\ \hline\hline
d345 2
a346 2
0.9098 &
\multicolumn{3}{c|}{0.8960} \\ \hline
d352 1
a352 1
\end{table}
d355 9
d370 3
a372 3
estimates is extremely high.  With the exception of NFS attribute (W)
events, all Pox plot and variance-time plot estimates were between 5-10\%
percent of each other.
d389 2
a390 194
      parameters produced for the NFS and Sprite traces via R/S analysis.
\end{itemize}

\subsection{ON/OFF Sources}\label{on_off_model}

The self-similar nature of file system traffic has now been convincingly
demonstrated using short-term file system traces.  However, we have not yet
attempted to explain the underlying cause of this observed phenomenon.
Willinger {\it et al.}\cite{Will95} proposed a physical explanation of
observed self-similarity in Ethernet LAN traffic, based on theory developed
initially by Mandelbrot\cite{Mand65} and then Taqqu and Levy\cite{Taqq86}.

The theory states that the aggregation of many ON/OFF sources, each
exhibiting a characteristic known as the {\it Noah effect}, results in
self-similar total traffic.  An individual source is classified as being an
ON/OFF source if the traffic from that source appears to contain highly
variable lengthed ON and OFF periods; an ON period contains much activity,
while an OFF period has a complete lack of activity.  The Noah effect
refers to the high variability of the ON and OFF periods. If the
distribution of ON and OFF period lengths from individual sources is
heavy-tailed,
\footnote{A heavy-tailed distribution is typically one
which exhibits infinite variance.  An example of a heavy-tailed
distribution is the Pareto distribution, whose general form is $P(x) =
\beta a^\beta x^{-\beta-1}$ with $a,\beta \geq 0$ and $x \geq a$.  The
Pareto distribution has infinite variance for values of $\beta < 1$.}
then the aggregate traffic exhibits the Noah effect, and can be shown
to exhibit self-similarity.

The theory presented in \cite{Taqq86} makes the simplifying assumption that
events within an ON period are evenly distributed.  The ON/OFF source model
is thus similar to packet-train models often used to model network sources,
but with the exception that packet interarrival times must have a
heavy-tailed distribution.

An examination of the trace events from individual source hosts within
the Sprite cluster and NFS installation should identify whether or not
the self-similarity of the file system traces can be explained using
this ON/OFF model.  Figure \ref{onoff_pic} presents two textured dot
strip plots obtained from the Sprite and NFS traces.  A textured dot
strip plot (proposed in \cite{Tuke90}) is a two-dimensional
representation of a one-dimensional time-series.  Each vertical column
in a plot corresponds to one time unit; the displacement of dots
(representing events) within that column represents the fractional
position of the event in that time unit.

\begin{figure*}[tbh]
\epsfxsize 0.75\hsize
\begin{center}
\makebox{
\epsfbox[0 0 387 255]{./figures/on_off.eps}
}
\end{center}
\caption{{\bf Textured dot strip plots:} Textured dot strip plots with
time units of 1 second for the (a) Sprite file open (W) events of host
42 and (b) NFS block read events of host 3068 are
illustrated.}\label{onoff_pic}
\end{figure*}

In order to better depict the ON/OFF behaviour of the sources, the subset
of the Sprite traces that was analyzed was extended from a 4 hour subset to
an approximately 41 hour subset; access privilege information was also
extracted from the Sprite traces.  Figure
\ref{onoff_pic}a illustrates the textured strip dot plot for Sprite
open (W) events originating from host workstation ID 42 (4168 such
events were extract from the trace.)  Similarly, figure
\ref{onoff_pic}b illustrates NFS block reads from
host workstation ID 3068, for which 23025 events were extracted.  These two
hosts were chosen because they were both quite active throughout the period
of analysis, and their high activity resulted in visually striking ON/OFF
periods within the dot plots.  It should be noted that even relatively
lightly active hosts could be seen to exhibit this ON/OFF behaviour,
although the behaviour was not as pronounced in their textured dot plots.

The ON/OFF behaviour of these two sources is unmistakable.  It is also
clear that the ON periods for NFS block read events are much sharper
and more dense than for Sprite file open (W) events.  This difference
is easily explainable.  First and foremost, files are known to be read
far more frequently than they are written; from table
\ref{trace_summary} we see that approximately 13.4\% of NFS files are
opened with write or RW privileges, while 86.6\% of files are opened
with read-only privilege.  Similarly, \cite{Bake91}
reported that 88\% of files within the Sprite traces were
opened with read-only privilege.   Secondly, many file read and
write events occur in between a given file open and close pair.  The
total amount of read and write events therefore greatly outnumbers the
amount of open or close events.

\subsubsection{The Analysis of ON and OFF periods}

\begin{figure*}[tbh]
\epsfxsize 0.75\hsize
\begin{center}
\makebox{
\epsfbox[0 0 521 422]{./figures/hill_qq.eps}
}
\end{center}
\caption{{\bf Hill and qq-plots:} (a) and (c) illustrate Hill and
qq-plots from Sprite data, while (b) and (d) illustrate similar
Poisson distributed synthetic data for the sake of comparison.}
\label{hill_qq_pic}
\end{figure*}

In order to verify the presence of the Noah effect, the ON and OFF periods
for these sources first need to be identified.  To do so, we use a method
similar to that described in \cite{Will95}.  The source's trace is scanned
linearly; given an event from the trace, we assume that subsequent events
belong to the same ON period if they occur within some threshold amount of
time, otherwise we mark the interval to the next event as an OFF period.
The ON/OFF period size distributions were calculated for the Sprite file
open (W) events, using a threshold value of 60 seconds.  The resulting
number of ON and OFF periods was $182$ and $181$, respectively.

If both the ON and OFF period length
distributions are heavy-tailed, i.e. they satisfy
\begin{equation}
      P(U>u) \sim cu^{-\alpha} \; \mbox{with} \; u \rightarrow \infty, 1 <
      \alpha < 2,
\label{heavytail_onoff}
\end{equation}
for period length U, and if the activity within an ON-period is
uniform, then the aggregation of many such sources gives rise to a
self-similar process with Hurst parameter $H =
\frac{3-\alpha}{2}$. \cite{Will95}

Once the ON and OFF periods from a given source have been identified,
we can verify the presence of the Noah effect using two techniques,
one heuristic and the other statistically more robust.  The first
technique makes use of complementary cumulative distribution plots, or
``qq-plots.''\cite{Krat95} The idea is simple: if we assume that the
period length distribution under analysis obeys equation
\ref{heavytail_onoff}, then by taking the $\log$ of both the sides of
equation, we obtain
\begin{equation}
    \log P(U > u) \sim \log (c) - \alpha \log (u) \; \mbox{as} \; u
\rightarrow \infty.
\label{qq_equation}
\end{equation}
Plotting the measured complementary cumulative period length
distribution $P(U > u)$ versus period length $u$ in a log-log plot
should thus yield a straight line with slope $-\alpha$, for large
enough values of $u$.

The observed linearity of the qq-plot in figure \ref{hill_qq_pic}a
clearly demonstrates that the observed ON-period lengths for Sprite
host 42's open (W) events follow a heavy tailed distribution.  For the
sake of comparison, a qq-plot for synthesized Poisson distributed
period lengths are shown in figure
\ref{hill_qq_pic}b.

The second method for identifying the Noah effect is through the use
of a {\it Hill plot}.  Given an ON/OFF process with a complementary
cumulative period length distribution satisfying equation
\ref{heavytail_onoff}, let the set of observed periods be $U_1, U_2,
\ldots, U_n$, with the corresponding order statistics $U_{(1)} \leq
U_{(2)} \leq \ldots \leq U_{(n)}$. The Hill estimate for $\alpha$ is
given by:
\begin{equation}
\hat{\alpha}_n = \left(
       \frac{1}{k} \sum_{i=0}^{i=k-1} ( \log U_{(n-i)} -
                                       \log U_{(n-k)} )
            \right),
\end{equation}
for some $1 < k \leq n$.  The plot of the estimated $\hat{\alpha}_n$
value versus the number of periods $k$ used in the estimation is
called a Hill plot.

A typical Hill plot shows instability for small values for $k$. It
then becomes stable for intermediate values of $k$ and finally
decreases slowly as more lower order terms (not belonging to the tail)
are added.  Ideally, we should observe that $\hat{\alpha}$ stabilizes
with a value between 1 and 2 for a Pareto distribution of $1 < \alpha
< 2$.  Indeed, figure \ref{hill_qq_pic}c shows a fairly stable
$\hat{\alpha}$ value of approximately 1.25 up until a $k$ value of 80,
followed by a gradual decrease, as expected.  For the sake of
comparison, figure \ref{hill_qq_pic}d shows a Hill plot for the same
synthesized Poisson distributed period lengths as were used in figure
\ref{hill_qq_pic}c.

Both the Hill and qq-plot analysis therefore confirm that the observed
ON and OFF periods lengths are Pareto distributed.  This is consistent,
since we have already shown that the aggregate traffic of all observed
ON/OFF sources is self-similar in nature.

To summarize, we have shown that:

\begin{itemize}
\item file system clients from both the NFS and Sprite file system
traces show ON/OFF behaviour.
\item the distribution of ON/OFF period lengths in both traces
is heavy-tailed, and therefore the traces show the Noah effect.
\item the Hurst parameters derived from calculated Hill estimates
$\hat{\alpha}_n$ are consistent with those calculated using variance-time
analysis and R/S analysis.
@


1.4
log
@More minor edits.
@
text
@d20 2
a21 2
complete information on file read and write events.  The total number of
bytes read from and written to files could be deduced from the seek and
d124 1
a124 1
distinguish among them -- the file system event process appears to exhibit
d260 1
a260 1
\item file system traffic is self-similar in nature.
d356 2
a357 1
\item R/S analysis has confirmed that file-system traffic is self-similar.
d420 4
a423 4
In order to better depict the ON/OFF behaviour of the sources, the
time period of the Sprite traces that was analyzed was extended from 4
hours in length to approximately 41 hours; access privilege
information was also extracted from the Sprite traces.  Figure
d451 13
a484 13

\begin{figure*}[tbh]
\epsfxsize 0.75\hsize
\begin{center}
\makebox{
\epsfbox[0 0 521 422]{./figures/hill_qq.eps}
}
\end{center}
\caption{{\bf Hill and qq-plots:} (a) and (c) illustrate Hill and
qq-plots from Sprite data, while (b) and (d) illustrate similar
Poisson distributed synthetic data for the sake of comparison.}
\label{hill_qq_pic}
\end{figure*}
@


1.3
log
@Another round of edits.
@
text
@d23 1
a23 1
write event could not be recovered \em these events were recorded in our
d124 1
a124 1
distinguish among them \em the file system event process appears to exhibit
@


1.2
log
@Added tim gibson's work, modified stuff, through out other stuff.
@
text
@d2 1
a2 1
\section{Self-Similarity in File Systems (short-term)}\label{measure_sec}
d14 4
a17 4
themselves.  The events within the file trace are at the level of kernel
calls on client machines with a precision of 10 milliseconds.  All file
system requests (including those that are satisfied by a client-side cache)
are present within the trace.
d23 1
a23 1
write event could not be recovered - these events were recorded in our
d26 8
a33 9
The second set of traces used in this paper was gathered from a
relatively large NFS installation served by a single Auspex NFS
server.  These traces were collected in 1994 to study the impact of
different cache policies on scalable network file system
performance\cite{Dahl94}.  The Auspex NFS server straddled four
separate Ethernets, on which resided a total of 237 clients.  The
traces were collected by monitoring network activity on each of the
four Ethernets.  Note that this implies that any file system request
satisfied by a client-side NFS cache was not present in the trace.
d42 5
a46 5
postprocessing likely introduced some minor distortion into our results, 
but this distortion is only on a very fine time scale (on the order
of milliseconds), and the fact that the statistical methods used to detect
self-similarity take into account many time scales (up to the order of many
hours) implies that the effects of such distortion are minimal.
d124 5
a128 5
distinguish among them - the file system event process appears to exhibit a
characteristically bursty behaviour that is independent of the time scale at
which it is displayed.  This burstiness provides strong evidence that the
process is in fact self-similar.  In comparison, the process models that
are typically used while modeling systems events (such as those having
d161 1
a161 1
between -1 and 0 correspond to Hurst parameters $H$ between 1 and 0.5;
d365 5
a369 5
The self-similar nature of file-system traffic has now been
convincingly demonstrated.  However, we have not yet attempted to
explain the underlying cause of this observed phenomenon.  Willinger {\it
et al.}\cite{Will95} proposed a physical explanation of observed
self-similarity in Ethernet LAN traffic, based on theory developed
d410 1
a410 1
\epsfbox[0 0 361 245]{./figures/on_off.eps}
d420 1
a420 1
interval of the Sprite traces that was analyzed was extended from 4
d460 1
a460 1
It is shown in \cite{Will95} that if both the ON and OFF period length
d469 2
a470 1
self-similar process with Hurst parameter $H = \frac{3-\alpha}{2}$.
@


1.1
log
@Initial revision
@
text
@d2 1
a2 1
\section{Self-Similarity in File Systems}\label{measure_sec}
d24 1
a24 1
second set of traces, however.
d55 2
a56 6
paper.\footnote{For the purpose of calculating open event interarrival
times, Sprite access privileges are considered in section \ref{model_sec}.
Forty-one hours worth of open events were analyzed, resulting in 232076
open (R) events, 21326 open (W) events, and 12643 open (RW) events.}
Four million events were analyzed from the Auspex traces - this represents
approximately 25 hours worth of trace data.
a57 1
\begin{footnotesize}
d61 4
a64 4
{\bf Event Type} & 
{\bf \# Sprite} &
\multicolumn{3}{c|}{\bf \# NFS Events} \\ 
 & Events & \multicolumn{1}{c|}{R} & W & RW \\ \hline\hline
d70 1
a70 1
{\bf Block Transfer} & n/a & 344564 & 71399 & 0 \\ \hline
d87 6
a92 2
14413s (4h)&
\multicolumn{3}{c|}{91337s (25.37h)} \\ \hline
a97 1
\end{footnotesize}
d212 1
a212 1
{\bf Block Transfer} & n/a & 0.8599 & 0.8478 & n/a \\ \hline
d254 3
a256 3
events are quickly followed by close events (see section
\ref{distributions}); the open and close event processes should therefore
be equally bursty and asymptotically self-similar.
d320 1
a320 1
{\bf Block Transfer} & n/a & 0.8071 & 0.8424 & n/a \\ \hline
d481 2
a482 2
Poisson distributed synthetic data for the sake of comparison.
illustrated.}\label{hill_qq_pic}
d555 8
@


1.1.1.1
log
@Sigmetrics paper

@
text
@@
