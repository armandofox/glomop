head	1.23;
access;
symbols;
locks; strict;
comment	@% @;


1.23
date	98.09.02.22.43.39;	author daw;	state Exp;
branches;
next	1.22;

1.22
date	98.09.02.20.55.10;	author iang;	state Exp;
branches;
next	1.21;

1.21
date	98.09.02.20.25.28;	author daw;	state Exp;
branches;
next	1.20;

1.20
date	98.09.02.20.16.34;	author daw;	state Exp;
branches;
next	1.19;

1.19
date	98.09.02.20.15.37;	author daw;	state Exp;
branches;
next	1.18;

1.18
date	98.09.02.20.15.19;	author iang;	state Exp;
branches;
next	1.17;

1.17
date	98.09.02.20.06.30;	author daw;	state Exp;
branches;
next	1.16;

1.16
date	98.09.02.19.39.48;	author daw;	state Exp;
branches;
next	1.15;

1.15
date	98.09.02.19.02.20;	author daw;	state Exp;
branches;
next	1.14;

1.14
date	98.09.02.18.55.44;	author iang;	state Exp;
branches;
next	1.13;

1.13
date	98.09.02.18.54.31;	author daw;	state Exp;
branches;
next	1.12;

1.12
date	98.09.02.18.51.19;	author daw;	state Exp;
branches;
next	1.11;

1.11
date	98.09.02.18.49.28;	author daw;	state Exp;
branches;
next	1.10;

1.10
date	98.09.02.18.44.41;	author iang;	state Exp;
branches;
next	1.9;

1.9
date	98.09.02.18.43.57;	author daw;	state Exp;
branches;
next	1.8;

1.8
date	98.09.02.18.11.15;	author iang;	state Exp;
branches;
next	1.7;

1.7
date	98.09.02.18.09.17;	author daw;	state Exp;
branches;
next	1.6;

1.6
date	98.09.02.18.06.44;	author daw;	state Exp;
branches;
next	1.5;

1.5
date	98.09.02.18.05.26;	author daw;	state Exp;
branches;
next	1.4;

1.4
date	98.09.02.17.41.55;	author daw;	state Exp;
branches;
next	1.3;

1.3
date	98.09.02.17.30.45;	author iang;	state Exp;
branches;
next	1.2;

1.2
date	98.09.02.17.24.49;	author daw;	state Exp;
branches;
next	1.1;

1.1
date	98.09.02.16.47.11;	author iang;	state Exp;
branches;
next	;


desc
@@


1.23
log
@*** empty log message ***
@
text
@\documentclass{slides}
\usepackage{times}
\usepackage{landscape}
\raggedright
\addtolength{\textheight}{1in}
\addtolength{\topmargin}{-1.3in}
\addtolength{\textwidth}{1in}
\pagestyle{plain}
\newcommand\dolengths{ %
\setlength{\topsep}{0in}
\setlength{\parsep}{0in}
\setlength{\parskip}{.1in}
\setlength{\itemsep}{0in}
}
\newcommand\bi{\begin{itemize} \dolengths}
\newcommand\ei{\end{itemize}}
\newcommand\vf{\vspace*{\fill}}
\newcommand\bs[1]{\begin{slide}{} \vf \centerline{\Large #1} \vf \vf}
\newcommand\es{\vf \vf \end{slide}}
\begin{document}

\begin{slide}{}
{
\Large
\begin{center}
Lecture 3

Design Principles for Security-conscious Systems \end{center}
}
\end{slide}

\bs{Administrivia}
From time to time, we may discuss vulnerabilities in widely-deployed
computer systems. This is not intended as an invitation to go exploit
those vulnerabilities. It is important that we be able to discuss
real-world experience candidly; students are expected to behave
responsibly.

Berkeley's policy (and our policy) on this is clear: you may not break
into machines that are not your own; you may not attempt to attack or
subvert system security. Breaking into other people's systems is
inappropriate; and the existence of a security hole is no excuse.
\es

\bs{\bf Economy of Mechanism}
\bi
\item Keep your implementation as simple as possible
\bi
\item Note that \emph{simple} is different from \emph{small}: just because
you \emph{can} write a CGI program in 300 bytes of line-noise Perl, doesn't
mean you \emph{should}
\item All the usual structured-programming tips help here: clean interfaces
between modules, avoid global state, etc.
\ei
\item Interactions are a nightmare
\bi
\item You often need to check how each pair of subsystems interacts,
   and possibly even each \emph{subset} of subsystems
\item For example, interactions between the password checker and the
page-fault mechanism
\item Complexity grows as $\Omega(n^2)$, possibly even $\Omega(2^n)$
\ei
\ei
\es

\bs{Bellovin's Fundamental Theorem of Firewalls}
\begin{description}
\item[Axiom 1 (Murphy)] All programs are buggy.

\item[Theorem 1 (Law of Large Programs)] Large programs are even buggier than
their size would indicate.

\item[Corollary 1.1] A security-relevant program has security bugs.

\item[Theorem 2] If you do not run a program, it does not matter whether or not
it is buggy.

\item[Corollary 2.1] If you do not run a program, it does not matter if it has
security holes.

\item[Theorem 3] Exposed machines should run as few programs as possible; the
ones that are run should be as small as possible.
\end{description}
\es

\bs{The sendmail wizard hole}
\bi
\item Memory segments: {\tt text} (code),
	{\tt data} (initialized variables),
	{\tt bss} (variables not explicitly initialized),
	{\tt heap} ({\tt malloc}ed)
\item Config file parsed, then a ``frozen'' version written out
	by dumping the {\tt bss} and {\tt heap} segments to a file
% to avoid parsing the config file every time
% requires the following invariant: that all results of parsing
% be contained in the bss or heap segments
\item Wizard mode feature allows extra access for remote debugging
\item Wizard mode implementation:
	\begin{quote} \tt
	int wizflag;        // password enabled? \\
	char *wizpw = NULL; // ptr to passwd
	\end{quote}
% oops: inadvertently put wizpw in the data segment!
% but wizflag is in bss
\item Results:
	\bi
	\item In production mode, wizard mode enabled without
		a password.
	\item But in development, the password was tested, and
		worked fine\ldots
	% because after all, you test code when you add it; and
	% you can't use a frozen config when you've just recompiled,
	% because the bss format is different
	% so wizard mode password always worked in testing, never in production
	% if "char *wizpw = NULL;" had been changed to the
	% seemingly equivalent "char *wizpw;", it would've worked fine
	\ei
\ei
{\small Credits: Bellovin.}
\es

\bs{The ftpd/tar hole}
\bi
\item To save network bandwidth,
	{\tt ftpd} allows client to run {\tt tar} on the ftp server.
\item This was fine, until people started using GNU tar.
\item Security hole:
\begin{quote} \tt
quote site exec tar -c -v --rsh-command=commandtorunasftp -f somebox:foo foo
\end{quote}
\item Beware the wrath of feeping creaturism\ldots
\ei
\es

\bs{\bf Fail-safe Defaults}
\bi
\item Start by denying all access, then allow only that which has been
explicitly permitted
\bi
\item By doing this, oversights will usually show up as ``false negatives''
(i.e.\ someone who should have access is denied it); these will be reported
quickly
\item The opposite policy leads to ``false positives'' (bad guys gain access
when they shouldn't); the bad guys don't tend to report these types of
problems
\ei
\item In production and commercial systems, the configuration \emph{as shipped}
is what matters.  This has often been done wrong in the past:
\bi
\item SunOS shipped with {\tt +} in {\tt /etc/hosts.equiv}
\item Irix shipped with {\tt xhost +} by default
\ei
\ei
\es

\bs{Canonicalization Problem}
\bi
\item If you try to specify what objects are restricted, you will almost
certainly run in to the \emph{canonicalization problem}.
\item On most systems, there are many ways to name the same object; if
you need to explicitly deny access to it, you need to be able to either
\bi
\item list them all, or
\item canonicalize any name for the object to a unique version to compare
against
\ei
Unfortunately, canonicalization is hard.
\item For example, if I instruct my web server that files under
{\tt \~{}iang/private} are to be restricted, what if someone references
{\tt \~{}iang//private} or {\tt \~{}iang/./private} or
{\tt \~{}daw/../iang/private}?
\item Both the NT webserver and the CERN webservers have suffered
	from vulnerabilities along these lines.
\ei
\es

\bs{Canonicalization Problem, cont.}
\bi
\item Better if you tag somehow tag the object \emph{directly}, instead of
by name
\bi
\item check a file's device and inode number, for example
\item or run the webserver as uid {\tt web}, and only ensure that
	uid {\tt web} only has read access to public files
\item the {\tt .htaccess} mechanism accomplishes this by putting the ACL
file \emph{in} the directory it protects: the \emph{name} of the directory
is irrelevant
\ei
\item Better still if you have a fail-safe default: explicity \emph{allow}
access to a particular name; everything else is denied
\bi
\item Attempts to access the object in a non-standard way will be denied,
but that's usually OK
\ei
\ei
\es

\bs{\bf Complete Mediation}
\bi
\item Check \emph{every} access to \emph{every} object
\item In rare cases, you can get away with less (caching)
\bi
\item but only if you're \emph{sure} that nothing relevant
in the environment has changed
\item and there's a lot that's relevant\ldots
\ei
\item Note that this is not the distinction between ACLs and capabilities:
both allow for complete mediation
\bi
\item For ACLs, the check involves knowing this identity of the requestor
and checking it against a list
\item For capabilities, the check is simply ``does the requestor have a
valid capability for this object''
\ei
\ei
\es

\bs{Incomplete mediation in NFS}
\bi
\item NFS is \emph{not} a good example of complete mediation
\item NFS protocol: contact {\tt mountd} to get a filehandle,
	use the filehandle for all reads/writes on that file
\item Access to an exported directory is checked only at mount time by
{\tt mountd} % , when it decides whether to give you a filehandle
\item If you can sniff or guess the filehandle, you don't have to
contact {\tt mountd} at all, and you can just access the files directly,
with no checks
\ei
\es


\bs{Imperfect bookkeeping in sendmail}
\bi
\item Sendmail treats program execution as an address;
	for security, it tries to restrict it to alias expansion.
\item This requires perfect bookkeeping: \\
 at every place an address can appear, one must check to ensure that
 it isn't program delivery.
\item But there are too many different places that addresses could appear.
\item Inevitable results: a few places where the check was forgotten,
	which has led to several security holes.
\ei
{\small Credits: Bellovin.}
\es

\bs{Mediation in Java}
\bi
\item Access control in Java libraries is done like this:
{\small \begin{verbatim}
public boolean mkdir() {
   SecurityManager security = System.getSecurityManager();
   if (security != null)
      security.checkWrite(path);
   return mkdir0(); // the real mkdir
}
\end{verbatim}}
\item But forgetting just one such check leaves the access
	control wide open
\item And there are 70 such calls in JDK1.1; what are the odds
	the developers forgot one?
\item Just for kicks: a fun comment from {\tt net/DatagramSocket.java}:
{\small \begin{verbatim}
// The reason you want to synchronize on datagram packet
// is because you dont want an applet to change the address
// while you are trying to send the packet for example
// after the security check but before the send.
\end{verbatim}}
\item Conclusion: it is not easy to convince oneself that Java
	exhibits complete mediation
% partially because Java wants such fine-grained control over permissions
\ei
\es

\bs{\bf Separation of Privilege}
\bi
\item Require more than one check before granting access to an object
\bi
\item A single check may fail, or be subverted.  The more checks, the harder
this should be
\item Something you know, something you have, something you are
\item e.g. Kerberos checks both your ticket and your IP address
\item e.g. Airport security checks both the shape of your hand and a PIN
\ei
\item Require that more than one principal ``sign off'' on an attempted
access before granting it
\bi
\item This is easy to do with cryptography: secret sharing can
\emph{mathematically} provide that a capability is released only when
$k$ out of $n$, for example, agree.
\ei
\ei
\es

\bs{Anonymous Remailers}
\bi
\item Anonymous remailers allow people to send email while hiding the
originating address
\item They work by a process known as \emph{chaining}: imagine the message
is placed in a series of nested envelopes, each addressed to one of the
remailers in the world
\item Each remailer can open only his own envelope (cryptography is used here)
\item Each remailer opens his envelope, and sends the contents to the
addressee; he does not know where it's going after that, or where it came
from before it got to him
\item In order to trace a message, \emph{all} the remailers in the
chain need to cooperate
\ei
\es

\bs{\bf Least Privilege}
\bi
\item Figure out exactly what capabilities a program requires in order to
run, and grant exactly those
\item This is not always easy, but by starting with granting none, and seeing
where errors occur, you can usually do OK
\bi
\item Watch out for unusual cases!
\ei
\item This is the principle used to design policy for sandboxes (e.g.\ Janus)
\item The Unix concept of {\tt root} is \emph{not} a good example of this
\bi
\item Some programs need to run as {\tt root} just to get one small privilege,
such as binding to a low-numbered port
\item This leaves them susceptible to buffer-overflow exploits that have
complete run of the machine
\ei
\ei
\es

\bs{Tractorbeaming wu-ftpd}
\bi
\item {\tt wu-ftpd} tries to run with least privilege, but occasionally
	elevates its privilege level with
	\begin{quote} \tt
	seteuid(0); \\
	// privileged critical section goes here... \\
	seteuid(getuid());
	\end{quote}
\item However, {\tt wu-ftpd} does not disable signals.
\item Thus, when it is running in a critical section, it can
	be ``tractorbeamed'' away to a signal handler not
	expecting to be run with root privileges.
\item Moreover, remote ftp users can cause {\tt wu-ftpd} to receive
	a signal just by aborting a file transfer.
\item Result: if you win a race condition, {\tt wu-ftpd} never
	reliquinshes {\tt root} privileges, and you get unrestricted access
	to all files.
\item Conclusion: uid/euid not a robust mechanism for implementing
	least privilege.
% and note that if there is a stack overflow *anywhere*, you
% can still get full root access by adding a seteuid(0); call in
% your overflow code
\ei
{\small Credits: Wietse Venema.}
\es

\bs{Sandboxes and code confinement}
\bi
\item Least privilege is the whole motivation behind the use
	of sandboxes to confine partially-untrusted code.
\item Example: sendmail
	\bi
	\item Once sendmail is broken into, intruder gains root access,
		and the game is over.
	\item Better would be for sendmail to run in a limited execution
		domain with access only to the mail subsystem.
	\ei
\item Example: Netscape plugins
	\bi
	\item Netscape plugins run in the browser's address space,
		with no protection.
	\item At one point, a bug in the popular Shockwave plugin could
		be used by malicious webmasters to read your email,
		by abusing {\tt mailbox:}-style URLs.
	\ei
\ei
\es



\bs{\bf Least Common Mechanism}
\bi
\item Be careful with shared code
\bi
\item The assumptions originally made may no longer be valid
\ei
\item LiveConnect: allows Java and Javascript and the browser to talk
to each other
\bi
\item But Java and Javascript have different ways to get at the same 
information, and also different security policies
\item A malicious Java applet could cooperate with a malicious Javascript
page to communicate information neither could have communicated alone
\ei
\item Some C library routines (and the C runtime) have excess features that
lead to security holes
\ei
\es

\bs{Eudora and Windows}
\bi
\item Windows exports an easy interface to IE's HTML-rendering code
\item Eudora, among other programs, uses this interface to display, for
example, HTML-formatted email
\item By default, parsing of Java and Javascript (J-Script) are \emph{enabled}
\item However, the HTML-rendering code ``knows'' that Java and
J-Script are unsafe when loaded from the Internet, but safe
when loaded from local disk
\item But the email \emph{is} loaded from local disk!
\item Oops\ldots
\item Enabling Java and J-Script by default in the common
HTML-rendering code violated the Principle of
Least Common Mechanism
\ei
\es

\bs{\bf Psychological Acceptability}
\bi
\item Very important for your users to buy into the security model.
	\bi
	\item If you force users to change their password every week,
	very soon most of them will simply write their password
	on a yellow sticky note attached to their monitor.
	\item If users think your firewall is too restrictive, they'll
	hook up a modem to their machine so they can dial in from home,
	and you're hosed.
	\item Never underestimate the ingenuity of engineers at bypassing
	obstacles that prevent them from getting work done!
	\item Also important that the management buys into security. \\
	(Proof by reading Dilbert.)
	\ei
\item And the user interface to security mechanisms should be
	in an intuitively understandable form.
	\bi
	\item NSA crypto gear stores keying material on a physical token
	in the shape of a key.
	To enable a ciphering device, you insert the key and turn it.
	\ei
\ei
\es

\bs{\bf Work Factor}
\bi
\item Work factor: an attempt to quantify the cost of breaking
system security.
\item Work factor issues are increasingly important today:
	\bi
	\item More and more ``script kiddies''
	\item And with {\tt www.rootshell.com} etc., discovery of a
	security hole is likely to lead to widespread exploitation with
	days
	\item So you should concentrate on increasing the cost of
	exploiting bugs, rather than focusing on the cost of discovering bugs
	\ei
\item One important distinguishing feature of crypto is the relative ease
which which you can put concrete numbers on the required work factor
(in terms of computational complexity).
\ei
\es

\bs{Work factor, cont.}
\bi
\item Remember, {\em security is economics}.
	\bi
	\item You can always improve site security with an increased
	investment, but you need to decide when such expenditures are
	economically sensible.
	\item Don't buy a \$10,000 firewall to protect \$1000 worth
	of trade secrets.
	\ei
\ei
\es

\bs{\bf Compromise Recording}
\bi
\item Compromise recording: if you can't prevent breakins,
in some cases it is enough to just detect them after-the-fact.
\item This is the whole idea behind modern-day intrusion detection systems
(network ``burglar alarms'').
\item And just saving audit logs can be useful, even if you don't
have an automated intrusion detection system.
	\bi
	\item If you discover an intruder has broken into one CS machine
	via the {\tt rpc.statd} hole (say), you might like to know how many
	other machines he broke into.
	\ei
\ei
\es

\bs{Compromise Recoding, cont.}
\bi
\item An important principle in hardware tamper resistance,
e.g. FIPS 140-1 standard:
	\bi
	\item Type II device is tamper-evident
	\item Type III device is tamper-resistant (but more expensive)
	\ei
\item Example: casinos often don't bother looking for fraud unless their 
daily take differs significantly from expectations.
	\bi
	\item Principle: you don't care about fraud if it doesn't affect
	your bottom line enough to notice.
	\ei
\ei
\es

\end{document}
@


1.22
log
@.
@
text
@d474 1
a474 1
\bs{\bf Compromise Recoding}
a503 85
\ei
\es

\bs{\bf Orthogonal Security}
\bi
\item Orthogonal security: security mechanisms should be implemented
	orthogonally to the systems they protect
\item Examples:
	\bi
	\item Wrappers to transparently improve system security,
	e.g. {\tt tcp\_wrappers}, {\tt securelib}, sandboxing, etc.
	\item Intrusion detection systems
	\item IP security, and out-board encryptors
	\ei
\item Advantages:
	\bi
	\item Simpler $\Rightarrow$ higher assurance
	\item Applicable to legacy, black-box, untrusted code
	\item Can be composed into multiple layer to provide
	more complete or redundant security
	\ei
\ei
\es

\bs{\bf Open Design}
\begin{quote}
``Drive your car to a master mechanic. Tell them that you want
a full diagnostic performed on the engine. Tell them that
they're to do that, but they can't open the hood to get at it.
They will look at you funny.'' \\
\hfill ---Marcus Ranum
\end{quote}

\bi
\item ``Security through obscurity'' is \emph{bad}.  This has been known
since 1853.
\item For security-critical code, you want as many people looking at it
as possible
\item Remember: the black hats trade info much more readily than the
white hats, so security information must be distributed to the white
hats (and everyone else) as quickly as possible
\item CERT does this \emph{badly}
\ei
\es

\bs{Open Design, cont.}
\bi
\item But being open doesn't automatically make you secure!
\item Firewall-1 was open source for years before anyone actually bothered
to look at it
\ei
\begin{center}
\begin{tabular}{|c||c|c||}
\hline
& Closed & Open \\
& Systems & Systems \\
\hline
\hline
Insecure & cellphones, & Firewall-1, \\
Systems & backdoors & Kerberos, X11\\
\hline
Secure & Military & pgp, \\
Systems & applications & ssh\\
\hline
\hline
\end{tabular}
\end{center}
\es

\bs{Open Design, cont.}
\bi
\item Strong vs.\ weak argument for open design:
\bi
\item Weak: Don't \emph{rely} on security through obscurity, because your
secrets will leak out eventually
\item Strong: Your system will actually \emph{benefit} from having everyone
examine its strength
\ei
\item Designing static vs.\ changing systems:
\bi
\item Don't only make your system secure \emph{now}; also know that there
will be bugs you will need to fix
\item You need a design that allows these bugs to be discovered, and
the fixes deployed gracefully
\ei
@


1.21
log
@*** empty log message ***
@
text
@d32 14
a45 1
\bs{Economy of Mechanism}
d135 1
a135 1
\bs{Fail-safe Defaults}
d198 1
a198 1
\bs{Complete Mediation}
d274 1
a274 11
\bs{Open Design}
\begin{quote}
``Drive your car to a master mechanic. Tell them that you want
a full diagnostic performed on the engine. Tell them that
they're to do that, but they can't open the hood to get at it.
They will look at you funny.'' \\
\hfill ---Marcus Ranum
\end{quote}
\es

\bs{Separation of Privilege}
d310 1
a310 1
\bs{Least Privilege}
d381 1
a381 1
\bs{Least Common Mechanism}
d383 1
a383 1
\item Be careful when code can be freely reused
d395 2
d417 1
a417 1
\bs{Psychological Acceptability}
d435 1
a435 1
	\item NSA crypto gear store keying material on a physical token
d442 1
a442 1
\bs{Work Factor}
d474 1
a474 1
\bs{Compromise Recoding}
d507 1
a507 1
\bs{Orthogonal Security}
d525 64
@


1.20
log
@*** empty log message ***
@
text
@d389 1
a389 1
\item A malicious java applet could cooperate with a malicious javascript
d406 2
a407 1
\item Enabling Java and J-Script by default violated the Principle of
@


1.19
log
@*** empty log message ***
@
text
@d514 1
a514 1
	\item Simpler $\rightarrow$ higher assurance
@


1.18
log
@.
@
text
@d502 18
@


1.17
log
@*** empty log message ***
@
text
@d379 30
@


1.16
log
@*** empty log message ***
@
text
@d407 29
@


1.15
log
@*** empty log message ***
@
text
@d354 24
d382 22
d410 30
@


1.14
log
@.
@
text
@d188 3
a190 2
\item In rare cases, you can get away with less (caching), but only if
you're \emph{sure} that nothing relevant (at there's a lot that's relevant)
d192 2
d205 1
a205 3
\bs{NFS}
\bi
\item NFS is \emph{not} a good example of this
d207 3
d211 1
a211 1
{\tt mountd}, when it decides whether to give you a filehandle
a215 1
\ei
d218 1
d228 1
a228 1
	led to several security holes.
@


1.13
log
@*** empty log message ***
@
text
@d186 27
@


1.12
log
@foo
@
text
@d146 1
a146 1
certainly run in to the \emph{canonicalization problem}
d159 2
d170 2
@


1.11
log
@foo
@
text
@d115 1
a115 1
{\small \begin{verbatim}
d117 1
a117 1
\end{verbatim}}
@


1.10
log
@.
@
text
@d109 13
@


1.9
log
@foo
@
text
@d224 33
d260 17
@


1.8
log
@.
@
text
@d185 28
@


1.7
log
@foo
@
text
@d19 1
a19 1
\newcommand\es{\vf \end{slide}}
d46 2
d54 2
a55 1
{\bf Axiom 1 (Murphy)} All programs are buggy.
d57 1
a57 1
{\bf Theorem 1 (Law of Large Programs)} Large programs are even buggier than
d60 1
a60 1
{\bf Corollary 1.1} A security-relevant program has security bugs.
d62 1
a62 1
{\bf Theorem 2} If you do not run a program, it does not matter whether or not
d65 1
a65 1
{\bf Corollary 2.1} If you do not run a program, it does not matter if it has
d68 1
a68 1
{\bf Theorem 3} Exposed machines should run as few programs as possible; the
d70 1
d110 37
d149 18
a193 1

@


1.6
log
@foo
@
text
@d157 1
a157 1
	a signal just by aborting the transfer.
d161 5
@


1.5
log
@foo
@
text
@a113 1
\item
@


1.4
log
@foo
@
text
@d128 7
d137 1
d142 22
@


1.3
log
@.
@
text
@d80 1
a80 1
\item Wizard mode feature allows full access for remote debugging
d83 1
a83 1
	int wizflag;        // password enabled?
d108 1
d110 15
@


1.2
log
@foo
@
text
@a3 1
\pagestyle{plain}
d7 1
a7 1
\begin{document}
d15 6
d28 1
a28 2
Design Principles for Security-conscious Systems
\end{center}
d32 10
a41 3
\begin{slide}{}
\begin{center} {\Large Economy of Mechanism} \end{center} \vskip 1.5in
\begin{itemize} \dolengths
d43 3
a45 1
\begin{itemize} \dolengths
d47 21
a67 4
\end{itemize}
\item foo
\end{itemize}
\end{slide}
d69 1
a69 2
\begin{slide}{}
\begin{center} {\Large The sendmail wizard hole} \end{center} \vskip 1.5in
d103 1
a103 1
\end{slide}
d105 2
a106 3
\begin{slide}{}
\begin{center} {\Large Fail-safe Defaults} \end{center} \vskip 1.5in
\end{slide}
d108 2
a109 3
\begin{slide}{}
\begin{center} {\Large Complete Mediation} \end{center} \vskip 1.5in
\end{slide}
d111 2
a112 3
\begin{slide}{}
\begin{center} {\Large Open Design} \end{center} \vskip 1.5in
\end{slide}
d114 2
a115 3
\begin{slide}{}
\begin{center} {\Large Separation of Privilege} \end{center} \vskip 1.5in
\end{slide}
d117 2
a118 3
\begin{slide}{}
\begin{center} {\Large Least Privilege} \end{center} \vskip 1.5in
\end{slide}
d120 2
a121 3
\begin{slide}{}
\begin{center} {\Large Least Common Mechanism} \end{center} \vskip 1.5in
\end{slide}
d123 2
a124 3
\begin{slide}{}
\begin{center} {\Large Psychological Acceptability} \end{center} \vskip 1.5in
\end{slide}
d126 2
a127 3
\begin{slide}{}
\begin{center} {\Large Work Factor} \end{center} \vskip 1.5in
\end{slide}
d129 2
a130 3
\begin{slide}{}
\begin{center} {\Large Compromise Recoding} \end{center} \vskip 1.5in
\end{slide}
d132 2
a133 3
\begin{slide}{}
\begin{center} {\Large Orthogonal Security} \end{center} \vskip 1.5in
\end{slide}
@


1.1
log
@.
@
text
@d40 37
@
